{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/nwon24/nlp/blob/main/W3/vectorise.ipynb",
      "authorship_tag": "ABX9TyOuyf/kInd/DC0p7Gwqkvxh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nwon24/nlp/blob/main/W3/vectorise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorising text\n",
        "\n",
        "Vectorising is the process of converting text into a form that can be used as input to a classifier or neural network. This is because intrinsically computers can't understand text! So we send our text through a preprocessing pipeline, which spits out at the end numerical representations of the tokens in our text.\n",
        "\n",
        "There are various vectorisers. A simple one is called Bag of Words, which simply counts the frequency of each word in the text (or collection of texts--corpus) and stores those frequency counts in a dictionary. The issue here is that common words that don't really add much to the meaning of the texts (e.g., articles, prepositions) are weighted quite heavily.\n",
        "\n",
        "An improved vectoriser is called Term Frequency - Inverse Document Frequency (TF-IDF). This vectoriser takes as input a corpus and spits out a score for each token in the each text of the corpus. The higher the score, the greater importance of that token to the meaning of that particular text. The trick here is that words are not only weighted according to their frequency in their own document, but also inversely to the frequency of their apperance in the entire corpus. This means that words that appear often in all the texts (such as stop words) will be weighted very lowly, while words that appear frequently but only in one text will be given a higher weighting, indicating that word is relatively more important to the meaning of that text.\n",
        "\n",
        "To assign a score to each word, this vectoriser first computes the term frequency of a word in a particular document: the number of times the word appears in the document divided by the number of words in the document. Then the iinverse document frequency is calculated by taking the logarithm of the number of documents in the corpus divided by the number of documents in which that word appears. The TF and IDF are then multiplied together to get the TF-IDF score for that particular word."
      ],
      "metadata": {
        "id": "c_dnnnZCkNl2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RoSfc66wh3b",
        "outputId": "8ae91961-207e-4c4e-c85d-a57d9e45223f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'this': 0.0, 'line': 0.0, 'text': 0.0, 'contain': 0.0, 'word': 0.0}\n",
            "{'this': 0.0, 'another': 0.053319013889226566, 'line': 0.0, 'text': 0.0, 'contain': 0.0, 'word': 0.0, 'group': 0.053319013889226566, 'ring': 0.053319013889226566, 'module': 0.053319013889226566}\n",
            "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 28 stored elements and shape (2, 19)>\n",
            "  Coords\tValues\n",
            "  (0, 17)\t1.0\n",
            "  (0, 5)\t1.0\n",
            "  (0, 7)\t1.0\n",
            "  (0, 10)\t1.0\n",
            "  (0, 14)\t1.0\n",
            "  (0, 1)\t1.0\n",
            "  (0, 6)\t1.0\n",
            "  (0, 3)\t1.0\n",
            "  (0, 13)\t1.4054651081081644\n",
            "  (0, 18)\t1.0\n",
            "  (1, 17)\t1.6931471805599454\n",
            "  (1, 5)\t1.6931471805599454\n",
            "  (1, 7)\t2.09861228866811\n",
            "  (1, 10)\t1.6931471805599454\n",
            "  (1, 14)\t1.6931471805599454\n",
            "  (1, 1)\t1.6931471805599454\n",
            "  (1, 6)\t1.0\n",
            "  (1, 3)\t1.0\n",
            "  (1, 18)\t1.0\n",
            "  (1, 2)\t1.4054651081081644\n",
            "  (1, 9)\t1.4054651081081644\n",
            "  (1, 15)\t1.4054651081081644\n",
            "  (1, 16)\t1.4054651081081644\n",
            "  (1, 11)\t1.4054651081081644\n",
            "  (1, 0)\t1.4054651081081644\n",
            "  (1, 4)\t1.4054651081081644\n",
            "  (1, 12)\t1.4054651081081644\n",
            "  (1, 8)\t1.4054651081081644\n",
            "{'this': 17, 'is': 5, 'line': 7, 'of': 10, 'text': 14, 'and': 1, 'it': 6, 'contains': 3, 'some': 13, 'words': 18, 'another': 2, 'more': 9, 'than': 15, 'the': 16, 'other': 11, 'about': 0, 'groups': 4, 'rings': 12, 'modules': 8}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import re\n",
        "import nltk\n",
        "from nltk import FreqDist, word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "\n",
        "lem=WordNetLemmatizer()\n",
        "\n",
        "def get_pos(tag):\n",
        "    if re.match(r\"^JJ\",tag):\n",
        "        return \"a\"\n",
        "    elif re.match(r\"^NN\",tag) or re.match(r\"^PRP\",tag):\n",
        "        return \"n\"\n",
        "    elif re.match(r\"^RB\",tag):\n",
        "        return \"r\"\n",
        "    elif re.match(r\"^VB\",tag):\n",
        "        return \"v\"\n",
        "    return \"\"\n",
        "\n",
        "def tokenize(doc):\n",
        "    sents = sent_tokenize(doc)\n",
        "    #stem = nltk.stem.SnowballStemmer('english')\n",
        "    words=[word.lower() for sent in sents for word in word_tokenize(sent) if word.isalpha() and word not in stop_words]\n",
        "    tagged_words=nltk.pos_tag(words)\n",
        "    lemmed_words=[lem.lemmatize(word[0],pos=get_pos(word[1])) if get_pos(word[1])!=\"\" else lem.lemmatize(word[0]) for word in tagged_words]\n",
        "    return lemmed_words\n",
        "\n",
        "def tokenize_corpus(corpus):\n",
        "    return [tokenize(doc) for doc in corpus]\n",
        "\n",
        "def vectorize_tf_idf(tokens):\n",
        "    texts = nltk.text.TextCollection(tokens)\n",
        "    for doc in tokens:\n",
        "        yield {\n",
        "            term: texts.tf_idf(term, doc)\n",
        "            for term in doc\n",
        "        }\n",
        "\n",
        "def vectorize_bow(tokens):\n",
        "    for doc in tokens:\n",
        "        bow = dict.fromkeys(doc,0)\n",
        "        for word in doc:\n",
        "            bow[word] += 1\n",
        "        yield bow\n",
        "\n",
        "def pipeline(corpus):\n",
        "    return list(vectorize_tf_idf(tokenize_corpus(corpus)))\n",
        "\n",
        "corpusdir=\"corpus\"\n",
        "corpus=[]\n",
        "\n",
        "with os.scandir(corpusdir) as files:\n",
        "    for file in files:\n",
        "        if not os.path.isdir(file):\n",
        "            with open(file,\"r\") as f:\n",
        "                corpus.append(f.read())\n",
        "\n",
        "tf_idf_words = pipeline(corpus)\n",
        "for i in tf_idf_words:\n",
        "    print(i)\n",
        "\n",
        "tfidf = TfidfVectorizer(norm=None,smooth_idf=True,sublinear_tf=True,min_df=0.01)\n",
        "print(tfidf.fit_transform(corpus))\n",
        "print(tfidf.vocabulary_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qPQFypcDkMP5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R8gAFGlthWMF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}