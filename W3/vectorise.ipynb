{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0WitVQxu4LPdEWCYa1Pdy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nwon24/nlp/blob/main/W3/vectorise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorising text\n",
        "\n",
        "Vectorising is the process of converting text into a form that can be used as input to a classifier or neural network. This is because intrinsically computers can't understand text! So we send our text through a preprocessing pipeline, which spits out at the end numerical representations of the tokens in our text.\n",
        "\n",
        "There are various vectorisers. A simple one is called Bag of Words, which simply counts the frequency of each word in the text (or collection of texts--corpus) and stores those frequency counts in a dictionary. The issue here is that common words that don't really add much to the meaning of the texts (e.g., articles, prepositions) are weighted quite heavily.\n",
        "\n",
        "An improved vectoriser is called Term Frequency - Inverse Document Frequency (TF-IDF). This vectoriser takes as input a corpus and spits out a score for each token in the each text of the corpus. The higher the score, the greater importance of that token to the meaning of that particular text. The trick here is that words are not only weighted according to their frequency in their own document, but also inversely to the frequency of their apperance in the entire corpus. This means that words that appear often in all the texts (such as stop words) will be weighted very lowly, while words that appear frequently but only in one text will be given a higher weighting, indicating that word is relatively more important to the meaning of that text.\n",
        "\n",
        "To assign a score to each word, this vectoriser first computes the term frequency of a word in a particular document: the number of times the word appears in the document divided by the number of words in the document. Then the iinverse document frequency is calculated by taking the logarithm of the number of documents in the corpus divided by the number of documents in which that word appears. The TF and IDF are then multiplied together to get the TF-IDF score for that particular word."
      ],
      "metadata": {
        "id": "c_dnnnZCkNl2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "9RoSfc66wh3b",
        "outputId": "3c271e57-00cd-4a83-8474-79975ecc64e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'corpus'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-84f6f60458ef>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpusdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'corpus'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import re\n",
        "import nltk\n",
        "from nltk import FreqDist, word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "\n",
        "lem=WordNetLemmatizer()\n",
        "\n",
        "def get_pos(tag):\n",
        "    if re.match(r\"^JJ\",tag):\n",
        "        return \"a\"\n",
        "    elif re.match(r\"^NN\",tag) or re.match(r\"^PRP\",tag):\n",
        "        return \"n\"\n",
        "    elif re.match(r\"^RB\",tag):\n",
        "        return \"r\"\n",
        "    elif re.match(r\"^VB\",tag):\n",
        "        return \"v\"\n",
        "    return \"\"\n",
        "\n",
        "def tokenize(doc):\n",
        "    sents = sent_tokenize(doc)\n",
        "    #stem = nltk.stem.SnowballStemmer('english')\n",
        "    # Get only the unique words -- otherwise it's going to take a long, long time...\n",
        "    words=list(set([word.lower() for sent in sents for word in word_tokenize(sent) if word.isalpha()]))\n",
        "    tagged_words=nltk.pos_tag(words)\n",
        "    lemmed_words=[lem.lemmatize(word[0],pos=get_pos(word[1])) if get_pos(word[1])!=\"\" else lem.lemmatize(word[0]) for word in tagged_words]\n",
        "    return lemmed_words\n",
        "\n",
        "def tokenize_corpus(corpus):\n",
        "    return [tokenize(doc) for doc in corpus]\n",
        "\n",
        "def vectorize_tf_idf(tokens):\n",
        "    texts = nltk.text.TextCollection(tokens)\n",
        "    for doc in tokens:\n",
        "        yield {\n",
        "            term: texts.tf_idf(term, doc)\n",
        "            for term in doc\n",
        "        }\n",
        "\n",
        "def vectorize_bow(tokens):\n",
        "    for doc in tokens:\n",
        "        bow = dict.fromkeys(doc,0)\n",
        "        for word in doc:\n",
        "            bow[word] += 1\n",
        "        yield bow\n",
        "\n",
        "def pipeline(corpus):\n",
        "    return list(vectorize_tf_idf(tokenize_corpus(corpus)))\n",
        "\n",
        "corpusdir=\"corpus\"\n",
        "corpus=[]\n",
        "\n",
        "with os.scandir(corpusdir) as files:\n",
        "    for file in files:\n",
        "        if not os.path.isdir(file):\n",
        "            with open(file,\"r\") as f:\n",
        "                corpus.append(f.read())\n",
        "\n",
        "tf_idf_words = pipeline(corpus)\n",
        "for i in tf_idf_words:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qPQFypcDkMP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a={'a':1,'b':2}\n",
        "b=np.array(a)"
      ],
      "metadata": {
        "id": "IsWh3kAJ0nQx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "094c9d33-9599-46dd-f87f-2049b8755a84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6baf0d9a9b83>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R8gAFGlthWMF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}