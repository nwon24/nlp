{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nwon24/nlp/blob/main/W4/cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Using PyTorch\n",
        "\n",
        "For some reason, the cost never goes below around 0.69. :("
      ],
      "metadata": {
        "id": "rB8HcKBYJEsO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo6yf9SHFUkm",
        "outputId": "9724bf1f-6e0b-4b82-e3d3-43ef80018801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost: 0.708766  [   20/ 7000]\n",
            "cost: 0.697480  [  220/ 7000]\n",
            "cost: 0.686464  [  420/ 7000]\n",
            "cost: 0.697473  [  620/ 7000]\n",
            "cost: 0.711969  [  820/ 7000]\n",
            "cost: 0.693778  [ 1020/ 7000]\n",
            "cost: 0.697315  [ 1220/ 7000]\n",
            "cost: 0.683127  [ 1420/ 7000]\n",
            "cost: 0.683118  [ 1620/ 7000]\n",
            "cost: 0.683117  [ 1820/ 7000]\n",
            "cost: 0.690228  [ 2020/ 7000]\n",
            "cost: 0.675940  [ 2220/ 7000]\n",
            "cost: 0.690174  [ 2420/ 7000]\n",
            "cost: 0.711930  [ 2620/ 7000]\n",
            "cost: 0.690214  [ 2820/ 7000]\n",
            "cost: 0.700955  [ 3020/ 7000]\n",
            "cost: 0.700940  [ 3220/ 7000]\n",
            "cost: 0.693770  [ 3420/ 7000]\n",
            "cost: 0.686677  [ 3620/ 7000]\n",
            "cost: 0.700944  [ 3820/ 7000]\n",
            "cost: 0.704497  [ 4020/ 7000]\n",
            "cost: 0.693795  [ 4220/ 7000]\n",
            "cost: 0.700865  [ 4420/ 7000]\n",
            "cost: 0.711454  [ 4620/ 7000]\n",
            "cost: 0.700841  [ 4820/ 7000]\n",
            "cost: 0.690213  [ 5020/ 7000]\n",
            "cost: 0.690197  [ 5220/ 7000]\n",
            "cost: 0.683049  [ 5420/ 7000]\n",
            "cost: 0.700912  [ 5620/ 7000]\n",
            "cost: 0.700830  [ 5820/ 7000]\n",
            "cost: 0.690240  [ 6020/ 7000]\n",
            "cost: 0.697205  [ 6220/ 7000]\n",
            "cost: 0.690305  [ 6420/ 7000]\n",
            "cost: 0.686855  [ 6620/ 7000]\n",
            "cost: 0.700681  [ 6820/ 7000]\n",
            "cost: 0.707575  [   20/ 7000]\n",
            "cost: 0.697153  [  220/ 7000]\n",
            "cost: 0.686863  [  420/ 7000]\n",
            "cost: 0.697147  [  620/ 7000]\n",
            "cost: 0.710700  [  820/ 7000]\n",
            "cost: 0.693697  [ 1020/ 7000]\n",
            "cost: 0.697002  [ 1220/ 7000]\n",
            "cost: 0.683746  [ 1420/ 7000]\n",
            "cost: 0.683737  [ 1620/ 7000]\n",
            "cost: 0.683725  [ 1820/ 7000]\n",
            "cost: 0.690380  [ 2020/ 7000]\n",
            "cost: 0.677007  [ 2220/ 7000]\n",
            "cost: 0.690322  [ 2420/ 7000]\n",
            "cost: 0.710704  [ 2620/ 7000]\n",
            "cost: 0.690367  [ 2820/ 7000]\n",
            "cost: 0.700422  [ 3020/ 7000]\n",
            "cost: 0.700416  [ 3220/ 7000]\n",
            "cost: 0.693695  [ 3420/ 7000]\n",
            "cost: 0.687046  [ 3620/ 7000]\n",
            "cost: 0.700425  [ 3820/ 7000]\n",
            "cost: 0.703771  [ 4020/ 7000]\n",
            "cost: 0.693719  [ 4220/ 7000]\n",
            "cost: 0.700360  [ 4420/ 7000]\n",
            "cost: 0.710315  [ 4620/ 7000]\n",
            "cost: 0.700347  [ 4820/ 7000]\n",
            "cost: 0.690353  [ 5020/ 7000]\n",
            "cost: 0.690332  [ 5220/ 7000]\n",
            "cost: 0.683600  [ 5420/ 7000]\n",
            "cost: 0.700427  [ 5620/ 7000]\n",
            "cost: 0.700350  [ 5820/ 7000]\n",
            "cost: 0.690374  [ 6020/ 7000]\n",
            "cost: 0.696942  [ 6220/ 7000]\n",
            "cost: 0.690440  [ 6420/ 7000]\n",
            "cost: 0.687186  [ 6620/ 7000]\n",
            "cost: 0.700218  [ 6820/ 7000]\n",
            "cost: 0.706724  [   20/ 7000]\n",
            "cost: 0.696901  [  220/ 7000]\n",
            "cost: 0.687192  [  420/ 7000]\n",
            "cost: 0.696887  [  620/ 7000]\n",
            "cost: 0.709688  [  820/ 7000]\n",
            "cost: 0.693639  [ 1020/ 7000]\n",
            "cost: 0.696760  [ 1220/ 7000]\n",
            "cost: 0.684228  [ 1420/ 7000]\n",
            "cost: 0.684219  [ 1620/ 7000]\n",
            "cost: 0.684206  [ 1820/ 7000]\n",
            "cost: 0.690502  [ 2020/ 7000]\n",
            "cost: 0.677839  [ 2220/ 7000]\n",
            "cost: 0.690445  [ 2420/ 7000]\n",
            "cost: 0.709750  [ 2620/ 7000]\n",
            "cost: 0.690484  [ 2820/ 7000]\n",
            "cost: 0.700017  [ 3020/ 7000]\n",
            "cost: 0.700013  [ 3220/ 7000]\n",
            "cost: 0.693638  [ 3420/ 7000]\n",
            "cost: 0.687335  [ 3620/ 7000]\n",
            "cost: 0.700029  [ 3820/ 7000]\n",
            "cost: 0.703203  [ 4020/ 7000]\n",
            "cost: 0.693663  [ 4220/ 7000]\n",
            "cost: 0.699969  [ 4420/ 7000]\n",
            "cost: 0.709425  [ 4620/ 7000]\n",
            "cost: 0.699966  [ 4820/ 7000]\n",
            "cost: 0.690464  [ 5020/ 7000]\n",
            "cost: 0.690440  [ 5220/ 7000]\n",
            "cost: 0.684037  [ 5420/ 7000]\n",
            "cost: 0.700047  [ 5620/ 7000]\n",
            "cost: 0.699976  [ 5820/ 7000]\n",
            "cost: 0.690481  [ 6020/ 7000]\n",
            "cost: 0.696733  [ 6220/ 7000]\n",
            "cost: 0.690547  [ 6420/ 7000]\n",
            "cost: 0.687449  [ 6620/ 7000]\n",
            "cost: 0.699856  [ 6820/ 7000]\n",
            "cost: 0.706053  [   20/ 7000]\n",
            "cost: 0.696697  [  220/ 7000]\n",
            "cost: 0.687451  [  420/ 7000]\n",
            "cost: 0.696686  [  620/ 7000]\n",
            "cost: 0.708881  [  820/ 7000]\n",
            "cost: 0.693589  [ 1020/ 7000]\n",
            "cost: 0.696566  [ 1220/ 7000]\n",
            "cost: 0.684625  [ 1420/ 7000]\n",
            "cost: 0.684613  [ 1620/ 7000]\n",
            "cost: 0.684597  [ 1820/ 7000]\n",
            "cost: 0.690599  [ 2020/ 7000]\n",
            "cost: 0.678510  [ 2220/ 7000]\n",
            "cost: 0.690539  [ 2420/ 7000]\n",
            "cost: 0.709001  [ 2620/ 7000]\n",
            "cost: 0.690582  [ 2820/ 7000]\n",
            "cost: 0.699689  [ 3020/ 7000]\n",
            "cost: 0.699691  [ 3220/ 7000]\n",
            "cost: 0.693594  [ 3420/ 7000]\n",
            "cost: 0.687566  [ 3620/ 7000]\n",
            "cost: 0.699706  [ 3820/ 7000]\n",
            "cost: 0.702748  [ 4020/ 7000]\n",
            "cost: 0.693621  [ 4220/ 7000]\n",
            "cost: 0.699654  [ 4420/ 7000]\n",
            "cost: 0.708706  [ 4620/ 7000]\n",
            "cost: 0.699651  [ 4820/ 7000]\n",
            "cost: 0.690562  [ 5020/ 7000]\n",
            "cost: 0.690531  [ 5220/ 7000]\n",
            "cost: 0.684400  [ 5420/ 7000]\n",
            "cost: 0.699733  [ 5620/ 7000]\n",
            "cost: 0.699670  [ 5820/ 7000]\n",
            "cost: 0.690571  [ 6020/ 7000]\n",
            "cost: 0.696561  [ 6220/ 7000]\n",
            "cost: 0.690634  [ 6420/ 7000]\n",
            "cost: 0.687664  [ 6620/ 7000]\n",
            "cost: 0.699556  [ 6820/ 7000]\n",
            "cost: 0.705498  [   20/ 7000]\n",
            "cost: 0.696531  [  220/ 7000]\n",
            "cost: 0.687664  [  420/ 7000]\n",
            "cost: 0.696519  [  620/ 7000]\n",
            "cost: 0.708221  [  820/ 7000]\n",
            "cost: 0.693555  [ 1020/ 7000]\n",
            "cost: 0.696413  [ 1220/ 7000]\n",
            "cost: 0.684951  [ 1420/ 7000]\n",
            "cost: 0.684937  [ 1620/ 7000]\n",
            "cost: 0.684926  [ 1820/ 7000]\n",
            "cost: 0.690685  [ 2020/ 7000]\n",
            "cost: 0.679072  [ 2220/ 7000]\n",
            "cost: 0.690626  [ 2420/ 7000]\n",
            "cost: 0.708342  [ 2620/ 7000]\n",
            "cost: 0.690664  [ 2820/ 7000]\n",
            "cost: 0.699413  [ 3020/ 7000]\n",
            "cost: 0.699410  [ 3220/ 7000]\n",
            "cost: 0.693559  [ 3420/ 7000]\n",
            "cost: 0.687764  [ 3620/ 7000]\n",
            "cost: 0.699435  [ 3820/ 7000]\n",
            "cost: 0.702358  [ 4020/ 7000]\n",
            "cost: 0.693581  [ 4220/ 7000]\n",
            "cost: 0.699389  [ 4420/ 7000]\n",
            "cost: 0.708101  [ 4620/ 7000]\n",
            "cost: 0.699377  [ 4820/ 7000]\n",
            "cost: 0.690639  [ 5020/ 7000]\n",
            "cost: 0.690622  [ 5220/ 7000]\n",
            "cost: 0.684711  [ 5420/ 7000]\n",
            "cost: 0.699479  [ 5620/ 7000]\n",
            "cost: 0.699413  [ 5820/ 7000]\n",
            "cost: 0.690652  [ 6020/ 7000]\n",
            "cost: 0.696432  [ 6220/ 7000]\n",
            "cost: 0.690714  [ 6420/ 7000]\n",
            "cost: 0.687838  [ 6620/ 7000]\n",
            "cost: 0.699308  [ 6820/ 7000]\n",
            "cost: 0.705051  [   20/ 7000]\n",
            "cost: 0.696398  [  220/ 7000]\n",
            "cost: 0.687837  [  420/ 7000]\n",
            "cost: 0.696397  [  620/ 7000]\n",
            "cost: 0.707708  [  820/ 7000]\n",
            "cost: 0.693527  [ 1020/ 7000]\n",
            "cost: 0.696298  [ 1220/ 7000]\n",
            "cost: 0.685199  [ 1420/ 7000]\n",
            "cost: 0.685188  [ 1620/ 7000]\n",
            "cost: 0.685172  [ 1820/ 7000]\n",
            "cost: 0.690753  [ 2020/ 7000]\n",
            "cost: 0.679501  [ 2220/ 7000]\n",
            "cost: 0.690691  [ 2420/ 7000]\n",
            "cost: 0.707850  [ 2620/ 7000]\n",
            "cost: 0.690727  [ 2820/ 7000]\n",
            "cost: 0.699207  [ 3020/ 7000]\n",
            "cost: 0.699205  [ 3220/ 7000]\n",
            "cost: 0.693534  [ 3420/ 7000]\n",
            "cost: 0.687916  [ 3620/ 7000]\n",
            "cost: 0.699230  [ 3820/ 7000]\n",
            "cost: 0.702058  [ 4020/ 7000]\n",
            "cost: 0.693556  [ 4220/ 7000]\n",
            "cost: 0.699183  [ 4420/ 7000]\n",
            "cost: 0.707641  [ 4620/ 7000]\n",
            "cost: 0.699182  [ 4820/ 7000]\n",
            "cost: 0.690702  [ 5020/ 7000]\n",
            "cost: 0.690685  [ 5220/ 7000]\n",
            "cost: 0.684957  [ 5420/ 7000]\n",
            "cost: 0.699273  [ 5620/ 7000]\n",
            "cost: 0.699213  [ 5820/ 7000]\n",
            "cost: 0.690712  [ 6020/ 7000]\n",
            "cost: 0.696328  [ 6220/ 7000]\n",
            "cost: 0.690772  [ 6420/ 7000]\n",
            "cost: 0.687978  [ 6620/ 7000]\n",
            "cost: 0.699125  [ 6820/ 7000]\n",
            "cost: 0.704708  [   20/ 7000]\n",
            "cost: 0.696301  [  220/ 7000]\n",
            "cost: 0.687972  [  420/ 7000]\n",
            "cost: 0.696296  [  620/ 7000]\n",
            "cost: 0.707300  [  820/ 7000]\n",
            "cost: 0.693506  [ 1020/ 7000]\n",
            "cost: 0.696202  [ 1220/ 7000]\n",
            "cost: 0.685402  [ 1420/ 7000]\n",
            "cost: 0.685390  [ 1620/ 7000]\n",
            "cost: 0.685371  [ 1820/ 7000]\n",
            "cost: 0.690805  [ 2020/ 7000]\n",
            "cost: 0.679845  [ 2220/ 7000]\n",
            "cost: 0.690744  [ 2420/ 7000]\n",
            "cost: 0.707464  [ 2620/ 7000]\n",
            "cost: 0.690779  [ 2820/ 7000]\n",
            "cost: 0.699047  [ 3020/ 7000]\n",
            "cost: 0.699044  [ 3220/ 7000]\n",
            "cost: 0.693514  [ 3420/ 7000]\n",
            "cost: 0.688037  [ 3620/ 7000]\n",
            "cost: 0.699069  [ 3820/ 7000]\n",
            "cost: 0.701826  [ 4020/ 7000]\n",
            "cost: 0.693536  [ 4220/ 7000]\n",
            "cost: 0.699025  [ 4420/ 7000]\n",
            "cost: 0.707275  [ 4620/ 7000]\n",
            "cost: 0.699025  [ 4820/ 7000]\n",
            "cost: 0.690752  [ 5020/ 7000]\n",
            "cost: 0.690736  [ 5220/ 7000]\n",
            "cost: 0.685147  [ 5420/ 7000]\n",
            "cost: 0.699112  [ 5620/ 7000]\n",
            "cost: 0.699055  [ 5820/ 7000]\n",
            "cost: 0.690761  [ 6020/ 7000]\n",
            "cost: 0.696243  [ 6220/ 7000]\n",
            "cost: 0.690819  [ 6420/ 7000]\n",
            "cost: 0.688091  [ 6620/ 7000]\n",
            "cost: 0.698978  [ 6820/ 7000]\n",
            "cost: 0.704436  [   20/ 7000]\n",
            "cost: 0.696212  [  220/ 7000]\n",
            "cost: 0.688079  [  420/ 7000]\n",
            "cost: 0.696217  [  620/ 7000]\n",
            "cost: 0.706959  [  820/ 7000]\n",
            "cost: 0.693490  [ 1020/ 7000]\n",
            "cost: 0.696119  [ 1220/ 7000]\n",
            "cost: 0.685577  [ 1420/ 7000]\n",
            "cost: 0.685568  [ 1620/ 7000]\n",
            "cost: 0.685556  [ 1820/ 7000]\n",
            "cost: 0.690856  [ 2020/ 7000]\n",
            "cost: 0.680165  [ 2220/ 7000]\n",
            "cost: 0.690798  [ 2420/ 7000]\n",
            "cost: 0.707094  [ 2620/ 7000]\n",
            "cost: 0.690827  [ 2820/ 7000]\n",
            "cost: 0.698891  [ 3020/ 7000]\n",
            "cost: 0.698886  [ 3220/ 7000]\n",
            "cost: 0.693491  [ 3420/ 7000]\n",
            "cost: 0.688154  [ 3620/ 7000]\n",
            "cost: 0.698920  [ 3820/ 7000]\n",
            "cost: 0.701611  [ 4020/ 7000]\n",
            "cost: 0.693518  [ 4220/ 7000]\n",
            "cost: 0.698881  [ 4420/ 7000]\n",
            "cost: 0.706941  [ 4620/ 7000]\n",
            "cost: 0.698879  [ 4820/ 7000]\n",
            "cost: 0.690798  [ 5020/ 7000]\n",
            "cost: 0.690777  [ 5220/ 7000]\n",
            "cost: 0.685311  [ 5420/ 7000]\n",
            "cost: 0.698974  [ 5620/ 7000]\n",
            "cost: 0.698920  [ 5820/ 7000]\n",
            "cost: 0.690804  [ 6020/ 7000]\n",
            "cost: 0.696170  [ 6220/ 7000]\n",
            "cost: 0.690866  [ 6420/ 7000]\n",
            "cost: 0.688194  [ 6620/ 7000]\n",
            "cost: 0.698849  [ 6820/ 7000]\n",
            "cost: 0.704183  [   20/ 7000]\n",
            "cost: 0.696145  [  220/ 7000]\n",
            "cost: 0.688187  [  420/ 7000]\n",
            "cost: 0.696144  [  620/ 7000]\n",
            "cost: 0.706663  [  820/ 7000]\n",
            "cost: 0.693476  [ 1020/ 7000]\n",
            "cost: 0.696050  [ 1220/ 7000]\n",
            "cost: 0.685720  [ 1420/ 7000]\n",
            "cost: 0.685711  [ 1620/ 7000]\n",
            "cost: 0.685698  [ 1820/ 7000]\n",
            "cost: 0.690892  [ 2020/ 7000]\n",
            "cost: 0.680410  [ 2220/ 7000]\n",
            "cost: 0.690835  [ 2420/ 7000]\n",
            "cost: 0.706818  [ 2620/ 7000]\n",
            "cost: 0.690862  [ 2820/ 7000]\n",
            "cost: 0.698771  [ 3020/ 7000]\n",
            "cost: 0.698768  [ 3220/ 7000]\n",
            "cost: 0.693480  [ 3420/ 7000]\n",
            "cost: 0.688240  [ 3620/ 7000]\n",
            "cost: 0.698802  [ 3820/ 7000]\n",
            "cost: 0.701448  [ 4020/ 7000]\n",
            "cost: 0.693505  [ 4220/ 7000]\n",
            "cost: 0.698769  [ 4420/ 7000]\n",
            "cost: 0.706686  [ 4620/ 7000]\n",
            "cost: 0.698773  [ 4820/ 7000]\n",
            "cost: 0.690833  [ 5020/ 7000]\n",
            "cost: 0.690816  [ 5220/ 7000]\n",
            "cost: 0.685444  [ 5420/ 7000]\n",
            "cost: 0.698866  [ 5620/ 7000]\n",
            "cost: 0.698811  [ 5820/ 7000]\n",
            "cost: 0.690840  [ 6020/ 7000]\n",
            "cost: 0.696109  [ 6220/ 7000]\n",
            "cost: 0.690901  [ 6420/ 7000]\n",
            "cost: 0.688272  [ 6620/ 7000]\n",
            "cost: 0.698737  [ 6820/ 7000]\n",
            "cost: 0.703983  [   20/ 7000]\n",
            "cost: 0.696087  [  220/ 7000]\n",
            "cost: 0.688268  [  420/ 7000]\n",
            "cost: 0.696086  [  620/ 7000]\n",
            "cost: 0.706422  [  820/ 7000]\n",
            "cost: 0.693464  [ 1020/ 7000]\n",
            "cost: 0.695993  [ 1220/ 7000]\n",
            "cost: 0.685841  [ 1420/ 7000]\n",
            "cost: 0.685826  [ 1620/ 7000]\n",
            "cost: 0.685812  [ 1820/ 7000]\n",
            "cost: 0.690920  [ 2020/ 7000]\n",
            "cost: 0.680600  [ 2220/ 7000]\n",
            "cost: 0.690859  [ 2420/ 7000]\n",
            "cost: 0.706628  [ 2620/ 7000]\n",
            "cost: 0.690892  [ 2820/ 7000]\n",
            "cost: 0.698684  [ 3020/ 7000]\n",
            "cost: 0.698684  [ 3220/ 7000]\n",
            "cost: 0.693470  [ 3420/ 7000]\n",
            "cost: 0.688306  [ 3620/ 7000]\n",
            "cost: 0.698715  [ 3820/ 7000]\n",
            "cost: 0.701322  [ 4020/ 7000]\n",
            "cost: 0.693494  [ 4220/ 7000]\n",
            "cost: 0.698679  [ 4420/ 7000]\n",
            "cost: 0.706479  [ 4620/ 7000]\n",
            "cost: 0.698684  [ 4820/ 7000]\n",
            "cost: 0.690862  [ 5020/ 7000]\n",
            "cost: 0.690845  [ 5220/ 7000]\n",
            "cost: 0.685551  [ 5420/ 7000]\n",
            "cost: 0.698777  [ 5620/ 7000]\n",
            "cost: 0.698723  [ 5820/ 7000]\n",
            "cost: 0.690868  [ 6020/ 7000]\n",
            "cost: 0.696058  [ 6220/ 7000]\n",
            "cost: 0.690928  [ 6420/ 7000]\n",
            "cost: 0.688337  [ 6620/ 7000]\n",
            "cost: 0.698652  [ 6820/ 7000]\n",
            "Test Error: \n",
            " Accuracy: 50.6%, Avg loss: 0.693184 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device=torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "\n",
        "data=pd.read_csv(\"corpus.csv\",encoding=\"latin-1\")\n",
        "\n",
        "tfidf  = TfidfVectorizer()\n",
        "corpus = tfidf.fit_transform(data[\"text\"])\n",
        "train_corpus, test_corpus, train_label, test_label = model_selection.train_test_split(data[\"text\"],data['label'],test_size=0.3)\n",
        "Encoder = LabelEncoder()\n",
        "train_label = Encoder.fit_transform(train_label)\n",
        "test_label = Encoder.fit_transform(test_label)\n",
        "train_corpus_tfidf=tfidf.transform(train_corpus)\n",
        "test_corpus_tfidf=tfidf.transform(test_corpus)\n",
        "\n",
        "bsize=20\n",
        "epochs=10\n",
        "lr=1e-7\n",
        "\n",
        "class dense_classifier(nn.Module):\n",
        "    def __init__(self,n):\n",
        "        super(dense_classifier,self).__init__()\n",
        "        self.n=n\n",
        "        self.network=nn.Sequential(\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(self.n,10000),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(10000,360),\n",
        "            #nn.ReLU(),\n",
        "            nn.Linear(360,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        return self.network(x)\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    def __init__(self,n,kerns,pool_kerns,out_chans):\n",
        "        super(classifier,self).__init__()\n",
        "        self.kerns=kerns\n",
        "        self.pool_kerns=pool_kerns\n",
        "        self.convs=[]\n",
        "        self.pools=[]\n",
        "        self.stride=1\n",
        "        self.n=n\n",
        "        #self.embed=nn.Embedding(n,max_length)\n",
        "        in_chan=1\n",
        "        for i in range(len(kerns)):\n",
        "            out_chan=out_chans[i]\n",
        "            kern=kerns[i]\n",
        "            pool_kern=pool_kerns[i]\n",
        "            self.convs.append(nn.Conv1d(in_chan,out_chan,kern))\n",
        "            self.compute_size(kern,self.stride)\n",
        "            self.pools.append(nn.MaxPool1d(pool_kern))\n",
        "            self.compute_size(pool_kern,pool_kern)\n",
        "            in_chan=out_chan\n",
        "        self.convs=nn.ModuleList(self.convs)\n",
        "        self.pools=nn.ModuleList(self.pools)\n",
        "        self.fc1=nn.Linear(self.n*out_chan,100)\n",
        "        self.fc2=nn.Linear(100,1)\n",
        "\n",
        "    def compute_size(self,kernel,stride):\n",
        "        self.n=(self.n-kernel)//stride+1\n",
        "        return self.n\n",
        "\n",
        "    def forward(self,x):\n",
        "        #x=self.embed(x)\n",
        "        for i in range(len(self.kerns)):\n",
        "            x=nn.ReLU()(self.convs[i](x))\n",
        "            x=self.pools[i](x)\n",
        "        x=torch.flatten(x,1)\n",
        "        x=F.relu(self.fc1(x))\n",
        "        x=self.fc2(x)\n",
        "        #x=nn.Sigmoid()(x)\n",
        "        return x\n",
        "\n",
        "class corpus(Dataset):\n",
        "    def __init__(self,corpus,label):\n",
        "        self.corpus=corpus\n",
        "        self.label=label\n",
        "    def __len__(self):\n",
        "        return len(self.corpus)\n",
        "    def __getitem__(self,idx):\n",
        "        return self.corpus[idx],self.label[idx]\n",
        "\n",
        "def train(dataloader,model,loss_fn,optimizer):\n",
        "    size=len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch,(x,y) in enumerate(dataloader):\n",
        "        x,y=x.to(device),y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        #print(torch.sum(x[0]!=0))\n",
        "        pred=model(x.reshape(bsize,1,x.shape[1]).to(device))\n",
        "        cost=loss_fn(pred.flatten(),y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "        if batch % 10 == 0:\n",
        "            cost, current = cost.item(), batch * bsize + len(x)\n",
        "            print(f\"cost: {cost:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x,y=x.to(device),y.to(device)\n",
        "            pred=model(x.reshape(bsize,1,x.shape[1]).to(device))\n",
        "            test_loss += loss_fn(pred.flatten(), y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "text=\"ajbjdfowd\"\n",
        "#input =torch.from_numpy(tfidf.transform([text]).toarray()).float()\n",
        "train_corpus_data=torch.from_numpy(train_corpus_tfidf.toarray()).float()\n",
        "test_corpus_data=torch.from_numpy(test_corpus_tfidf.toarray()).float()\n",
        "train_label_data=torch.from_numpy(train_label).float()\n",
        "test_label_data=torch.from_numpy(test_label).float()\n",
        "\n",
        "train_loader=DataLoader(corpus(train_corpus_data,train_label_data),batch_size=bsize)\n",
        "test_loader=DataLoader(corpus(test_corpus_data,test_label_data),batch_size=bsize)\n",
        "\n",
        "#input=input.reshape(1,input.shape[0],input.shape[1]\n",
        "#c=classifier(train_corpus_data.shape[1],[2,3,4,5],[2,2,2,2],[6,16,24,32])\n",
        "cnn_classifier=classifier(train_corpus_data.shape[1],[10,20],[2,2],[5,10]).to(device)\n",
        "#input=input.reshape(1,input.shape[0],input.shape[1])\n",
        "#print(c.forward(input))\n",
        "dense_classifier=dense_classifier(train_corpus_data.shape[1]).to(device)\n",
        "\n",
        "c=cnn_classifier\n",
        "\n",
        "#loss=nn.BCELoss()\n",
        "loss=nn.BCEWithLogitsLoss()\n",
        "#optim=torch.optim.SGD(c.parameters(),momentum=0.8,lr=lr)\n",
        "optim=torch.optim.Adam(c.parameters(),lr=lr)\n",
        "#optim=torch.optim.ASGD(c.parameters(),lr=lr)\n",
        "\n",
        "#print(torch.sum(train_label_data==1))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(train_loader,c,loss,optim)\n",
        "test_loop(test_loader,c,loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keras\n",
        "\n",
        "Yay, it works! (A bit of overfitting is happening, but at least the first few epochs look good.)"
      ],
      "metadata": {
        "id": "hjDMLQokJMwW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXrIZ-1tH3TX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 943
        },
        "outputId": "6e9e7c26-2a4c-4111-d71a-d5bcefd7318d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m31596\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │         \u001b[38;5;34m2,112\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_8 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m15798\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m15798\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m15783\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m32,800\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_9 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m7891\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m7891\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m7884\u001b[0m, \u001b[38;5;34m18\u001b[0m)         │         \u001b[38;5;34m4,626\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_10 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m3942\u001b[0m, \u001b[38;5;34m18\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m3942\u001b[0m, \u001b[38;5;34m18\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_6 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m70956\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m100\u001b[0m)              │     \u001b[38;5;34m7,095,700\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m50\u001b[0m)               │         \u001b[38;5;34m5,050\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m1\u001b[0m)                │            \u001b[38;5;34m51\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31596</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15798</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15798</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15783</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,800</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7891</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7891</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7884</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,626</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3942</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3942</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70956</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)              │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,095,700</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,050</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,140,339\u001b[0m (27.24 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,140,339</span> (27.24 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,140,339\u001b[0m (27.24 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,140,339</span> (27.24 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 120ms/step - accuracy: 0.6141 - loss: 0.6222 - val_accuracy: 0.8203 - val_loss: 0.3966\n",
            "Epoch 2/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 115ms/step - accuracy: 0.8457 - loss: 0.3616 - val_accuracy: 0.8360 - val_loss: 0.3698\n",
            "Epoch 3/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 115ms/step - accuracy: 0.9094 - loss: 0.2162 - val_accuracy: 0.8423 - val_loss: 0.3727\n",
            "Epoch 4/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 114ms/step - accuracy: 0.9672 - loss: 0.0913 - val_accuracy: 0.8247 - val_loss: 0.6252\n",
            "Epoch 5/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 115ms/step - accuracy: 0.9913 - loss: 0.0314 - val_accuracy: 0.8227 - val_loss: 0.8427\n",
            "Epoch 6/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 114ms/step - accuracy: 0.9973 - loss: 0.0116 - val_accuracy: 0.8273 - val_loss: 0.9651\n",
            "Epoch 7/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.9958 - loss: 0.0120 - val_accuracy: 0.8173 - val_loss: 0.9560\n",
            "Epoch 8/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 114ms/step - accuracy: 0.9957 - loss: 0.0133 - val_accuracy: 0.8210 - val_loss: 0.9922\n",
            "Epoch 9/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 114ms/step - accuracy: 0.9968 - loss: 0.0111 - val_accuracy: 0.8237 - val_loss: 1.2856\n",
            "Epoch 10/10\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 114ms/step - accuracy: 0.9983 - loss: 0.0052 - val_accuracy: 0.8207 - val_loss: 1.1225\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b2f1f5f4090>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv1D\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import InputLayer\n",
        "from keras.preprocessing import sequence\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection\n",
        "\n",
        "data=pd.read_csv(\"corpus.csv\",encoding=\"latin-1\")\n",
        "\n",
        "tfidf  = TfidfVectorizer()\n",
        "corpus = tfidf.fit_transform(data[\"text\"])\n",
        "train_corpus, test_corpus, train_label, test_label = model_selection.train_test_split(data[\"text\"],data['label'],test_size=0.3)\n",
        "Encoder = LabelEncoder()\n",
        "train_label = Encoder.fit_transform(train_label)\n",
        "test_label = Encoder.fit_transform(test_label)\n",
        "train_corpus_tfidf=tfidf.transform(train_corpus)\n",
        "test_corpus_tfidf=tfidf.transform(test_corpus)\n",
        "\n",
        "bsize=20\n",
        "epochs=10\n",
        "lr=1e-7\n",
        "\n",
        "features=train_corpus_tfidf.shape[1]\n",
        "train_corpus_tfidf=train_corpus_tfidf.toarray()\n",
        "test_corpus_tfidf=test_corpus_tfidf.toarray()\n",
        "train_corpus_tfidf=train_corpus_tfidf.reshape(train_corpus_tfidf.shape[0],train_corpus_tfidf.shape[1],1)\n",
        "test_corpus_tfidf=test_corpus_tfidf.reshape(test_corpus_tfidf.shape[0],test_corpus_tfidf.shape[1],1)\n",
        "\n",
        "model=Sequential([InputLayer(shape=(features,1),batch_size=bsize),\n",
        "                  Conv1D(64,32,activation=\"relu\"),\n",
        "                  MaxPooling1D(),\n",
        "                  Dropout(0.5),\n",
        "                  Conv1D(32,16,activation=\"relu\"),\n",
        "                  MaxPooling1D(),\n",
        "                  Dropout(0.2),\n",
        "                  Conv1D(18,8,activation=\"relu\"),\n",
        "                  MaxPooling1D(),\n",
        "                  Dropout(0.2),\n",
        "                  Flatten(),\n",
        "                  Dense(100,activation=\"relu\"),\n",
        "                  Dense(50,activation=\"relu\"),\n",
        "                  Dense(1,activation=\"sigmoid\")\n",
        "                  ])\n",
        "#model=Sequential([InputLayer(shape=(features,)),\n",
        "#                  Dense(100,activation=\"relu\"),\n",
        "#                  Dense(1,activation=\"sigmoid\")\n",
        "#                  ])\n",
        "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "model.fit(train_corpus_tfidf, train_label, epochs=epochs, batch_size=bsize, validation_data=(test_corpus_tfidf, test_label))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QyiQwKpt0gr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dntrKyWWI_f5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOh6icixfl7dcesNv0U8Uoa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}