{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nwon24/nlp/blob/main/W4/cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB8HcKBYJEsO"
      },
      "source": [
        "# CNN Using PyTorch\n",
        "\n",
        "For some reason, the cost never goes below around 0.69. :("
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo6yf9SHFUkm",
        "outputId": "ebfa2538-b09e-4844-fa85-079eb1dda34d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost: 0.694041  [   50/ 7000]\n",
            "cost: 0.692548  [  550/ 7000]\n",
            "cost: 0.693420  [ 1050/ 7000]\n",
            "cost: 0.693384  [ 1550/ 7000]\n",
            "cost: 0.693351  [ 2050/ 7000]\n",
            "cost: 0.692249  [ 2550/ 7000]\n",
            "cost: 0.693125  [ 3050/ 7000]\n",
            "cost: 0.693475  [ 3550/ 7000]\n",
            "cost: 0.693925  [ 4050/ 7000]\n",
            "cost: 0.693143  [ 4550/ 7000]\n",
            "cost: 0.693470  [ 5050/ 7000]\n",
            "cost: 0.693638  [ 5550/ 7000]\n",
            "cost: 0.693339  [ 6050/ 7000]\n",
            "cost: 0.692208  [ 6550/ 7000]\n",
            "cost: 0.693187  [   50/ 7000]\n",
            "cost: 0.692034  [  550/ 7000]\n",
            "cost: 0.692733  [ 1050/ 7000]\n",
            "cost: 0.692704  [ 1550/ 7000]\n",
            "cost: 0.692464  [ 2050/ 7000]\n",
            "cost: 0.691785  [ 2550/ 7000]\n",
            "cost: 0.692551  [ 3050/ 7000]\n",
            "cost: 0.692459  [ 3550/ 7000]\n",
            "cost: 0.692592  [ 4050/ 7000]\n",
            "cost: 0.692199  [ 4550/ 7000]\n",
            "cost: 0.691954  [ 5050/ 7000]\n",
            "cost: 0.692444  [ 5550/ 7000]\n",
            "cost: 0.692123  [ 6050/ 7000]\n",
            "cost: 0.691025  [ 6550/ 7000]\n",
            "cost: 0.691568  [   50/ 7000]\n",
            "cost: 0.690668  [  550/ 7000]\n",
            "cost: 0.691161  [ 1050/ 7000]\n",
            "cost: 0.691105  [ 1550/ 7000]\n",
            "cost: 0.690331  [ 2050/ 7000]\n",
            "cost: 0.690343  [ 2550/ 7000]\n",
            "cost: 0.691091  [ 3050/ 7000]\n",
            "cost: 0.690053  [ 3550/ 7000]\n",
            "cost: 0.689531  [ 4050/ 7000]\n",
            "cost: 0.689895  [ 4550/ 7000]\n",
            "cost: 0.688412  [ 5050/ 7000]\n",
            "cost: 0.689697  [ 5550/ 7000]\n",
            "cost: 0.689210  [ 6050/ 7000]\n",
            "cost: 0.687875  [ 6550/ 7000]\n",
            "cost: 0.687822  [   50/ 7000]\n",
            "cost: 0.687116  [  550/ 7000]\n",
            "cost: 0.687528  [ 1050/ 7000]\n",
            "cost: 0.687284  [ 1550/ 7000]\n",
            "cost: 0.685429  [ 2050/ 7000]\n",
            "cost: 0.686478  [ 2550/ 7000]\n",
            "cost: 0.687635  [ 3050/ 7000]\n",
            "cost: 0.685043  [ 3550/ 7000]\n",
            "cost: 0.683413  [ 4050/ 7000]\n",
            "cost: 0.685113  [ 4550/ 7000]\n",
            "cost: 0.681732  [ 5050/ 7000]\n",
            "cost: 0.684386  [ 5550/ 7000]\n",
            "cost: 0.683516  [ 6050/ 7000]\n",
            "cost: 0.681548  [ 6550/ 7000]\n",
            "cost: 0.680959  [   50/ 7000]\n",
            "cost: 0.680345  [  550/ 7000]\n",
            "cost: 0.681017  [ 1050/ 7000]\n",
            "cost: 0.680423  [ 1550/ 7000]\n",
            "cost: 0.676974  [ 2050/ 7000]\n",
            "cost: 0.679400  [ 2550/ 7000]\n",
            "cost: 0.681616  [ 3050/ 7000]\n",
            "cost: 0.676946  [ 3550/ 7000]\n",
            "cost: 0.673726  [ 4050/ 7000]\n",
            "cost: 0.677294  [ 4550/ 7000]\n",
            "cost: 0.671511  [ 5050/ 7000]\n",
            "cost: 0.676153  [ 5550/ 7000]\n",
            "cost: 0.674736  [ 6050/ 7000]\n",
            "cost: 0.671702  [ 6550/ 7000]\n",
            "cost: 0.670667  [   50/ 7000]\n",
            "cost: 0.670260  [  550/ 7000]\n",
            "cost: 0.671478  [ 1050/ 7000]\n",
            "cost: 0.670338  [ 1550/ 7000]\n",
            "cost: 0.664980  [ 2050/ 7000]\n",
            "cost: 0.669092  [ 2550/ 7000]\n",
            "cost: 0.673070  [ 3050/ 7000]\n",
            "cost: 0.665835  [ 3550/ 7000]\n",
            "cost: 0.660544  [ 4050/ 7000]\n",
            "cost: 0.666534  [ 4550/ 7000]\n",
            "cost: 0.657902  [ 5050/ 7000]\n",
            "cost: 0.665248  [ 5550/ 7000]\n",
            "cost: 0.663097  [ 6050/ 7000]\n",
            "cost: 0.658818  [ 6550/ 7000]\n",
            "cost: 0.657283  [   50/ 7000]\n",
            "cost: 0.657428  [  550/ 7000]\n",
            "cost: 0.659188  [ 1050/ 7000]\n",
            "cost: 0.657373  [ 1550/ 7000]\n",
            "cost: 0.650010  [ 2050/ 7000]\n",
            "cost: 0.656111  [ 2550/ 7000]\n",
            "cost: 0.662401  [ 3050/ 7000]\n",
            "cost: 0.652091  [ 3550/ 7000]\n",
            "cost: 0.644410  [ 4050/ 7000]\n",
            "cost: 0.653344  [ 4550/ 7000]\n",
            "cost: 0.641183  [ 5050/ 7000]\n",
            "cost: 0.651921  [ 5550/ 7000]\n",
            "cost: 0.648831  [ 6050/ 7000]\n",
            "cost: 0.643451  [ 6550/ 7000]\n",
            "cost: 0.641127  [   50/ 7000]\n",
            "cost: 0.642457  [  550/ 7000]\n",
            "cost: 0.644487  [ 1050/ 7000]\n",
            "cost: 0.641966  [ 1550/ 7000]\n",
            "cost: 0.632612  [ 2050/ 7000]\n",
            "cost: 0.641131  [ 2550/ 7000]\n",
            "cost: 0.649937  [ 3050/ 7000]\n",
            "cost: 0.636001  [ 3550/ 7000]\n",
            "cost: 0.625684  [ 4050/ 7000]\n",
            "cost: 0.638072  [ 4550/ 7000]\n",
            "cost: 0.621885  [ 5050/ 7000]\n",
            "cost: 0.636636  [ 5550/ 7000]\n",
            "cost: 0.632356  [ 6050/ 7000]\n",
            "cost: 0.626109  [ 6550/ 7000]\n",
            "cost: 0.622561  [   50/ 7000]\n",
            "cost: 0.625645  [  550/ 7000]\n",
            "cost: 0.627638  [ 1050/ 7000]\n",
            "cost: 0.624476  [ 1550/ 7000]\n",
            "cost: 0.613104  [ 2050/ 7000]\n",
            "cost: 0.624326  [ 2550/ 7000]\n",
            "cost: 0.635863  [ 3050/ 7000]\n",
            "cost: 0.617836  [ 3550/ 7000]\n",
            "cost: 0.604801  [ 4050/ 7000]\n",
            "cost: 0.620925  [ 4550/ 7000]\n",
            "cost: 0.600224  [ 5050/ 7000]\n",
            "cost: 0.619534  [ 5550/ 7000]\n",
            "cost: 0.613802  [ 6050/ 7000]\n",
            "cost: 0.606942  [ 6550/ 7000]\n",
            "cost: 0.601834  [   50/ 7000]\n",
            "cost: 0.607057  [  550/ 7000]\n",
            "cost: 0.608879  [ 1050/ 7000]\n",
            "cost: 0.605122  [ 1550/ 7000]\n",
            "cost: 0.591582  [ 2050/ 7000]\n",
            "cost: 0.605705  [ 2550/ 7000]\n",
            "cost: 0.620158  [ 3050/ 7000]\n",
            "cost: 0.597504  [ 3550/ 7000]\n",
            "cost: 0.581613  [ 4050/ 7000]\n",
            "cost: 0.601776  [ 4550/ 7000]\n",
            "cost: 0.575880  [ 5050/ 7000]\n",
            "cost: 0.600351  [ 5550/ 7000]\n",
            "cost: 0.592830  [ 6050/ 7000]\n",
            "cost: 0.585478  [ 6550/ 7000]\n",
            "42\n",
            "44\n",
            "44\n",
            "39\n",
            "42\n",
            "45\n",
            "44\n",
            "46\n",
            "40\n",
            "41\n",
            "44\n",
            "45\n",
            "45\n",
            "44\n",
            "42\n",
            "43\n",
            "43\n",
            "38\n",
            "44\n",
            "42\n",
            "45\n",
            "43\n",
            "45\n",
            "47\n",
            "42\n",
            "38\n",
            "45\n",
            "42\n",
            "44\n",
            "43\n",
            "43\n",
            "46\n",
            "42\n",
            "47\n",
            "40\n",
            "45\n",
            "45\n",
            "45\n",
            "43\n",
            "42\n",
            "45\n",
            "38\n",
            "45\n",
            "43\n",
            "47\n",
            "43\n",
            "41\n",
            "44\n",
            "43\n",
            "42\n",
            "41\n",
            "44\n",
            "45\n",
            "47\n",
            "39\n",
            "40\n",
            "45\n",
            "42\n",
            "40\n",
            "43\n",
            "Test Error: \n",
            " Accuracy: 86.2%, Avg loss: 0.012278 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device=torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "\n",
        "data=pd.read_csv(\"corpus.csv\",encoding=\"latin-1\")\n",
        "\n",
        "tfidf  = TfidfVectorizer()\n",
        "corpus = tfidf.fit_transform(data[\"text\"])\n",
        "train_corpus, test_corpus, train_label, test_label = model_selection.train_test_split(data[\"text\"],data[\"label\"],test_size=0.3)\n",
        "Encoder = LabelEncoder()\n",
        "train_label = Encoder.fit_transform(train_label)\n",
        "test_label = Encoder.fit_transform(test_label)\n",
        "train_corpus_tfidf=tfidf.transform(train_corpus)\n",
        "test_corpus_tfidf=tfidf.transform(test_corpus)\n",
        "\n",
        "bsize=50\n",
        "epochs=10\n",
        "lr=1e-5\n",
        "\n",
        "class dense_classifier(nn.Module):\n",
        "    def __init__(self,n):\n",
        "        super(dense_classifier,self).__init__()\n",
        "        self.n=n\n",
        "        self.network=nn.Sequential(\n",
        "            nn.Linear(self.n,360),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(360,180),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(180,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        return self.network(x)\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    def __init__(self,n,kerns,pool_kerns,out_chans):\n",
        "        super(classifier,self).__init__()\n",
        "        self.kerns=kerns\n",
        "        self.pool_kerns=pool_kerns\n",
        "        self.convs=[]\n",
        "        self.pools=[]\n",
        "        self.stride=1\n",
        "        self.n=n\n",
        "        #self.embed=nn.Embedding(n,max_length)\n",
        "        in_chan=1\n",
        "        for i in range(len(kerns)):\n",
        "            out_chan=out_chans[i]\n",
        "            kern=kerns[i]\n",
        "            pool_kern=pool_kerns[i]\n",
        "            self.convs.append(nn.Conv1d(in_chan,out_chan,kern))\n",
        "            self.compute_size(kern,self.stride)\n",
        "            self.pools.append(nn.MaxPool1d(pool_kern))\n",
        "            self.compute_size(pool_kern,pool_kern)\n",
        "            in_chan=out_chan\n",
        "        self.convs=nn.ModuleList(self.convs)\n",
        "        self.pools=nn.ModuleList(self.pools)\n",
        "        self.fc1=nn.Linear(self.n*out_chan,100)\n",
        "        self.fc2=nn.Linear(100,1)\n",
        "\n",
        "    def compute_size(self,kernel,stride):\n",
        "        self.n=(self.n-kernel)//stride+1\n",
        "        return self.n\n",
        "\n",
        "    def forward(self,x):\n",
        "        #x=self.embed(x)\n",
        "        for i in range(len(self.kerns)):\n",
        "            x=nn.ReLU()(self.convs[i](x))\n",
        "            x=self.pools[i](x)\n",
        "        x=torch.flatten(x,1)\n",
        "        x=F.relu(self.fc1(x))\n",
        "        x=self.fc2(x)\n",
        "        #x=nn.Sigmoid()(x)\n",
        "        return x\n",
        "\n",
        "class corpus(Dataset):\n",
        "    def __init__(self,corpus,label):\n",
        "        self.corpus=corpus\n",
        "        self.label=label\n",
        "    def __len__(self):\n",
        "        return len(self.corpus)\n",
        "    def __getitem__(self,idx):\n",
        "        return self.corpus[idx],self.label[idx]\n",
        "\n",
        "def train(dataloader,model,loss_fn,optimizer):\n",
        "    size=len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch,(x,y) in enumerate(dataloader):\n",
        "        x,y=x.to(device),y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        #print(torch.sum(x[0]!=0))\n",
        "        pred=model(x.reshape(bsize,1,x.shape[1]).to(device))\n",
        "        cost=loss_fn(pred.flatten(),y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "        if batch % 10 == 0:\n",
        "            cost, current = cost.item(), batch * bsize + len(x)\n",
        "            print(f\"cost: {cost:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x,y=x.to(device),y.to(device)\n",
        "            pred=model(x.reshape(bsize,1,x.shape[1]).to(device))\n",
        "            test_loss += loss_fn(pred.flatten(), y).item()\n",
        "            ncorrect = (torch.round(pred.flatten())==y).sum().item()\n",
        "            print(ncorrect)\n",
        "            correct += ncorrect\n",
        "\n",
        "    #test_loss /= num_batches\n",
        "    #correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {100*(correct/size):>0.1f}%, Avg loss: {test_loss/size:>8f} \\n\")\n",
        "\n",
        "text=\"ajbjdfowd\"\n",
        "#input =torch.from_numpy(tfidf.transform([text]).toarray()).float()\n",
        "train_corpus_data=torch.from_numpy(train_corpus_tfidf.toarray()).float()\n",
        "test_corpus_data=torch.from_numpy(test_corpus_tfidf.toarray()).float()\n",
        "train_label_data=torch.from_numpy(train_label).float()\n",
        "test_label_data=torch.from_numpy(test_label).float()\n",
        "\n",
        "train_loader=DataLoader(corpus(train_corpus_data,train_label_data),batch_size=bsize)\n",
        "test_loader=DataLoader(corpus(test_corpus_data,test_label_data),batch_size=bsize)\n",
        "\n",
        "#input=input.reshape(1,input.shape[0],input.shape[1]\n",
        "#c=classifier(train_corpus_data.shape[1],[2,3,4,5],[2,2,2,2],[6,16,24,32])\n",
        "cnn_classifier=classifier(train_corpus_data.shape[1],[10,20],[2,2],[5,10]).to(device)\n",
        "#input=input.reshape(1,input.shape[0],input.shape[1])\n",
        "#print(c.forward(input))\n",
        "dense_classifier=dense_classifier(train_corpus_data.shape[1]).to(device)\n",
        "\n",
        "c=dense_classifier\n",
        "\n",
        "loss=nn.BCELoss()\n",
        "#loss=nn.BCEWithLogitsLoss()\n",
        "#optim=torch.optim.SGD(c.parameters(),momentum=0.8,lr=lr)\n",
        "optim=torch.optim.Adam(c.parameters(),lr=lr)\n",
        "#optim=torch.optim.ASGD(c.parameters(),lr=lr)\n",
        "\n",
        "#print(torch.sum(train_label_data==1))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(train_loader,c,loss,optim)\n",
        "test_loop(test_loader,c,loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjDMLQokJMwW"
      },
      "source": [
        "# Keras\n",
        "\n",
        "Yay, it works! (A bit of overfitting is happening, but at least the first few epochs look good.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "eXrIZ-1tH3TX",
        "outputId": "fe05bd22-f5bc-476e-c730-35d357da8c80"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31625</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15812</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15812</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94872</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │        <span style=\"color: #00af00; text-decoration-color: #00af00\">94,873</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m31625\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │            \u001b[38;5;34m24\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m15812\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m15812\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m94872\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m1\u001b[0m)                │        \u001b[38;5;34m94,873\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">94,897</span> (370.69 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m94,897\u001b[0m (370.69 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">94,897</span> (370.69 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m94,897\u001b[0m (370.69 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 48ms/step - accuracy: 0.6814 - loss: 0.6697 - val_accuracy: 0.8383 - val_loss: 0.5470\n",
            "Epoch 2/4\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 43ms/step - accuracy: 0.8827 - loss: 0.4724 - val_accuracy: 0.8660 - val_loss: 0.3947\n",
            "Epoch 3/4\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 43ms/step - accuracy: 0.9170 - loss: 0.3100 - val_accuracy: 0.8690 - val_loss: 0.3317\n",
            "Epoch 4/4\n",
            "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 43ms/step - accuracy: 0.9405 - loss: 0.2226 - val_accuracy: 0.8640 - val_loss: 0.3110\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8593 - loss: 0.3091\n",
            "0.3109709620475769 0.8640000224113464\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv1D\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import InputLayer\n",
        "from keras.preprocessing import sequence\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection\n",
        "\n",
        "data=pd.read_csv(\"corpus.csv\",encoding=\"latin-1\")\n",
        "\n",
        "tfidf  = TfidfVectorizer()\n",
        "corpus = tfidf.fit_transform(data[\"text\"])\n",
        "train_corpus, test_corpus, train_label, test_label = model_selection.train_test_split(data[\"text\"],data['label'],test_size=0.3)\n",
        "Encoder = LabelEncoder()\n",
        "train_label = Encoder.fit_transform(train_label)\n",
        "test_label = Encoder.fit_transform(test_label)\n",
        "train_corpus_tfidf=tfidf.transform(train_corpus)\n",
        "test_corpus_tfidf=tfidf.transform(test_corpus)\n",
        "\n",
        "bsize=20\n",
        "epochs=4\n",
        "lr=1e-7\n",
        "\n",
        "features=train_corpus_tfidf.shape[1]\n",
        "train_corpus_tfidf=train_corpus_tfidf.toarray()\n",
        "test_corpus_tfidf=test_corpus_tfidf.toarray()\n",
        "train_corpus_tfidf=train_corpus_tfidf.reshape(train_corpus_tfidf.shape[0],train_corpus_tfidf.shape[1],1)\n",
        "test_corpus_tfidf=test_corpus_tfidf.reshape(test_corpus_tfidf.shape[0],test_corpus_tfidf.shape[1],1)\n",
        "\n",
        "model=Sequential([InputLayer(shape=(features,1),batch_size=bsize),\n",
        "                  #Conv1D(64,32,activation=\"relu\"),\n",
        "                  Conv1D(6,3,activation=\"relu\"),\n",
        "                  MaxPooling1D(),\n",
        "                  Dropout(0.5),\n",
        "                  #Conv1D(3,3,activation=\"relu\"),\n",
        "                  #MaxPooling1D(),\n",
        "                  #Dropout(0.2),\n",
        "                  #Conv1D(18,8,activation=\"relu\"),\n",
        "                  #MaxPooling1D(),\n",
        "                  #Dropout(0.2),\n",
        "                  Flatten(),\n",
        "                  #Dense(100,activation=\"relu\"),\n",
        "                  #Dense(50,activation=\"relu\"),\n",
        "                  Dense(1,activation=\"sigmoid\")\n",
        "                  ])\n",
        "#model=Sequential([InputLayer(shape=(features,)),\n",
        "#                  Dense(100,activation=\"relu\"),\n",
        "#                  Dense(1,activation=\"sigmoid\")\n",
        "#                  ])\n",
        "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "model.fit(train_corpus_tfidf, train_label, epochs=epochs, batch_size=bsize, validation_data=(test_corpus_tfidf, test_label))\n",
        "eval_loss,eval_accuracy=model.evaluate(test_corpus_tfidf,test_label)\n",
        "print(eval_loss,eval_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyiQwKpt0gr-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dntrKyWWI_f5"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwbY+WTBJBlvgutEJDI9m7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}