{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nwon24/nlp/blob/main/W4/amazon_review_polarity_csv/cnn_Amazon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB8HcKBYJEsO"
      },
      "source": [
        "# CNN Using PyTorch\n",
        "\n",
        "For some reason, the cost never goes below around 0.69. :("
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo6yf9SHFUkm",
        "outputId": "e562f02a-6375-4485-e5cb-3817d711c2bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params = 70810\n",
            "cost: 0.700127  [   50/14000]\n",
            "cost: 0.966084  [  550/14000]\n",
            "cost: 0.798464  [ 1050/14000]\n",
            "cost: 1.119800  [ 1550/14000]\n",
            "cost: 0.830485  [ 2050/14000]\n",
            "cost: 0.691831  [ 2550/14000]\n",
            "cost: 0.660004  [ 3050/14000]\n",
            "cost: 0.726754  [ 3550/14000]\n",
            "cost: 0.714737  [ 4050/14000]\n",
            "cost: 0.760426  [ 4550/14000]\n",
            "cost: 0.818406  [ 5050/14000]\n",
            "cost: 0.752084  [ 5550/14000]\n",
            "cost: 0.810581  [ 6050/14000]\n",
            "cost: 0.674271  [ 6550/14000]\n",
            "cost: 0.811731  [ 7050/14000]\n",
            "cost: 0.676185  [ 7550/14000]\n",
            "cost: 0.708537  [ 8050/14000]\n",
            "cost: 0.773656  [ 8550/14000]\n",
            "cost: 0.697883  [ 9050/14000]\n",
            "cost: 0.681019  [ 9550/14000]\n",
            "cost: 0.835877  [10050/14000]\n",
            "cost: 0.667464  [10550/14000]\n",
            "cost: 0.875347  [11050/14000]\n",
            "cost: 0.692823  [11550/14000]\n",
            "cost: 0.947867  [12050/14000]\n",
            "cost: 0.663875  [12550/14000]\n",
            "cost: 0.863540  [13050/14000]\n",
            "cost: 0.739048  [13550/14000]\n",
            "cost: 0.813286  [   50/14000]\n",
            "cost: 0.675583  [  550/14000]\n",
            "cost: 0.629764  [ 1050/14000]\n",
            "cost: 0.682934  [ 1550/14000]\n",
            "cost: 0.659353  [ 2050/14000]\n",
            "cost: 0.733334  [ 2550/14000]\n",
            "cost: 0.623724  [ 3050/14000]\n",
            "cost: 0.720540  [ 3550/14000]\n",
            "cost: 0.667638  [ 4050/14000]\n",
            "cost: 0.708611  [ 4550/14000]\n",
            "cost: 0.761061  [ 5050/14000]\n",
            "cost: 0.726876  [ 5550/14000]\n",
            "cost: 0.762129  [ 6050/14000]\n",
            "cost: 0.656739  [ 6550/14000]\n",
            "cost: 0.665689  [ 7050/14000]\n",
            "cost: 0.648027  [ 7550/14000]\n",
            "cost: 0.629404  [ 8050/14000]\n",
            "cost: 0.745187  [ 8550/14000]\n",
            "cost: 0.643370  [ 9050/14000]\n",
            "cost: 0.635758  [ 9550/14000]\n",
            "cost: 0.701488  [10050/14000]\n",
            "cost: 0.618913  [10550/14000]\n",
            "cost: 0.721640  [11050/14000]\n",
            "cost: 0.636737  [11550/14000]\n",
            "cost: 0.666523  [12050/14000]\n",
            "cost: 0.609148  [12550/14000]\n",
            "cost: 0.673264  [13050/14000]\n",
            "cost: 0.594869  [13550/14000]\n",
            "cost: 0.685504  [   50/14000]\n",
            "cost: 0.623860  [  550/14000]\n",
            "cost: 0.581270  [ 1050/14000]\n",
            "cost: 0.577404  [ 1550/14000]\n",
            "cost: 0.599226  [ 2050/14000]\n",
            "cost: 0.628575  [ 2550/14000]\n",
            "cost: 0.566862  [ 3050/14000]\n",
            "cost: 0.600839  [ 3550/14000]\n",
            "cost: 0.634859  [ 4050/14000]\n",
            "cost: 0.670142  [ 4550/14000]\n",
            "cost: 0.587838  [ 5050/14000]\n",
            "cost: 0.629839  [ 5550/14000]\n",
            "cost: 0.664763  [ 6050/14000]\n",
            "cost: 0.566112  [ 6550/14000]\n",
            "cost: 0.583258  [ 7050/14000]\n",
            "cost: 0.572780  [ 7550/14000]\n",
            "cost: 0.548988  [ 8050/14000]\n",
            "cost: 0.542215  [ 8550/14000]\n",
            "cost: 0.566934  [ 9050/14000]\n",
            "cost: 0.568539  [ 9550/14000]\n",
            "cost: 0.575138  [10050/14000]\n",
            "cost: 0.633679  [10550/14000]\n",
            "cost: 0.541509  [11050/14000]\n",
            "cost: 0.810172  [11550/14000]\n",
            "cost: 0.862527  [12050/14000]\n",
            "cost: 0.518396  [12550/14000]\n",
            "cost: 0.590571  [13050/14000]\n",
            "cost: 0.524884  [13550/14000]\n",
            "cost: 0.625311  [   50/14000]\n",
            "cost: 0.523923  [  550/14000]\n",
            "cost: 0.462189  [ 1050/14000]\n",
            "cost: 0.483124  [ 1550/14000]\n",
            "cost: 0.535599  [ 2050/14000]\n",
            "cost: 0.525983  [ 2550/14000]\n",
            "cost: 0.472865  [ 3050/14000]\n",
            "cost: 0.506600  [ 3550/14000]\n",
            "cost: 0.544917  [ 4050/14000]\n",
            "cost: 0.502535  [ 4550/14000]\n",
            "cost: 0.558057  [ 5050/14000]\n",
            "cost: 0.653682  [ 5550/14000]\n",
            "cost: 0.548337  [ 6050/14000]\n",
            "cost: 0.501601  [ 6550/14000]\n",
            "cost: 0.581161  [ 7050/14000]\n",
            "cost: 0.520285  [ 7550/14000]\n",
            "cost: 0.509008  [ 8050/14000]\n",
            "cost: 0.505174  [ 8550/14000]\n",
            "cost: 0.470801  [ 9050/14000]\n",
            "cost: 0.516256  [ 9550/14000]\n",
            "cost: 0.523721  [10050/14000]\n",
            "cost: 0.471268  [10550/14000]\n",
            "cost: 0.513285  [11050/14000]\n",
            "cost: 0.540894  [11550/14000]\n",
            "cost: 0.468331  [12050/14000]\n",
            "cost: 0.437646  [12550/14000]\n",
            "cost: 0.399275  [13050/14000]\n",
            "cost: 0.421491  [13550/14000]\n",
            "cost: 0.485677  [   50/14000]\n",
            "cost: 0.440122  [  550/14000]\n",
            "cost: 0.387409  [ 1050/14000]\n",
            "cost: 0.359177  [ 1550/14000]\n",
            "cost: 0.485794  [ 2050/14000]\n",
            "cost: 0.405016  [ 2550/14000]\n",
            "cost: 0.354518  [ 3050/14000]\n",
            "cost: 0.432894  [ 3550/14000]\n",
            "cost: 0.412360  [ 4050/14000]\n",
            "cost: 0.404725  [ 4550/14000]\n",
            "cost: 0.377717  [ 5050/14000]\n",
            "cost: 0.534449  [ 5550/14000]\n",
            "cost: 0.443041  [ 6050/14000]\n",
            "cost: 0.427622  [ 6550/14000]\n",
            "cost: 0.359164  [ 7050/14000]\n",
            "cost: 0.379100  [ 7550/14000]\n",
            "cost: 0.371644  [ 8050/14000]\n",
            "cost: 0.383067  [ 8550/14000]\n",
            "cost: 0.391934  [ 9050/14000]\n",
            "cost: 0.417237  [ 9550/14000]\n",
            "cost: 0.454255  [10050/14000]\n",
            "cost: 0.384047  [10550/14000]\n",
            "cost: 0.413766  [11050/14000]\n",
            "cost: 0.456135  [11550/14000]\n",
            "cost: 0.390998  [12050/14000]\n",
            "cost: 0.356318  [12550/14000]\n",
            "cost: 0.329851  [13050/14000]\n",
            "cost: 0.323044  [13550/14000]\n",
            "41\n",
            "40\n",
            "42\n",
            "43\n",
            "41\n",
            "42\n",
            "44\n",
            "44\n",
            "41\n",
            "40\n",
            "42\n",
            "42\n",
            "41\n",
            "40\n",
            "37\n",
            "42\n",
            "37\n",
            "40\n",
            "43\n",
            "44\n",
            "38\n",
            "38\n",
            "43\n",
            "37\n",
            "39\n",
            "39\n",
            "40\n",
            "38\n",
            "42\n",
            "43\n",
            "43\n",
            "40\n",
            "41\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "45\n",
            "42\n",
            "38\n",
            "40\n",
            "45\n",
            "43\n",
            "47\n",
            "44\n",
            "43\n",
            "43\n",
            "39\n",
            "44\n",
            "38\n",
            "38\n",
            "39\n",
            "34\n",
            "40\n",
            "40\n",
            "40\n",
            "38\n",
            "43\n",
            "42\n",
            "44\n",
            "44\n",
            "41\n",
            "44\n",
            "41\n",
            "41\n",
            "39\n",
            "41\n",
            "43\n",
            "40\n",
            "44\n",
            "41\n",
            "39\n",
            "43\n",
            "40\n",
            "41\n",
            "41\n",
            "43\n",
            "41\n",
            "42\n",
            "45\n",
            "41\n",
            "45\n",
            "41\n",
            "44\n",
            "42\n",
            "35\n",
            "40\n",
            "44\n",
            "43\n",
            "42\n",
            "39\n",
            "38\n",
            "47\n",
            "44\n",
            "41\n",
            "40\n",
            "40\n",
            "39\n",
            "41\n",
            "45\n",
            "42\n",
            "44\n",
            "40\n",
            "46\n",
            "40\n",
            "41\n",
            "44\n",
            "39\n",
            "43\n",
            "45\n",
            "42\n",
            "42\n",
            "41\n",
            "45\n",
            "43\n",
            "44\n",
            "40\n",
            "42\n",
            "43\n",
            "42\n",
            "4975 6000\n",
            "Test Error: \n",
            " Accuracy: 82.9%, Avg loss: 0.008372 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device=torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "\n",
        "data=pd.read_csv(\"xab\",encoding=\"utf-8\")\n",
        "\n",
        "tfidf  = TfidfVectorizer()\n",
        "corpus = tfidf.fit_transform(data[\"text\"])\n",
        "train_corpus, test_corpus, train_label, test_label = model_selection.train_test_split(data[\"text\"],data['label'],test_size=0.3)\n",
        "Encoder = LabelEncoder()\n",
        "train_label = Encoder.fit_transform(train_label)\n",
        "test_label = Encoder.fit_transform(test_label)\n",
        "train_corpus_tfidf=tfidf.transform(train_corpus)\n",
        "test_corpus_tfidf=tfidf.transform(test_corpus)\n",
        "\n",
        "bsize=50\n",
        "epochs=5\n",
        "lr=1e-3\n",
        "\n",
        "class fc_classifier(nn.Module):\n",
        "    def __init__(self,n):\n",
        "        super(fc_classifier,self).__init__()\n",
        "        self.n=n\n",
        "        self.network=nn.Sequential(\n",
        "            nn.Linear(self.n,360),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(360,180),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(180,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        return self.network(x)\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    def __init__(self,n,kerns,pool_kerns,out_chans):\n",
        "        super(classifier,self).__init__()\n",
        "        self.kerns=kerns\n",
        "        self.pool_kerns=pool_kerns\n",
        "        self.convs=[]\n",
        "        self.pools=[]\n",
        "        self.stride=1\n",
        "        self.n=n\n",
        "        #self.embed=nn.Embedding(n,max_length)\n",
        "        in_chan=1\n",
        "        for i in range(len(kerns)):\n",
        "            out_chan=out_chans[i]\n",
        "            kern=kerns[i]\n",
        "            pool_kern=pool_kerns[i]\n",
        "            self.convs.append(nn.Conv1d(in_chan,out_chan,kern))\n",
        "            self.compute_size(kern,self.stride)\n",
        "            self.pools.append(nn.MaxPool1d(pool_kern))\n",
        "            self.compute_size(pool_kern,pool_kern)\n",
        "            in_chan=out_chan\n",
        "        self.convs=nn.ModuleList(self.convs)\n",
        "        self.pools=nn.ModuleList(self.pools)\n",
        "        self.fc1=nn.Linear(self.n*out_chan,1)\n",
        "        #self.fc2=nn.Linear(100,1)\n",
        "\n",
        "    def compute_size(self,kernel,stride):\n",
        "        self.n=(self.n-kernel)//stride+1\n",
        "        return self.n\n",
        "\n",
        "    def forward(self,x):\n",
        "        #x=self.embed(x)\n",
        "        for i in range(len(self.kerns)):\n",
        "            x=nn.LeakyReLU()(self.convs[i](x))\n",
        "            x=self.pools[i](x)\n",
        "            x=nn.Dropout(0.2)(x)\n",
        "        x=torch.flatten(x,1)\n",
        "        #x=F.relu(self.fc1(x))\n",
        "        #x=self.fc2(x)\n",
        "        x=self.fc1(x)\n",
        "        x=nn.Sigmoid()(x)\n",
        "        return x\n",
        "\n",
        "class corpus(Dataset):\n",
        "    def __init__(self,corpus,label):\n",
        "        self.corpus=corpus\n",
        "        self.label=label\n",
        "    def __len__(self):\n",
        "        return len(self.corpus)\n",
        "    def __getitem__(self,idx):\n",
        "        return self.corpus[idx],self.label[idx]\n",
        "\n",
        "def train(dataloader,model,loss_fn,optimizer):\n",
        "    size=len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch,(x,y) in enumerate(dataloader):\n",
        "        x,y=x.to(device),y.to(device)\n",
        "        #print(torch.sum(x[0]!=0))\n",
        "        pred=model(x.reshape(bsize,1,x.shape[1]).to(device))\n",
        "        #pred=model(x)\n",
        "        cost=loss_fn(pred.flatten(),y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % 10 == 0:\n",
        "            cost, current = cost.item(), batch * bsize + len(x)\n",
        "            print(f\"cost: {cost:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x,y=x.to(device),y.to(device)\n",
        "            pred=model(x.reshape(bsize,1,x.shape[1]).to(device))\n",
        "            #pred=model(x)\n",
        "            #print(torch.round(pred.flatten()),y)\n",
        "            test_loss += loss_fn(pred.flatten(), y).item()\n",
        "            ncorrect = (torch.round(pred.flatten()) == y).sum().item()\n",
        "            print(ncorrect)\n",
        "            correct += ncorrect\n",
        "\n",
        "    print(correct,size)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct/size):>0.1f}%, Avg loss: {test_loss/size:>8f} \\n\")\n",
        "\n",
        "text=\"ajbjdfowd\"\n",
        "#input =torch.from_numpy(tfidf.transform([text]).toarray()).float()\n",
        "train_corpus_data=torch.from_numpy(train_corpus_tfidf.toarray()).float()\n",
        "test_corpus_data=torch.from_numpy(test_corpus_tfidf.toarray()).float()\n",
        "train_label_data=torch.from_numpy(train_label).float()\n",
        "test_label_data=torch.from_numpy(test_label).float()\n",
        "\n",
        "train_loader=DataLoader(corpus(train_corpus_data,train_label_data),batch_size=bsize)\n",
        "test_loader=DataLoader(corpus(test_corpus_data,test_label_data),batch_size=bsize)\n",
        "\n",
        "#input=input.reshape(1,input.shape[0],input.shape[1]\n",
        "#c=classifier(train_corpus_data.shape[1],[2,3,4,5],[2,2,2,2],[6,16,24,32])\n",
        "cnn_classifier=classifier(train_corpus_data.shape[1],[2],[2],[3]).to(device)\n",
        "#input=input.reshape(1,input.shape[0],input.shape[1])\n",
        "#print(c.forward(input))\n",
        "dense_classifier=fc_classifier(train_corpus_data.shape[1]).to(device)\n",
        "\n",
        "c=cnn_classifier\n",
        "\n",
        "loss=nn.BCELoss()\n",
        "#loss=nn.BCEWithLogitsLoss()\n",
        "#optim=torch.optim.SGD(c.parameters(),lr=lr)\n",
        "optim=torch.optim.Adam(c.parameters(),lr=lr)\n",
        "#optim=torch.optim.ASGD(c.parameters(),lr=lr)\n",
        "\n",
        "#print(torch.sum(train_label_data==1))\n",
        "total_params = sum(p.numel() for p in c.parameters() if p.requires_grad)\n",
        "print(f\"Total params = {total_params}\")\n",
        "for epoch in range(epochs):\n",
        "    train(train_loader,c,loss,optim)\n",
        "test_loop(test_loader,c,loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjDMLQokJMwW"
      },
      "source": [
        "# Keras\n",
        "\n",
        "Yay, it works! (A bit of overfitting is happening, but at least the first few epochs look good.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "eXrIZ-1tH3TX",
        "outputId": "9e110236-56fa-4563-9081-b44e13c4f951"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m47200\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │             \u001b[38;5;34m9\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m23600\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m23600\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m70800\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m)                │        \u001b[38;5;34m70,801\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70800</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │        <span style=\"color: #00af00; text-decoration-color: #00af00\">70,801</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m70,810\u001b[0m (276.60 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">70,810</span> (276.60 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m70,810\u001b[0m (276.60 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">70,810</span> (276.60 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.6585 - loss: 0.6847 - val_accuracy: 0.7866 - val_loss: 0.6317\n",
            "Epoch 2/8\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.8402 - loss: 0.5876 - val_accuracy: 0.8267 - val_loss: 0.5157\n",
            "Epoch 3/8\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.8727 - loss: 0.4515 - val_accuracy: 0.8347 - val_loss: 0.4394\n",
            "Epoch 4/8\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.8957 - loss: 0.3603 - val_accuracy: 0.8391 - val_loss: 0.3973\n",
            "Epoch 5/8\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.9168 - loss: 0.2960 - val_accuracy: 0.8414 - val_loss: 0.3749\n",
            "Epoch 6/8\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9275 - loss: 0.2482 - val_accuracy: 0.8434 - val_loss: 0.3613\n",
            "Epoch 7/8\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9346 - loss: 0.2169 - val_accuracy: 0.8472 - val_loss: 0.3550\n",
            "Epoch 8/8\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9419 - loss: 0.1913 - val_accuracy: 0.8428 - val_loss: 0.3533\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8426 - loss: 0.3610\n",
            "0.3533210754394531 0.8427500128746033\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv1D\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import InputLayer\n",
        "from keras.preprocessing import sequence\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection\n",
        "\n",
        "\n",
        "data=pd.read_csv(\"xab\",encoding=\"utf-8\")\n",
        "\n",
        "tfidf  = TfidfVectorizer()\n",
        "corpus = tfidf.fit_transform(data[\"text\"])\n",
        "train_corpus, test_corpus, train_label, test_label = model_selection.train_test_split(data[\"text\"],data[\"label\"],test_size=0.4)\n",
        "Encoder = LabelEncoder()\n",
        "train_label = Encoder.fit_transform(train_label)\n",
        "test_label = Encoder.fit_transform(test_label)\n",
        "train_corpus_tfidf=tfidf.transform(train_corpus)\n",
        "test_corpus_tfidf=tfidf.transform(test_corpus)\n",
        "\n",
        "bsize=32\n",
        "epochs=8\n",
        "lr=1e-5\n",
        "\n",
        "features=train_corpus_tfidf.shape[1]\n",
        "train_corpus_tfidf=train_corpus_tfidf.toarray()\n",
        "test_corpus_tfidf=test_corpus_tfidf.toarray()\n",
        "train_corpus_tfidf=train_corpus_tfidf.reshape(train_corpus_tfidf.shape[0],train_corpus_tfidf.shape[1],1)\n",
        "test_corpus_tfidf=test_corpus_tfidf.reshape(test_corpus_tfidf.shape[0],test_corpus_tfidf.shape[1],1)\n",
        "\n",
        "model=Sequential([InputLayer(shape=(features,1),batch_size=bsize),\n",
        "                  Conv1D(3,2,activation=\"relu\"),\n",
        "                  MaxPooling1D(),\n",
        "                  Dropout(0.2),\n",
        "                  #Conv1D(32,16,activation=\"relu\"),\n",
        "                  #MaxPooling1D(),\n",
        "                  #Dropout(0.2),\n",
        "                  #Conv1D(3,2,activation=\"relu\"),\n",
        "                  #MaxPooling1D(),\n",
        "                  #Dropout(0.2),\n",
        "                  #Conv1D(36,4,activation=\"relu\"),\n",
        "                  #MaxPooling1D(),\n",
        "                  #Dropout(0.2),\n",
        "                  #Conv1D(18,8,activation=\"relu\"),\n",
        "                  #MaxPooling1D(),\n",
        "                  #Dropout(0.2),\n",
        "                  Flatten(),\n",
        "                  #Dense(100,activation=\"relu\"),\n",
        "                  #Dense(50,activation=\"relu\"),\n",
        "                  Dense(1,activation=\"sigmoid\")\n",
        "                  ])\n",
        "#model=Sequential([InputLayer(shape=(features,)),\n",
        "#                  Dense(100,activation=\"relu\"),\n",
        "#                  Dense(1,activation=\"sigmoid\")\n",
        "#                  ])\n",
        "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "model.fit(train_corpus_tfidf, train_label, epochs=epochs, batch_size=bsize, validation_data=(test_corpus_tfidf, test_label),shuffle=True)\n",
        "eval_loss,eval_accuracy=model.evaluate(test_corpus_tfidf,test_label)\n",
        "print(eval_loss,eval_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyiQwKpt0gr-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dntrKyWWI_f5"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNCjxLqNu/yzMSI6vY0D0La",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}