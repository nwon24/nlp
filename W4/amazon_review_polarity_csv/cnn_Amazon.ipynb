{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nwon24/nlp/blob/main/W4/amazon_review_polarity_csv/cnn_Amazon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB8HcKBYJEsO"
      },
      "source": [
        "# CNN Using PyTorch\n",
        "\n",
        "For some reason, the cost never goes below around 0.69. :("
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo6yf9SHFUkm",
        "outputId": "5a75d94c-a7b0-43f0-927d-2b7ec91affcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params = 17057881\n",
            "cost: 0.695346  [   50/14000]\n",
            "cost: 0.694246  [  550/14000]\n",
            "cost: 0.689062  [ 1050/14000]\n",
            "cost: 0.695261  [ 1550/14000]\n",
            "cost: 0.695199  [ 2050/14000]\n",
            "cost: 0.694643  [ 2550/14000]\n",
            "cost: 0.694058  [ 3050/14000]\n",
            "cost: 0.692053  [ 3550/14000]\n",
            "cost: 0.692522  [ 4050/14000]\n",
            "cost: 0.692008  [ 4550/14000]\n",
            "cost: 0.694612  [ 5050/14000]\n",
            "cost: 0.693052  [ 5550/14000]\n",
            "cost: 0.696089  [ 6050/14000]\n",
            "cost: 0.697394  [ 6550/14000]\n",
            "cost: 0.693950  [ 7050/14000]\n",
            "cost: 0.692403  [ 7550/14000]\n",
            "cost: 0.692849  [ 8050/14000]\n",
            "cost: 0.691841  [ 8550/14000]\n",
            "cost: 0.696042  [ 9050/14000]\n",
            "cost: 0.692717  [ 9550/14000]\n",
            "cost: 0.695420  [10050/14000]\n",
            "cost: 0.691160  [10550/14000]\n",
            "cost: 0.691194  [11050/14000]\n",
            "cost: 0.692793  [11550/14000]\n",
            "cost: 0.694160  [12050/14000]\n",
            "cost: 0.693166  [12550/14000]\n",
            "cost: 0.693623  [13050/14000]\n",
            "cost: 0.692750  [13550/14000]\n",
            "cost: 0.693210  [   50/14000]\n",
            "cost: 0.692391  [  550/14000]\n",
            "cost: 0.689135  [ 1050/14000]\n",
            "cost: 0.692891  [ 1550/14000]\n",
            "cost: 0.692444  [ 2050/14000]\n",
            "cost: 0.691947  [ 2550/14000]\n",
            "cost: 0.691528  [ 3050/14000]\n",
            "cost: 0.690495  [ 3550/14000]\n",
            "cost: 0.690722  [ 4050/14000]\n",
            "cost: 0.690411  [ 4550/14000]\n",
            "cost: 0.691554  [ 5050/14000]\n",
            "cost: 0.690848  [ 5550/14000]\n",
            "cost: 0.692638  [ 6050/14000]\n",
            "cost: 0.692610  [ 6550/14000]\n",
            "cost: 0.690435  [ 7050/14000]\n",
            "cost: 0.689454  [ 7550/14000]\n",
            "cost: 0.688865  [ 8050/14000]\n",
            "cost: 0.688069  [ 8550/14000]\n",
            "cost: 0.690182  [ 9050/14000]\n",
            "cost: 0.688443  [ 9550/14000]\n",
            "cost: 0.689750  [10050/14000]\n",
            "cost: 0.686412  [10550/14000]\n",
            "cost: 0.686227  [11050/14000]\n",
            "cost: 0.686538  [11550/14000]\n",
            "cost: 0.687601  [12050/14000]\n",
            "cost: 0.686391  [12550/14000]\n",
            "cost: 0.687680  [13050/14000]\n",
            "cost: 0.686447  [13550/14000]\n",
            "cost: 0.684365  [   50/14000]\n",
            "cost: 0.683830  [  550/14000]\n",
            "cost: 0.684034  [ 1050/14000]\n",
            "cost: 0.683345  [ 1550/14000]\n",
            "cost: 0.681622  [ 2050/14000]\n",
            "cost: 0.680882  [ 2550/14000]\n",
            "cost: 0.680916  [ 3050/14000]\n",
            "cost: 0.682180  [ 3550/14000]\n",
            "cost: 0.681984  [ 4050/14000]\n",
            "cost: 0.682359  [ 4550/14000]\n",
            "cost: 0.680447  [ 5050/14000]\n",
            "cost: 0.681954  [ 5550/14000]\n",
            "cost: 0.682507  [ 6050/14000]\n",
            "cost: 0.679673  [ 6550/14000]\n",
            "cost: 0.678650  [ 7050/14000]\n",
            "cost: 0.678776  [ 7550/14000]\n",
            "cost: 0.675406  [ 8050/14000]\n",
            "cost: 0.674749  [ 8550/14000]\n",
            "cost: 0.674933  [ 9050/14000]\n",
            "cost: 0.675359  [ 9550/14000]\n",
            "cost: 0.675554  [10050/14000]\n",
            "cost: 0.671804  [10550/14000]\n",
            "cost: 0.671298  [11050/14000]\n",
            "cost: 0.670059  [11550/14000]\n",
            "cost: 0.671510  [12050/14000]\n",
            "cost: 0.669844  [12550/14000]\n",
            "cost: 0.674075  [13050/14000]\n",
            "cost: 0.671014  [13550/14000]\n",
            "cost: 0.665402  [   50/14000]\n",
            "cost: 0.665404  [  550/14000]\n",
            "cost: 0.669493  [ 1050/14000]\n",
            "cost: 0.664289  [ 1550/14000]\n",
            "cost: 0.660497  [ 2050/14000]\n",
            "cost: 0.659338  [ 2550/14000]\n",
            "cost: 0.660260  [ 3050/14000]\n",
            "cost: 0.665002  [ 3550/14000]\n",
            "cost: 0.664206  [ 4050/14000]\n",
            "cost: 0.666154  [ 4550/14000]\n",
            "cost: 0.660031  [ 5050/14000]\n",
            "cost: 0.665335  [ 5550/14000]\n",
            "cost: 0.665164  [ 6050/14000]\n",
            "cost: 0.658733  [ 6550/14000]\n",
            "cost: 0.658301  [ 7050/14000]\n",
            "cost: 0.660558  [ 7550/14000]\n",
            "cost: 0.652711  [ 8050/14000]\n",
            "cost: 0.652262  [ 8550/14000]\n",
            "cost: 0.651287  [ 9050/14000]\n",
            "cost: 0.653974  [ 9550/14000]\n",
            "cost: 0.654249  [10050/14000]\n",
            "cost: 0.648587  [10550/14000]\n",
            "cost: 0.648082  [11050/14000]\n",
            "cost: 0.645459  [11550/14000]\n",
            "cost: 0.647708  [12050/14000]\n",
            "cost: 0.645281  [12550/14000]\n",
            "cost: 0.654310  [13050/14000]\n",
            "cost: 0.647893  [13550/14000]\n",
            "cost: 0.638609  [   50/14000]\n",
            "cost: 0.639880  [  550/14000]\n",
            "cost: 0.647241  [ 1050/14000]\n",
            "cost: 0.638244  [ 1550/14000]\n",
            "cost: 0.631884  [ 2050/14000]\n",
            "cost: 0.630349  [ 2550/14000]\n",
            "cost: 0.632480  [ 3050/14000]\n",
            "cost: 0.640983  [ 3550/14000]\n",
            "cost: 0.639646  [ 4050/14000]\n",
            "cost: 0.643617  [ 4550/14000]\n",
            "cost: 0.632723  [ 5050/14000]\n",
            "cost: 0.642664  [ 5550/14000]\n",
            "cost: 0.642180  [ 6050/14000]\n",
            "cost: 0.631829  [ 6550/14000]\n",
            "cost: 0.631681  [ 7050/14000]\n",
            "cost: 0.637400  [ 7550/14000]\n",
            "cost: 0.623670  [ 8050/14000]\n",
            "cost: 0.623349  [ 8550/14000]\n",
            "cost: 0.621067  [ 9050/14000]\n",
            "cost: 0.626475  [ 9550/14000]\n",
            "cost: 0.627353  [10050/14000]\n",
            "cost: 0.619935  [10550/14000]\n",
            "cost: 0.619739  [11050/14000]\n",
            "cost: 0.615079  [11550/14000]\n",
            "cost: 0.618187  [12050/14000]\n",
            "cost: 0.615158  [12550/14000]\n",
            "cost: 0.629433  [13050/14000]\n",
            "cost: 0.619044  [13550/14000]\n",
            "cost: 0.605535  [   50/14000]\n",
            "cost: 0.608620  [  550/14000]\n",
            "cost: 0.620259  [ 1050/14000]\n",
            "cost: 0.606107  [ 1550/14000]\n",
            "cost: 0.596933  [ 2050/14000]\n",
            "cost: 0.595235  [ 2550/14000]\n",
            "cost: 0.599070  [ 3050/14000]\n",
            "cost: 0.611385  [ 3550/14000]\n",
            "cost: 0.609964  [ 4050/14000]\n",
            "cost: 0.616099  [ 4550/14000]\n",
            "cost: 0.599547  [ 5050/14000]\n",
            "cost: 0.614815  [ 5550/14000]\n",
            "cost: 0.613726  [ 6050/14000]\n",
            "cost: 0.599058  [ 6550/14000]\n",
            "cost: 0.599598  [ 7050/14000]\n",
            "cost: 0.609443  [ 7550/14000]\n",
            "cost: 0.588905  [ 8050/14000]\n",
            "cost: 0.588718  [ 8550/14000]\n",
            "cost: 0.584777  [ 9050/14000]\n",
            "cost: 0.593413  [ 9550/14000]\n",
            "cost: 0.595404  [10050/14000]\n",
            "cost: 0.585833  [10550/14000]\n",
            "cost: 0.586232  [11050/14000]\n",
            "cost: 0.578742  [11550/14000]\n",
            "cost: 0.583609  [12050/14000]\n",
            "cost: 0.579703  [12550/14000]\n",
            "cost: 0.599588  [13050/14000]\n",
            "cost: 0.584708  [13550/14000]\n",
            "cost: 0.566844  [   50/14000]\n",
            "cost: 0.572018  [  550/14000]\n",
            "cost: 0.588186  [ 1050/14000]\n",
            "cost: 0.568245  [ 1550/14000]\n",
            "cost: 0.556320  [ 2050/14000]\n",
            "cost: 0.554615  [ 2550/14000]\n",
            "cost: 0.560199  [ 3050/14000]\n",
            "cost: 0.576362  [ 3550/14000]\n",
            "cost: 0.575639  [ 4050/14000]\n",
            "cost: 0.583769  [ 4550/14000]\n",
            "cost: 0.561604  [ 5050/14000]\n",
            "cost: 0.582202  [ 5550/14000]\n",
            "cost: 0.580892  [ 6050/14000]\n",
            "cost: 0.561686  [ 6550/14000]\n",
            "cost: 0.562859  [ 7050/14000]\n",
            "cost: 0.577008  [ 7550/14000]\n",
            "cost: 0.549276  [ 8050/14000]\n",
            "cost: 0.549203  [ 8550/14000]\n",
            "cost: 0.543877  [ 9050/14000]\n",
            "cost: 0.556000  [ 9550/14000]\n",
            "cost: 0.559820  [10050/14000]\n",
            "cost: 0.547150  [10550/14000]\n",
            "cost: 0.548411  [11050/14000]\n",
            "cost: 0.538037  [11550/14000]\n",
            "cost: 0.545421  [12050/14000]\n",
            "cost: 0.540526  [12550/14000]\n",
            "cost: 0.566245  [13050/14000]\n",
            "cost: 0.546179  [13550/14000]\n",
            "cost: 0.524601  [   50/14000]\n",
            "cost: 0.531886  [  550/14000]\n",
            "cost: 0.551660  [ 1050/14000]\n",
            "cost: 0.526627  [ 1550/14000]\n",
            "cost: 0.512134  [ 2050/14000]\n",
            "cost: 0.510331  [ 2550/14000]\n",
            "cost: 0.517553  [ 3050/14000]\n",
            "cost: 0.537113  [ 3550/14000]\n",
            "cost: 0.537844  [ 4050/14000]\n",
            "cost: 0.547684  [ 4550/14000]\n",
            "cost: 0.520747  [ 5050/14000]\n",
            "cost: 0.546026  [ 5550/14000]\n",
            "cost: 0.545495  [ 6050/14000]\n",
            "cost: 0.521956  [ 6550/14000]\n",
            "cost: 0.522928  [ 7050/14000]\n",
            "cost: 0.541309  [ 7550/14000]\n",
            "cost: 0.506413  [ 8050/14000]\n",
            "cost: 0.506214  [ 8550/14000]\n",
            "cost: 0.500236  [ 9050/14000]\n",
            "cost: 0.515693  [ 9550/14000]\n",
            "cost: 0.522131  [10050/14000]\n",
            "cost: 0.505223  [10550/14000]\n",
            "cost: 0.507555  [11050/14000]\n",
            "cost: 0.494955  [11550/14000]\n",
            "cost: 0.505124  [12050/14000]\n",
            "cost: 0.499274  [12550/14000]\n",
            "cost: 0.530848  [13050/14000]\n",
            "cost: 0.505110  [13550/14000]\n",
            "cost: 0.480660  [   50/14000]\n",
            "cost: 0.490004  [  550/14000]\n",
            "cost: 0.511691  [ 1050/14000]\n",
            "cost: 0.483265  [ 1550/14000]\n",
            "cost: 0.466615  [ 2050/14000]\n",
            "cost: 0.464630  [ 2550/14000]\n",
            "cost: 0.473408  [ 3050/14000]\n",
            "cost: 0.495273  [ 3550/14000]\n",
            "cost: 0.498278  [ 4050/14000]\n",
            "cost: 0.509371  [ 4550/14000]\n",
            "cost: 0.479072  [ 5050/14000]\n",
            "cost: 0.507914  [ 5550/14000]\n",
            "cost: 0.509545  [ 6050/14000]\n",
            "cost: 0.482312  [ 6550/14000]\n",
            "cost: 0.481720  [ 7050/14000]\n",
            "cost: 0.504232  [ 7550/14000]\n",
            "cost: 0.462534  [ 8050/14000]\n",
            "cost: 0.461868  [ 8550/14000]\n",
            "cost: 0.456283  [ 9050/14000]\n",
            "cost: 0.474696  [ 9550/14000]\n",
            "cost: 0.484612  [10050/14000]\n",
            "cost: 0.462105  [10550/14000]\n",
            "cost: 0.465651  [11050/14000]\n",
            "cost: 0.451872  [11550/14000]\n",
            "cost: 0.464650  [12050/14000]\n",
            "cost: 0.457902  [12550/14000]\n",
            "cost: 0.495068  [13050/14000]\n",
            "cost: 0.463572  [13550/14000]\n",
            "cost: 0.437002  [   50/14000]\n",
            "cost: 0.448250  [  550/14000]\n",
            "cost: 0.469817  [ 1050/14000]\n",
            "cost: 0.439880  [ 1550/14000]\n",
            "cost: 0.421634  [ 2050/14000]\n",
            "cost: 0.419380  [ 2550/14000]\n",
            "cost: 0.429333  [ 3050/14000]\n",
            "cost: 0.451926  [ 3550/14000]\n",
            "cost: 0.458132  [ 4050/14000]\n",
            "cost: 0.469748  [ 4550/14000]\n",
            "cost: 0.437705  [ 5050/14000]\n",
            "cost: 0.468870  [ 5550/14000]\n",
            "cost: 0.473974  [ 6050/14000]\n",
            "cost: 0.443773  [ 6550/14000]\n",
            "cost: 0.440234  [ 7050/14000]\n",
            "cost: 0.466587  [ 7550/14000]\n",
            "cost: 0.418684  [ 8050/14000]\n",
            "cost: 0.417213  [ 8550/14000]\n",
            "cost: 0.412789  [ 9050/14000]\n",
            "cost: 0.433810  [ 9550/14000]\n",
            "cost: 0.448030  [10050/14000]\n",
            "cost: 0.418720  [10550/14000]\n",
            "cost: 0.423657  [11050/14000]\n",
            "cost: 0.410060  [11550/14000]\n",
            "cost: 0.424968  [12050/14000]\n",
            "cost: 0.417436  [12550/14000]\n",
            "cost: 0.459850  [13050/14000]\n",
            "cost: 0.422901  [13550/14000]\n",
            "45\n",
            "42\n",
            "43\n",
            "41\n",
            "47\n",
            "43\n",
            "41\n",
            "41\n",
            "42\n",
            "45\n",
            "45\n",
            "41\n",
            "44\n",
            "44\n",
            "39\n",
            "41\n",
            "40\n",
            "43\n",
            "43\n",
            "42\n",
            "45\n",
            "41\n",
            "39\n",
            "39\n",
            "42\n",
            "42\n",
            "42\n",
            "38\n",
            "38\n",
            "41\n",
            "48\n",
            "44\n",
            "38\n",
            "45\n",
            "39\n",
            "38\n",
            "39\n",
            "40\n",
            "46\n",
            "43\n",
            "41\n",
            "41\n",
            "42\n",
            "44\n",
            "39\n",
            "44\n",
            "43\n",
            "41\n",
            "39\n",
            "43\n",
            "44\n",
            "46\n",
            "41\n",
            "42\n",
            "39\n",
            "40\n",
            "43\n",
            "44\n",
            "43\n",
            "39\n",
            "40\n",
            "42\n",
            "37\n",
            "41\n",
            "34\n",
            "41\n",
            "41\n",
            "44\n",
            "43\n",
            "41\n",
            "42\n",
            "45\n",
            "43\n",
            "43\n",
            "43\n",
            "39\n",
            "42\n",
            "42\n",
            "39\n",
            "40\n",
            "39\n",
            "43\n",
            "44\n",
            "43\n",
            "38\n",
            "45\n",
            "38\n",
            "40\n",
            "45\n",
            "33\n",
            "41\n",
            "45\n",
            "38\n",
            "45\n",
            "44\n",
            "42\n",
            "42\n",
            "48\n",
            "40\n",
            "35\n",
            "42\n",
            "43\n",
            "41\n",
            "41\n",
            "42\n",
            "44\n",
            "39\n",
            "43\n",
            "37\n",
            "43\n",
            "40\n",
            "43\n",
            "39\n",
            "46\n",
            "38\n",
            "36\n",
            "42\n",
            "44\n",
            "44\n",
            "39\n",
            "4993 6000\n",
            "Test Error: \n",
            " Accuracy: 83.2%, Avg loss: 0.009698 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device=torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "\n",
        "data=pd.read_csv(\"xab\",encoding=\"utf-8\")\n",
        "\n",
        "tfidf  = TfidfVectorizer()\n",
        "corpus = tfidf.fit_transform(data[\"text\"])\n",
        "train_corpus, test_corpus, train_label, test_label = model_selection.train_test_split(data[\"text\"],data['label'],test_size=0.3)\n",
        "Encoder = LabelEncoder()\n",
        "train_label = Encoder.fit_transform(train_label)\n",
        "test_label = Encoder.fit_transform(test_label)\n",
        "train_corpus_tfidf=tfidf.transform(train_corpus)\n",
        "test_corpus_tfidf=tfidf.transform(test_corpus)\n",
        "\n",
        "bsize=50\n",
        "epochs=10\n",
        "lr=1e-5\n",
        "\n",
        "class fc_classifier(nn.Module):\n",
        "    def __init__(self,n):\n",
        "        super(fc_classifier,self).__init__()\n",
        "        self.n=n\n",
        "        self.network=nn.Sequential(\n",
        "            nn.Linear(self.n,360),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(360,180),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(180,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        return self.network(x)\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    def __init__(self,n,kerns,pool_kerns,out_chans):\n",
        "        super(classifier,self).__init__()\n",
        "        self.kerns=kerns\n",
        "        self.pool_kerns=pool_kerns\n",
        "        self.convs=[]\n",
        "        self.pools=[]\n",
        "        self.stride=1\n",
        "        self.n=n\n",
        "        #self.embed=nn.Embedding(n,max_length)\n",
        "        in_chan=1\n",
        "        for i in range(len(kerns)):\n",
        "            out_chan=out_chans[i]\n",
        "            kern=kerns[i]\n",
        "            pool_kern=pool_kerns[i]\n",
        "            self.convs.append(nn.Conv1d(in_chan,out_chan,kern))\n",
        "            self.compute_size(kern,self.stride)\n",
        "            self.pools.append(nn.MaxPool1d(pool_kern))\n",
        "            self.compute_size(pool_kern,pool_kern)\n",
        "            in_chan=out_chan\n",
        "        self.convs=nn.ModuleList(self.convs)\n",
        "        self.pools=nn.ModuleList(self.pools)\n",
        "        self.fc1=nn.Linear(self.n*out_chan,1)\n",
        "        #self.fc2=nn.Linear(100,1)\n",
        "\n",
        "    def compute_size(self,kernel,stride):\n",
        "        self.n=(self.n-kernel)//stride+1\n",
        "        return self.n\n",
        "\n",
        "    def forward(self,x):\n",
        "        #x=self.embed(x)\n",
        "        for i in range(len(self.kerns)):\n",
        "            x=nn.LeakyReLU()(self.convs[i](x))\n",
        "            x=self.pools[i](x)\n",
        "            x=nn.Dropout(0.2)(x)\n",
        "        x=torch.flatten(x,1)\n",
        "        #x=F.relu(self.fc1(x))\n",
        "        #x=self.fc2(x)\n",
        "        x=self.fc1(x)\n",
        "        x=nn.Sigmoid()(x)\n",
        "        return x\n",
        "\n",
        "class corpus(Dataset):\n",
        "    def __init__(self,corpus,label):\n",
        "        self.corpus=corpus\n",
        "        self.label=label\n",
        "    def __len__(self):\n",
        "        return len(self.corpus)\n",
        "    def __getitem__(self,idx):\n",
        "        return self.corpus[idx],self.label[idx]\n",
        "\n",
        "def train(dataloader,model,loss_fn,optimizer):\n",
        "    size=len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch,(x,y) in enumerate(dataloader):\n",
        "        x,y=x.to(device),y.to(device)\n",
        "        #print(torch.sum(x[0]!=0))\n",
        "        pred=model(x.reshape(bsize,1,x.shape[1]).to(device))\n",
        "        #pred=model(x)\n",
        "        cost=loss_fn(pred.flatten(),y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % 10 == 0:\n",
        "            cost, current = cost.item(), batch * bsize + len(x)\n",
        "            print(f\"cost: {cost:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x,y=x.to(device),y.to(device)\n",
        "            pred=model(x.reshape(bsize,1,x.shape[1]).to(device))\n",
        "            #pred=model(x)\n",
        "            #print(torch.round(pred.flatten()),y)\n",
        "            test_loss += loss_fn(pred.flatten(), y).item()\n",
        "            ncorrect = (torch.round(pred.flatten()) == y).sum().item()\n",
        "            print(ncorrect)\n",
        "            correct += ncorrect\n",
        "\n",
        "    print(correct,size)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct/size):>0.1f}%, Avg loss: {test_loss/size:>8f} \\n\")\n",
        "\n",
        "text=\"ajbjdfowd\"\n",
        "#input =torch.from_numpy(tfidf.transform([text]).toarray()).float()\n",
        "train_corpus_data=torch.from_numpy(train_corpus_tfidf.toarray()).float()\n",
        "test_corpus_data=torch.from_numpy(test_corpus_tfidf.toarray()).float()\n",
        "train_label_data=torch.from_numpy(train_label).float()\n",
        "test_label_data=torch.from_numpy(test_label).float()\n",
        "\n",
        "train_loader=DataLoader(corpus(train_corpus_data,train_label_data),batch_size=bsize)\n",
        "test_loader=DataLoader(corpus(test_corpus_data,test_label_data),batch_size=bsize)\n",
        "\n",
        "#input=input.reshape(1,input.shape[0],input.shape[1]\n",
        "#c=classifier(train_corpus_data.shape[1],[2,3,4,5],[2,2,2,2],[6,16,24,32])\n",
        "cnn_classifier=classifier(train_corpus_data.shape[1],[2],[2],[3]).to(device)\n",
        "#input=input.reshape(1,input.shape[0],input.shape[1])\n",
        "#print(c.forward(input))\n",
        "dense_classifier=fc_classifier(train_corpus_data.shape[1]).to(device)\n",
        "\n",
        "c=dense_classifier\n",
        "\n",
        "loss=nn.BCELoss()\n",
        "#loss=nn.BCEWithLogitsLoss()\n",
        "#optim=torch.optim.SGD(c.parameters(),momentum=0.8,lr=lr)\n",
        "optim=torch.optim.Adam(c.parameters(),lr=lr)\n",
        "#optim=torch.optim.ASGD(c.parameters(),lr=lr)\n",
        "\n",
        "#print(torch.sum(train_label_data==1))\n",
        "total_params = sum(p.numel() for p in c.parameters() if p.requires_grad)\n",
        "print(f\"Total params = {total_params}\")\n",
        "for epoch in range(epochs):\n",
        "    train(train_loader,c,loss,optim)\n",
        "test_loop(test_loader,c,loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjDMLQokJMwW"
      },
      "source": [
        "# Keras\n",
        "\n",
        "Yay, it works! (A bit of overfitting is happening, but at least the first few epochs look good.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "eXrIZ-1tH3TX",
        "outputId": "9e110236-56fa-4563-9081-b44e13c4f951"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m47200\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │             \u001b[38;5;34m9\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m23600\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m23600\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m70800\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m)                │        \u001b[38;5;34m70,801\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70800</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │        <span style=\"color: #00af00; text-decoration-color: #00af00\">70,801</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m70,810\u001b[0m (276.60 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">70,810</span> (276.60 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m70,810\u001b[0m (276.60 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">70,810</span> (276.60 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.6585 - loss: 0.6847 - val_accuracy: 0.7866 - val_loss: 0.6317\n",
            "Epoch 2/8\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.8402 - loss: 0.5876 - val_accuracy: 0.8267 - val_loss: 0.5157\n",
            "Epoch 3/8\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.8727 - loss: 0.4515 - val_accuracy: 0.8347 - val_loss: 0.4394\n",
            "Epoch 4/8\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.8957 - loss: 0.3603 - val_accuracy: 0.8391 - val_loss: 0.3973\n",
            "Epoch 5/8\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.9168 - loss: 0.2960 - val_accuracy: 0.8414 - val_loss: 0.3749\n",
            "Epoch 6/8\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9275 - loss: 0.2482 - val_accuracy: 0.8434 - val_loss: 0.3613\n",
            "Epoch 7/8\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9346 - loss: 0.2169 - val_accuracy: 0.8472 - val_loss: 0.3550\n",
            "Epoch 8/8\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9419 - loss: 0.1913 - val_accuracy: 0.8428 - val_loss: 0.3533\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8426 - loss: 0.3610\n",
            "0.3533210754394531 0.8427500128746033\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv1D\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import InputLayer\n",
        "from keras.preprocessing import sequence\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection\n",
        "\n",
        "\n",
        "data=pd.read_csv(\"xab\",encoding=\"utf-8\")\n",
        "\n",
        "tfidf  = TfidfVectorizer()\n",
        "corpus = tfidf.fit_transform(data[\"text\"])\n",
        "train_corpus, test_corpus, train_label, test_label = model_selection.train_test_split(data[\"text\"],data[\"label\"],test_size=0.4)\n",
        "Encoder = LabelEncoder()\n",
        "train_label = Encoder.fit_transform(train_label)\n",
        "test_label = Encoder.fit_transform(test_label)\n",
        "train_corpus_tfidf=tfidf.transform(train_corpus)\n",
        "test_corpus_tfidf=tfidf.transform(test_corpus)\n",
        "\n",
        "bsize=32\n",
        "epochs=8\n",
        "lr=1e-5\n",
        "\n",
        "features=train_corpus_tfidf.shape[1]\n",
        "train_corpus_tfidf=train_corpus_tfidf.toarray()\n",
        "test_corpus_tfidf=test_corpus_tfidf.toarray()\n",
        "train_corpus_tfidf=train_corpus_tfidf.reshape(train_corpus_tfidf.shape[0],train_corpus_tfidf.shape[1],1)\n",
        "test_corpus_tfidf=test_corpus_tfidf.reshape(test_corpus_tfidf.shape[0],test_corpus_tfidf.shape[1],1)\n",
        "\n",
        "model=Sequential([InputLayer(shape=(features,1),batch_size=bsize),\n",
        "                  Conv1D(3,2,activation=\"relu\"),\n",
        "                  MaxPooling1D(),\n",
        "                  Dropout(0.2),\n",
        "                  #Conv1D(32,16,activation=\"relu\"),\n",
        "                  #MaxPooling1D(),\n",
        "                  #Dropout(0.2),\n",
        "                  #Conv1D(3,2,activation=\"relu\"),\n",
        "                  #MaxPooling1D(),\n",
        "                  #Dropout(0.2),\n",
        "                  #Conv1D(36,4,activation=\"relu\"),\n",
        "                  #MaxPooling1D(),\n",
        "                  #Dropout(0.2),\n",
        "                  #Conv1D(18,8,activation=\"relu\"),\n",
        "                  #MaxPooling1D(),\n",
        "                  #Dropout(0.2),\n",
        "                  Flatten(),\n",
        "                  #Dense(100,activation=\"relu\"),\n",
        "                  #Dense(50,activation=\"relu\"),\n",
        "                  Dense(1,activation=\"sigmoid\")\n",
        "                  ])\n",
        "#model=Sequential([InputLayer(shape=(features,)),\n",
        "#                  Dense(100,activation=\"relu\"),\n",
        "#                  Dense(1,activation=\"sigmoid\")\n",
        "#                  ])\n",
        "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "model.fit(train_corpus_tfidf, train_label, epochs=epochs, batch_size=bsize, validation_data=(test_corpus_tfidf, test_label),shuffle=True)\n",
        "eval_loss,eval_accuracy=model.evaluate(test_corpus_tfidf,test_label)\n",
        "print(eval_loss,eval_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyiQwKpt0gr-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dntrKyWWI_f5"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMh9TfSLbf5dN6NaHtyWQYd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}