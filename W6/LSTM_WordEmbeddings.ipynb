{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nwon24/nlp/blob/main/W6/LSTM_WordEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SNJn2LK1JFjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dWAcH6mWYxkE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "29759bef-1cbb-4ba1-c878-7b8efd7c1784"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m172\u001b[0m, \u001b[38;5;34m300\u001b[0m)         │     \u001b[38;5;34m6,000,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m172\u001b[0m, \u001b[38;5;34m20\u001b[0m)          │        \u001b[38;5;34m25,680\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m3440\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m)                │         \u001b[38;5;34m3,441\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">172</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,000,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">172</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,680</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3440</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,441</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,029,121\u001b[0m (23.00 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,029,121</span> (23.00 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,029,121\u001b[0m (23.00 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,029,121</span> (23.00 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost: 0.694806, accuracy: 0.440000  [   50/45000]\n",
            "cost: 0.660890, accuracy: 0.640000  [  550/45000]\n",
            "cost: 0.698293, accuracy: 0.540000  [ 1050/45000]\n",
            "cost: 0.671092, accuracy: 0.600000  [ 1550/45000]\n",
            "cost: 0.590873, accuracy: 0.740000  [ 2050/45000]\n",
            "cost: 0.557198, accuracy: 0.640000  [ 2550/45000]\n",
            "cost: 0.506126, accuracy: 0.780000  [ 3050/45000]\n",
            "cost: 0.455519, accuracy: 0.820000  [ 3550/45000]\n",
            "cost: 0.534119, accuracy: 0.720000  [ 4050/45000]\n",
            "cost: 0.466430, accuracy: 0.860000  [ 4550/45000]\n",
            "cost: 0.408967, accuracy: 0.820000  [ 5050/45000]\n",
            "cost: 0.497463, accuracy: 0.820000  [ 5550/45000]\n",
            "cost: 0.579175, accuracy: 0.700000  [ 6050/45000]\n",
            "cost: 0.595281, accuracy: 0.720000  [ 6550/45000]\n",
            "cost: 0.582344, accuracy: 0.700000  [ 7050/45000]\n",
            "cost: 0.497096, accuracy: 0.780000  [ 7550/45000]\n",
            "cost: 0.499177, accuracy: 0.720000  [ 8050/45000]\n",
            "cost: 0.414826, accuracy: 0.800000  [ 8550/45000]\n",
            "cost: 0.370612, accuracy: 0.860000  [ 9050/45000]\n",
            "cost: 0.355935, accuracy: 0.920000  [ 9550/45000]\n",
            "cost: 0.449840, accuracy: 0.760000  [10050/45000]\n",
            "cost: 0.366248, accuracy: 0.820000  [10550/45000]\n",
            "cost: 0.473815, accuracy: 0.800000  [11050/45000]\n",
            "cost: 0.429599, accuracy: 0.760000  [11550/45000]\n",
            "cost: 0.465265, accuracy: 0.820000  [12050/45000]\n",
            "cost: 0.349110, accuracy: 0.840000  [12550/45000]\n",
            "cost: 0.412936, accuracy: 0.840000  [13050/45000]\n",
            "cost: 0.291822, accuracy: 0.840000  [13550/45000]\n",
            "cost: 0.523897, accuracy: 0.700000  [14050/45000]\n",
            "cost: 0.438118, accuracy: 0.800000  [14550/45000]\n",
            "cost: 0.528898, accuracy: 0.720000  [15050/45000]\n",
            "cost: 0.362617, accuracy: 0.840000  [15550/45000]\n",
            "cost: 0.372732, accuracy: 0.820000  [16050/45000]\n",
            "cost: 0.338679, accuracy: 0.900000  [16550/45000]\n",
            "cost: 0.337198, accuracy: 0.820000  [17050/45000]\n",
            "cost: 0.337662, accuracy: 0.880000  [17550/45000]\n",
            "cost: 0.389266, accuracy: 0.880000  [18050/45000]\n",
            "cost: 0.417421, accuracy: 0.800000  [18550/45000]\n",
            "cost: 0.477085, accuracy: 0.760000  [19050/45000]\n",
            "cost: 0.353675, accuracy: 0.920000  [19550/45000]\n",
            "cost: 0.413300, accuracy: 0.840000  [20050/45000]\n",
            "cost: 0.300496, accuracy: 0.900000  [20550/45000]\n",
            "cost: 0.400288, accuracy: 0.740000  [21050/45000]\n",
            "cost: 0.446942, accuracy: 0.820000  [21550/45000]\n",
            "cost: 0.459619, accuracy: 0.780000  [22050/45000]\n",
            "cost: 0.444343, accuracy: 0.800000  [22550/45000]\n",
            "cost: 0.428314, accuracy: 0.760000  [23050/45000]\n",
            "cost: 0.634763, accuracy: 0.720000  [23550/45000]\n",
            "cost: 0.520090, accuracy: 0.720000  [24050/45000]\n",
            "cost: 0.470564, accuracy: 0.740000  [24550/45000]\n",
            "cost: 0.396702, accuracy: 0.800000  [25050/45000]\n",
            "cost: 0.374026, accuracy: 0.780000  [25550/45000]\n",
            "cost: 0.368137, accuracy: 0.840000  [26050/45000]\n",
            "cost: 0.445417, accuracy: 0.760000  [26550/45000]\n",
            "cost: 0.557815, accuracy: 0.700000  [27050/45000]\n",
            "cost: 0.459827, accuracy: 0.800000  [27550/45000]\n",
            "cost: 0.338794, accuracy: 0.840000  [28050/45000]\n",
            "cost: 0.406280, accuracy: 0.820000  [28550/45000]\n",
            "cost: 0.370566, accuracy: 0.820000  [29050/45000]\n",
            "cost: 0.363401, accuracy: 0.840000  [29550/45000]\n",
            "cost: 0.364308, accuracy: 0.840000  [30050/45000]\n",
            "cost: 0.542188, accuracy: 0.740000  [30550/45000]\n",
            "cost: 0.390689, accuracy: 0.860000  [31050/45000]\n",
            "cost: 0.367646, accuracy: 0.840000  [31550/45000]\n",
            "cost: 0.411283, accuracy: 0.780000  [32050/45000]\n",
            "cost: 0.280589, accuracy: 0.920000  [32550/45000]\n",
            "cost: 0.432475, accuracy: 0.760000  [33050/45000]\n",
            "cost: 0.345315, accuracy: 0.860000  [33550/45000]\n",
            "cost: 0.389410, accuracy: 0.860000  [34050/45000]\n",
            "cost: 0.399332, accuracy: 0.780000  [34550/45000]\n",
            "cost: 0.353622, accuracy: 0.820000  [35050/45000]\n",
            "cost: 0.443217, accuracy: 0.800000  [35550/45000]\n",
            "cost: 0.409965, accuracy: 0.780000  [36050/45000]\n",
            "cost: 0.377462, accuracy: 0.840000  [36550/45000]\n",
            "cost: 0.331828, accuracy: 0.880000  [37050/45000]\n",
            "cost: 0.367899, accuracy: 0.840000  [37550/45000]\n",
            "cost: 0.374746, accuracy: 0.800000  [38050/45000]\n",
            "cost: 0.248726, accuracy: 0.840000  [38550/45000]\n",
            "cost: 0.449043, accuracy: 0.820000  [39050/45000]\n",
            "cost: 0.309367, accuracy: 0.840000  [39550/45000]\n",
            "cost: 0.368630, accuracy: 0.840000  [40050/45000]\n",
            "cost: 0.371743, accuracy: 0.860000  [40550/45000]\n",
            "cost: 0.424376, accuracy: 0.820000  [41050/45000]\n",
            "cost: 0.345010, accuracy: 0.840000  [41550/45000]\n",
            "cost: 0.430732, accuracy: 0.820000  [42050/45000]\n",
            "cost: 0.391139, accuracy: 0.780000  [42550/45000]\n",
            "cost: 0.296899, accuracy: 0.860000  [43050/45000]\n",
            "cost: 0.507844, accuracy: 0.780000  [43550/45000]\n",
            "cost: 0.414436, accuracy: 0.840000  [44050/45000]\n",
            "cost: 0.346363, accuracy: 0.840000  [44550/45000]\n",
            "cost: 0.453210, accuracy: 0.780000  [   50/45000]\n",
            "cost: 0.207904, accuracy: 0.940000  [  550/45000]\n",
            "cost: 0.280556, accuracy: 0.840000  [ 1050/45000]\n",
            "cost: 0.400740, accuracy: 0.820000  [ 1550/45000]\n",
            "cost: 0.404013, accuracy: 0.800000  [ 2050/45000]\n",
            "cost: 0.421385, accuracy: 0.780000  [ 2550/45000]\n",
            "cost: 0.297662, accuracy: 0.800000  [ 3050/45000]\n",
            "cost: 0.509329, accuracy: 0.800000  [ 3550/45000]\n",
            "cost: 0.343746, accuracy: 0.860000  [ 4050/45000]\n",
            "cost: 0.303187, accuracy: 0.800000  [ 4550/45000]\n",
            "cost: 0.379217, accuracy: 0.820000  [ 5050/45000]\n",
            "cost: 0.464232, accuracy: 0.800000  [ 5550/45000]\n",
            "cost: 0.450764, accuracy: 0.820000  [ 6050/45000]\n",
            "cost: 0.366357, accuracy: 0.820000  [ 6550/45000]\n",
            "cost: 0.315983, accuracy: 0.860000  [ 7050/45000]\n",
            "cost: 0.201197, accuracy: 0.960000  [ 7550/45000]\n",
            "cost: 0.323122, accuracy: 0.880000  [ 8050/45000]\n",
            "cost: 0.542492, accuracy: 0.740000  [ 8550/45000]\n",
            "cost: 0.464706, accuracy: 0.800000  [ 9050/45000]\n",
            "cost: 0.517533, accuracy: 0.760000  [ 9550/45000]\n",
            "cost: 0.504375, accuracy: 0.760000  [10050/45000]\n",
            "cost: 0.299772, accuracy: 0.860000  [10550/45000]\n",
            "cost: 0.408233, accuracy: 0.820000  [11050/45000]\n",
            "cost: 0.370902, accuracy: 0.820000  [11550/45000]\n",
            "cost: 0.286335, accuracy: 0.860000  [12050/45000]\n",
            "cost: 0.314193, accuracy: 0.880000  [12550/45000]\n",
            "cost: 0.251060, accuracy: 0.960000  [13050/45000]\n",
            "cost: 0.319181, accuracy: 0.880000  [13550/45000]\n",
            "cost: 0.421058, accuracy: 0.780000  [14050/45000]\n",
            "cost: 0.427092, accuracy: 0.820000  [14550/45000]\n",
            "cost: 0.393357, accuracy: 0.820000  [15050/45000]\n",
            "cost: 0.363945, accuracy: 0.800000  [15550/45000]\n",
            "cost: 0.288347, accuracy: 0.900000  [16050/45000]\n",
            "cost: 0.338659, accuracy: 0.860000  [16550/45000]\n",
            "cost: 0.441232, accuracy: 0.780000  [17050/45000]\n",
            "cost: 0.243407, accuracy: 0.860000  [17550/45000]\n",
            "cost: 0.293546, accuracy: 0.900000  [18050/45000]\n",
            "cost: 0.276832, accuracy: 0.900000  [18550/45000]\n",
            "cost: 0.359476, accuracy: 0.860000  [19050/45000]\n",
            "cost: 0.362444, accuracy: 0.880000  [19550/45000]\n",
            "cost: 0.348395, accuracy: 0.800000  [20050/45000]\n",
            "cost: 0.351514, accuracy: 0.880000  [20550/45000]\n",
            "cost: 0.374682, accuracy: 0.900000  [21050/45000]\n",
            "cost: 0.394657, accuracy: 0.800000  [21550/45000]\n",
            "cost: 0.422892, accuracy: 0.800000  [22050/45000]\n",
            "cost: 0.370158, accuracy: 0.820000  [22550/45000]\n",
            "cost: 0.309919, accuracy: 0.880000  [23050/45000]\n",
            "cost: 0.323327, accuracy: 0.880000  [23550/45000]\n",
            "cost: 0.343044, accuracy: 0.880000  [24050/45000]\n",
            "cost: 0.333426, accuracy: 0.820000  [24550/45000]\n",
            "cost: 0.421538, accuracy: 0.760000  [25050/45000]\n",
            "cost: 0.353360, accuracy: 0.820000  [25550/45000]\n",
            "cost: 0.363867, accuracy: 0.880000  [26050/45000]\n",
            "cost: 0.213416, accuracy: 0.940000  [26550/45000]\n",
            "cost: 0.271523, accuracy: 0.860000  [27050/45000]\n",
            "cost: 0.409279, accuracy: 0.800000  [27550/45000]\n",
            "cost: 0.311996, accuracy: 0.880000  [28050/45000]\n",
            "cost: 0.365193, accuracy: 0.840000  [28550/45000]\n",
            "cost: 0.285888, accuracy: 0.920000  [29050/45000]\n",
            "cost: 0.314470, accuracy: 0.880000  [29550/45000]\n",
            "cost: 0.293794, accuracy: 0.860000  [30050/45000]\n",
            "cost: 0.293163, accuracy: 0.860000  [30550/45000]\n",
            "cost: 0.244932, accuracy: 0.900000  [31050/45000]\n",
            "cost: 0.283553, accuracy: 0.880000  [31550/45000]\n",
            "cost: 0.380177, accuracy: 0.860000  [32050/45000]\n",
            "cost: 0.318703, accuracy: 0.920000  [32550/45000]\n",
            "cost: 0.273806, accuracy: 0.900000  [33050/45000]\n",
            "cost: 0.368329, accuracy: 0.840000  [33550/45000]\n",
            "cost: 0.401779, accuracy: 0.800000  [34050/45000]\n",
            "cost: 0.460631, accuracy: 0.800000  [34550/45000]\n",
            "cost: 0.417892, accuracy: 0.820000  [35050/45000]\n",
            "cost: 0.414817, accuracy: 0.900000  [35550/45000]\n",
            "cost: 0.327366, accuracy: 0.860000  [36050/45000]\n",
            "cost: 0.227220, accuracy: 0.980000  [36550/45000]\n",
            "cost: 0.466520, accuracy: 0.780000  [37050/45000]\n",
            "cost: 0.413614, accuracy: 0.740000  [37550/45000]\n",
            "cost: 0.354297, accuracy: 0.840000  [38050/45000]\n",
            "cost: 0.283216, accuracy: 0.900000  [38550/45000]\n",
            "cost: 0.412842, accuracy: 0.780000  [39050/45000]\n",
            "cost: 0.292366, accuracy: 0.940000  [39550/45000]\n",
            "cost: 0.333669, accuracy: 0.860000  [40050/45000]\n",
            "cost: 0.398595, accuracy: 0.840000  [40550/45000]\n",
            "cost: 0.414285, accuracy: 0.760000  [41050/45000]\n",
            "cost: 0.310036, accuracy: 0.880000  [41550/45000]\n",
            "cost: 0.429954, accuracy: 0.880000  [42050/45000]\n",
            "cost: 0.399157, accuracy: 0.820000  [42550/45000]\n",
            "cost: 0.237162, accuracy: 0.840000  [43050/45000]\n",
            "cost: 0.455180, accuracy: 0.840000  [43550/45000]\n",
            "cost: 0.381883, accuracy: 0.780000  [44050/45000]\n",
            "cost: 0.371247, accuracy: 0.820000  [44550/45000]\n",
            "cost: 0.262394, accuracy: 0.860000  [   50/45000]\n",
            "cost: 0.315281, accuracy: 0.880000  [  550/45000]\n",
            "cost: 0.216758, accuracy: 0.920000  [ 1050/45000]\n",
            "cost: 0.391313, accuracy: 0.800000  [ 1550/45000]\n",
            "cost: 0.297505, accuracy: 0.920000  [ 2050/45000]\n",
            "cost: 0.471077, accuracy: 0.860000  [ 2550/45000]\n",
            "cost: 0.263456, accuracy: 0.920000  [ 3050/45000]\n",
            "cost: 0.342923, accuracy: 0.800000  [ 3550/45000]\n",
            "cost: 0.395471, accuracy: 0.780000  [ 4050/45000]\n",
            "cost: 0.407702, accuracy: 0.800000  [ 4550/45000]\n",
            "cost: 0.315355, accuracy: 0.920000  [ 5050/45000]\n",
            "cost: 0.260372, accuracy: 0.880000  [ 5550/45000]\n",
            "cost: 0.347773, accuracy: 0.820000  [ 6050/45000]\n",
            "cost: 0.467962, accuracy: 0.840000  [ 6550/45000]\n",
            "cost: 0.360424, accuracy: 0.880000  [ 7050/45000]\n",
            "cost: 0.228003, accuracy: 0.940000  [ 7550/45000]\n",
            "cost: 0.279911, accuracy: 0.860000  [ 8050/45000]\n",
            "cost: 0.194945, accuracy: 0.940000  [ 8550/45000]\n",
            "cost: 0.183554, accuracy: 0.900000  [ 9050/45000]\n",
            "cost: 0.269587, accuracy: 0.900000  [ 9550/45000]\n",
            "cost: 0.345003, accuracy: 0.900000  [10050/45000]\n",
            "cost: 0.393737, accuracy: 0.800000  [10550/45000]\n",
            "cost: 0.347978, accuracy: 0.840000  [11050/45000]\n",
            "cost: 0.237627, accuracy: 0.880000  [11550/45000]\n",
            "cost: 0.359685, accuracy: 0.820000  [12050/45000]\n",
            "cost: 0.282440, accuracy: 0.880000  [12550/45000]\n",
            "cost: 0.341851, accuracy: 0.840000  [13050/45000]\n",
            "cost: 0.324891, accuracy: 0.900000  [13550/45000]\n",
            "cost: 0.295564, accuracy: 0.860000  [14050/45000]\n",
            "cost: 0.390457, accuracy: 0.800000  [14550/45000]\n",
            "cost: 0.484960, accuracy: 0.800000  [15050/45000]\n",
            "cost: 0.245474, accuracy: 0.920000  [15550/45000]\n",
            "cost: 0.337724, accuracy: 0.800000  [16050/45000]\n",
            "cost: 0.302964, accuracy: 0.880000  [16550/45000]\n",
            "cost: 0.329346, accuracy: 0.880000  [17050/45000]\n",
            "cost: 0.251824, accuracy: 0.920000  [17550/45000]\n",
            "cost: 0.315366, accuracy: 0.900000  [18050/45000]\n",
            "cost: 0.253498, accuracy: 0.900000  [18550/45000]\n",
            "cost: 0.345287, accuracy: 0.840000  [19050/45000]\n",
            "cost: 0.349077, accuracy: 0.800000  [19550/45000]\n",
            "cost: 0.218030, accuracy: 0.920000  [20050/45000]\n",
            "cost: 0.275380, accuracy: 0.860000  [20550/45000]\n",
            "cost: 0.438190, accuracy: 0.820000  [21050/45000]\n",
            "cost: 0.272948, accuracy: 0.920000  [21550/45000]\n",
            "cost: 0.242774, accuracy: 0.900000  [22050/45000]\n",
            "cost: 0.435048, accuracy: 0.860000  [22550/45000]\n",
            "cost: 0.264489, accuracy: 0.880000  [23050/45000]\n",
            "cost: 0.371713, accuracy: 0.840000  [23550/45000]\n",
            "cost: 0.347843, accuracy: 0.840000  [24050/45000]\n",
            "cost: 0.335969, accuracy: 0.860000  [24550/45000]\n",
            "cost: 0.260303, accuracy: 0.920000  [25050/45000]\n",
            "cost: 0.221839, accuracy: 0.900000  [25550/45000]\n",
            "cost: 0.364544, accuracy: 0.820000  [26050/45000]\n",
            "cost: 0.382231, accuracy: 0.780000  [26550/45000]\n",
            "cost: 0.223380, accuracy: 0.920000  [27050/45000]\n",
            "cost: 0.337205, accuracy: 0.840000  [27550/45000]\n",
            "cost: 0.359188, accuracy: 0.820000  [28050/45000]\n",
            "cost: 0.351457, accuracy: 0.800000  [28550/45000]\n",
            "cost: 0.248835, accuracy: 0.900000  [29050/45000]\n",
            "cost: 0.242212, accuracy: 0.920000  [29550/45000]\n",
            "cost: 0.305823, accuracy: 0.900000  [30050/45000]\n",
            "cost: 0.384714, accuracy: 0.800000  [30550/45000]\n",
            "cost: 0.139025, accuracy: 0.940000  [31050/45000]\n",
            "cost: 0.142673, accuracy: 0.980000  [31550/45000]\n",
            "cost: 0.179130, accuracy: 0.920000  [32050/45000]\n",
            "cost: 0.229070, accuracy: 0.900000  [32550/45000]\n",
            "cost: 0.616369, accuracy: 0.740000  [33050/45000]\n",
            "cost: 0.236308, accuracy: 0.920000  [33550/45000]\n",
            "cost: 0.326765, accuracy: 0.840000  [34050/45000]\n",
            "cost: 0.309215, accuracy: 0.880000  [34550/45000]\n",
            "cost: 0.193710, accuracy: 0.920000  [35050/45000]\n",
            "cost: 0.312901, accuracy: 0.920000  [35550/45000]\n",
            "cost: 0.226036, accuracy: 0.940000  [36050/45000]\n",
            "cost: 0.387759, accuracy: 0.800000  [36550/45000]\n",
            "cost: 0.314757, accuracy: 0.860000  [37050/45000]\n",
            "cost: 0.169915, accuracy: 0.940000  [37550/45000]\n",
            "cost: 0.287736, accuracy: 0.860000  [38050/45000]\n",
            "cost: 0.324378, accuracy: 0.840000  [38550/45000]\n",
            "cost: 0.261178, accuracy: 0.920000  [39050/45000]\n",
            "cost: 0.252250, accuracy: 0.860000  [39550/45000]\n",
            "cost: 0.388325, accuracy: 0.800000  [40050/45000]\n",
            "cost: 0.416114, accuracy: 0.780000  [40550/45000]\n",
            "cost: 0.185953, accuracy: 0.940000  [41050/45000]\n",
            "cost: 0.449147, accuracy: 0.840000  [41550/45000]\n",
            "cost: 0.316968, accuracy: 0.840000  [42050/45000]\n",
            "cost: 0.275139, accuracy: 0.840000  [42550/45000]\n",
            "cost: 0.279700, accuracy: 0.860000  [43050/45000]\n",
            "cost: 0.337587, accuracy: 0.880000  [43550/45000]\n",
            "cost: 0.291154, accuracy: 0.880000  [44050/45000]\n",
            "cost: 0.253237, accuracy: 0.900000  [44550/45000]\n",
            "cost: 0.351848, accuracy: 0.880000  [   50/45000]\n",
            "cost: 0.234912, accuracy: 0.920000  [  550/45000]\n",
            "cost: 0.405673, accuracy: 0.840000  [ 1050/45000]\n",
            "cost: 0.287197, accuracy: 0.900000  [ 1550/45000]\n",
            "cost: 0.359264, accuracy: 0.820000  [ 2050/45000]\n",
            "cost: 0.320710, accuracy: 0.900000  [ 2550/45000]\n",
            "cost: 0.277911, accuracy: 0.800000  [ 3050/45000]\n",
            "cost: 0.262264, accuracy: 0.860000  [ 3550/45000]\n",
            "cost: 0.254183, accuracy: 0.840000  [ 4050/45000]\n",
            "cost: 0.275952, accuracy: 0.860000  [ 4550/45000]\n",
            "cost: 0.316737, accuracy: 0.860000  [ 5050/45000]\n",
            "cost: 0.261934, accuracy: 0.900000  [ 5550/45000]\n",
            "cost: 0.309789, accuracy: 0.820000  [ 6050/45000]\n",
            "cost: 0.202529, accuracy: 0.920000  [ 6550/45000]\n",
            "cost: 0.279014, accuracy: 0.860000  [ 7050/45000]\n",
            "cost: 0.168286, accuracy: 0.940000  [ 7550/45000]\n",
            "cost: 0.157475, accuracy: 0.940000  [ 8050/45000]\n",
            "cost: 0.269817, accuracy: 0.900000  [ 8550/45000]\n",
            "cost: 0.277911, accuracy: 0.860000  [ 9050/45000]\n",
            "cost: 0.124449, accuracy: 0.980000  [ 9550/45000]\n",
            "cost: 0.255575, accuracy: 0.860000  [10050/45000]\n",
            "cost: 0.263586, accuracy: 0.900000  [10550/45000]\n",
            "cost: 0.283270, accuracy: 0.900000  [11050/45000]\n",
            "cost: 0.286727, accuracy: 0.880000  [11550/45000]\n",
            "cost: 0.305737, accuracy: 0.840000  [12050/45000]\n",
            "cost: 0.213325, accuracy: 0.920000  [12550/45000]\n",
            "cost: 0.280516, accuracy: 0.860000  [13050/45000]\n",
            "cost: 0.298466, accuracy: 0.880000  [13550/45000]\n",
            "cost: 0.252925, accuracy: 0.920000  [14050/45000]\n",
            "cost: 0.230419, accuracy: 0.920000  [14550/45000]\n",
            "cost: 0.162137, accuracy: 0.940000  [15050/45000]\n",
            "cost: 0.258320, accuracy: 0.920000  [15550/45000]\n",
            "cost: 0.368651, accuracy: 0.860000  [16050/45000]\n",
            "cost: 0.236357, accuracy: 0.900000  [16550/45000]\n",
            "cost: 0.361195, accuracy: 0.840000  [17050/45000]\n",
            "cost: 0.233020, accuracy: 0.900000  [17550/45000]\n",
            "cost: 0.300720, accuracy: 0.840000  [18050/45000]\n",
            "cost: 0.296880, accuracy: 0.880000  [18550/45000]\n",
            "cost: 0.195150, accuracy: 0.940000  [19050/45000]\n",
            "cost: 0.247127, accuracy: 0.900000  [19550/45000]\n",
            "cost: 0.298485, accuracy: 0.920000  [20050/45000]\n",
            "cost: 0.294196, accuracy: 0.900000  [20550/45000]\n",
            "cost: 0.237178, accuracy: 0.920000  [21050/45000]\n",
            "cost: 0.257415, accuracy: 0.900000  [21550/45000]\n",
            "cost: 0.325670, accuracy: 0.860000  [22050/45000]\n",
            "cost: 0.397974, accuracy: 0.840000  [22550/45000]\n",
            "cost: 0.375865, accuracy: 0.800000  [23050/45000]\n",
            "cost: 0.350115, accuracy: 0.840000  [23550/45000]\n",
            "cost: 0.159586, accuracy: 0.980000  [24050/45000]\n",
            "cost: 0.199039, accuracy: 0.940000  [24550/45000]\n",
            "cost: 0.263064, accuracy: 0.920000  [25050/45000]\n",
            "cost: 0.157645, accuracy: 0.960000  [25550/45000]\n",
            "cost: 0.312372, accuracy: 0.840000  [26050/45000]\n",
            "cost: 0.241539, accuracy: 0.900000  [26550/45000]\n",
            "cost: 0.245572, accuracy: 0.900000  [27050/45000]\n",
            "cost: 0.228008, accuracy: 0.920000  [27550/45000]\n",
            "cost: 0.327338, accuracy: 0.920000  [28050/45000]\n",
            "cost: 0.187213, accuracy: 0.940000  [28550/45000]\n",
            "cost: 0.222066, accuracy: 0.960000  [29050/45000]\n",
            "cost: 0.222999, accuracy: 0.880000  [29550/45000]\n",
            "cost: 0.305804, accuracy: 0.880000  [30050/45000]\n",
            "cost: 0.423592, accuracy: 0.840000  [30550/45000]\n",
            "cost: 0.465947, accuracy: 0.860000  [31050/45000]\n",
            "cost: 0.322235, accuracy: 0.840000  [31550/45000]\n",
            "cost: 0.337470, accuracy: 0.820000  [32050/45000]\n",
            "cost: 0.238294, accuracy: 0.920000  [32550/45000]\n",
            "cost: 0.308772, accuracy: 0.820000  [33050/45000]\n",
            "cost: 0.443404, accuracy: 0.800000  [33550/45000]\n",
            "cost: 0.196121, accuracy: 0.920000  [34050/45000]\n",
            "cost: 0.371440, accuracy: 0.860000  [34550/45000]\n",
            "cost: 0.379803, accuracy: 0.840000  [35050/45000]\n",
            "cost: 0.304161, accuracy: 0.820000  [35550/45000]\n",
            "cost: 0.184288, accuracy: 0.940000  [36050/45000]\n",
            "cost: 0.207157, accuracy: 0.920000  [36550/45000]\n",
            "cost: 0.287754, accuracy: 0.920000  [37050/45000]\n",
            "cost: 0.256264, accuracy: 0.880000  [37550/45000]\n",
            "cost: 0.254520, accuracy: 0.920000  [38050/45000]\n",
            "cost: 0.352258, accuracy: 0.800000  [38550/45000]\n",
            "cost: 0.235251, accuracy: 0.900000  [39050/45000]\n",
            "cost: 0.327633, accuracy: 0.840000  [39550/45000]\n",
            "cost: 0.326205, accuracy: 0.860000  [40050/45000]\n",
            "cost: 0.326321, accuracy: 0.900000  [40550/45000]\n",
            "cost: 0.232266, accuracy: 0.900000  [41050/45000]\n",
            "cost: 0.326882, accuracy: 0.800000  [41550/45000]\n",
            "cost: 0.366581, accuracy: 0.840000  [42050/45000]\n",
            "cost: 0.288390, accuracy: 0.840000  [42550/45000]\n",
            "cost: 0.364367, accuracy: 0.880000  [43050/45000]\n",
            "cost: 0.179739, accuracy: 0.940000  [43550/45000]\n",
            "cost: 0.237398, accuracy: 0.880000  [44050/45000]\n",
            "cost: 0.441452, accuracy: 0.820000  [44550/45000]\n",
            "cost: 0.185779, accuracy: 0.940000  [   50/45000]\n",
            "cost: 0.333222, accuracy: 0.880000  [  550/45000]\n",
            "cost: 0.282854, accuracy: 0.900000  [ 1050/45000]\n",
            "cost: 0.282468, accuracy: 0.920000  [ 1550/45000]\n",
            "cost: 0.233628, accuracy: 0.920000  [ 2050/45000]\n",
            "cost: 0.362145, accuracy: 0.840000  [ 2550/45000]\n",
            "cost: 0.172033, accuracy: 0.940000  [ 3050/45000]\n",
            "cost: 0.293577, accuracy: 0.840000  [ 3550/45000]\n",
            "cost: 0.224665, accuracy: 0.920000  [ 4050/45000]\n",
            "cost: 0.304088, accuracy: 0.840000  [ 4550/45000]\n",
            "cost: 0.399724, accuracy: 0.840000  [ 5050/45000]\n",
            "cost: 0.173085, accuracy: 0.940000  [ 5550/45000]\n",
            "cost: 0.169491, accuracy: 0.940000  [ 6050/45000]\n",
            "cost: 0.252323, accuracy: 0.940000  [ 6550/45000]\n",
            "cost: 0.170069, accuracy: 0.900000  [ 7050/45000]\n",
            "cost: 0.155515, accuracy: 0.960000  [ 7550/45000]\n",
            "cost: 0.385043, accuracy: 0.840000  [ 8050/45000]\n",
            "cost: 0.236824, accuracy: 0.920000  [ 8550/45000]\n",
            "cost: 0.234385, accuracy: 0.880000  [ 9050/45000]\n",
            "cost: 0.239460, accuracy: 0.860000  [ 9550/45000]\n",
            "cost: 0.315106, accuracy: 0.880000  [10050/45000]\n",
            "cost: 0.173408, accuracy: 0.900000  [10550/45000]\n",
            "cost: 0.273623, accuracy: 0.860000  [11050/45000]\n",
            "cost: 0.258624, accuracy: 0.900000  [11550/45000]\n",
            "cost: 0.233562, accuracy: 0.920000  [12050/45000]\n",
            "cost: 0.228763, accuracy: 0.920000  [12550/45000]\n",
            "cost: 0.357525, accuracy: 0.840000  [13050/45000]\n",
            "cost: 0.163208, accuracy: 0.940000  [13550/45000]\n",
            "cost: 0.173845, accuracy: 0.920000  [14050/45000]\n",
            "cost: 0.223876, accuracy: 0.920000  [14550/45000]\n",
            "cost: 0.210110, accuracy: 0.900000  [15050/45000]\n",
            "cost: 0.244450, accuracy: 0.900000  [15550/45000]\n",
            "cost: 0.302960, accuracy: 0.900000  [16050/45000]\n",
            "cost: 0.251540, accuracy: 0.880000  [16550/45000]\n",
            "cost: 0.173721, accuracy: 0.940000  [17050/45000]\n",
            "cost: 0.147911, accuracy: 0.940000  [17550/45000]\n",
            "cost: 0.221037, accuracy: 0.940000  [18050/45000]\n",
            "cost: 0.115064, accuracy: 0.960000  [18550/45000]\n",
            "cost: 0.109856, accuracy: 0.960000  [19050/45000]\n",
            "cost: 0.278463, accuracy: 0.900000  [19550/45000]\n",
            "cost: 0.233895, accuracy: 0.940000  [20050/45000]\n",
            "cost: 0.387920, accuracy: 0.900000  [20550/45000]\n",
            "cost: 0.287624, accuracy: 0.860000  [21050/45000]\n",
            "cost: 0.281760, accuracy: 0.880000  [21550/45000]\n",
            "cost: 0.082345, accuracy: 1.000000  [22050/45000]\n",
            "cost: 0.242251, accuracy: 0.840000  [22550/45000]\n",
            "cost: 0.245202, accuracy: 0.900000  [23050/45000]\n",
            "cost: 0.427610, accuracy: 0.820000  [23550/45000]\n",
            "cost: 0.306769, accuracy: 0.860000  [24050/45000]\n",
            "cost: 0.265299, accuracy: 0.840000  [24550/45000]\n",
            "cost: 0.275226, accuracy: 0.900000  [25050/45000]\n",
            "cost: 0.232970, accuracy: 0.880000  [25550/45000]\n",
            "cost: 0.218888, accuracy: 0.860000  [26050/45000]\n",
            "cost: 0.194319, accuracy: 0.920000  [26550/45000]\n",
            "cost: 0.135391, accuracy: 0.960000  [27050/45000]\n",
            "cost: 0.276348, accuracy: 0.900000  [27550/45000]\n",
            "cost: 0.323392, accuracy: 0.880000  [28050/45000]\n",
            "cost: 0.228696, accuracy: 0.900000  [28550/45000]\n",
            "cost: 0.108007, accuracy: 0.980000  [29050/45000]\n",
            "cost: 0.119199, accuracy: 0.960000  [29550/45000]\n",
            "cost: 0.430785, accuracy: 0.820000  [30050/45000]\n",
            "cost: 0.273720, accuracy: 0.900000  [30550/45000]\n",
            "cost: 0.264529, accuracy: 0.920000  [31050/45000]\n",
            "cost: 0.270858, accuracy: 0.860000  [31550/45000]\n",
            "cost: 0.314344, accuracy: 0.840000  [32050/45000]\n",
            "cost: 0.504743, accuracy: 0.820000  [32550/45000]\n",
            "cost: 0.426446, accuracy: 0.820000  [33050/45000]\n",
            "cost: 0.232518, accuracy: 0.900000  [33550/45000]\n",
            "cost: 0.150843, accuracy: 0.940000  [34050/45000]\n",
            "cost: 0.454031, accuracy: 0.820000  [34550/45000]\n",
            "cost: 0.312856, accuracy: 0.820000  [35050/45000]\n",
            "cost: 0.376713, accuracy: 0.840000  [35550/45000]\n",
            "cost: 0.158845, accuracy: 0.940000  [36050/45000]\n",
            "cost: 0.398005, accuracy: 0.860000  [36550/45000]\n",
            "cost: 0.220528, accuracy: 0.920000  [37050/45000]\n",
            "cost: 0.274338, accuracy: 0.920000  [37550/45000]\n",
            "cost: 0.198530, accuracy: 0.920000  [38050/45000]\n",
            "cost: 0.316596, accuracy: 0.880000  [38550/45000]\n",
            "cost: 0.350547, accuracy: 0.840000  [39050/45000]\n",
            "cost: 0.217359, accuracy: 0.920000  [39550/45000]\n",
            "cost: 0.194320, accuracy: 0.920000  [40050/45000]\n",
            "cost: 0.231572, accuracy: 0.900000  [40550/45000]\n",
            "cost: 0.326793, accuracy: 0.820000  [41050/45000]\n",
            "cost: 0.255641, accuracy: 0.900000  [41550/45000]\n",
            "cost: 0.229987, accuracy: 0.920000  [42050/45000]\n",
            "cost: 0.219118, accuracy: 0.900000  [42550/45000]\n",
            "cost: 0.143300, accuracy: 0.940000  [43050/45000]\n",
            "cost: 0.265729, accuracy: 0.900000  [43550/45000]\n",
            "cost: 0.172733, accuracy: 0.940000  [44050/45000]\n",
            "cost: 0.143866, accuracy: 0.980000  [44550/45000]\n",
            "cost: 0.151546, accuracy: 0.960000  [   50/45000]\n",
            "cost: 0.204183, accuracy: 0.940000  [  550/45000]\n",
            "cost: 0.278702, accuracy: 0.900000  [ 1050/45000]\n",
            "cost: 0.257034, accuracy: 0.880000  [ 1550/45000]\n",
            "cost: 0.176684, accuracy: 0.920000  [ 2050/45000]\n",
            "cost: 0.271835, accuracy: 0.880000  [ 2550/45000]\n",
            "cost: 0.193430, accuracy: 0.920000  [ 3050/45000]\n",
            "cost: 0.132835, accuracy: 0.960000  [ 3550/45000]\n",
            "cost: 0.207789, accuracy: 0.900000  [ 4050/45000]\n",
            "cost: 0.244930, accuracy: 0.940000  [ 4550/45000]\n",
            "cost: 0.095768, accuracy: 0.960000  [ 5050/45000]\n",
            "cost: 0.183545, accuracy: 0.920000  [ 5550/45000]\n",
            "cost: 0.253189, accuracy: 0.900000  [ 6050/45000]\n",
            "cost: 0.324373, accuracy: 0.840000  [ 6550/45000]\n",
            "cost: 0.226407, accuracy: 0.880000  [ 7050/45000]\n",
            "cost: 0.196454, accuracy: 0.940000  [ 7550/45000]\n",
            "cost: 0.314087, accuracy: 0.860000  [ 8050/45000]\n",
            "cost: 0.234512, accuracy: 0.860000  [ 8550/45000]\n",
            "cost: 0.468358, accuracy: 0.840000  [ 9050/45000]\n",
            "cost: 0.202154, accuracy: 0.900000  [ 9550/45000]\n",
            "cost: 0.197343, accuracy: 0.920000  [10050/45000]\n",
            "cost: 0.182490, accuracy: 0.940000  [10550/45000]\n",
            "cost: 0.159512, accuracy: 0.920000  [11050/45000]\n",
            "cost: 0.231967, accuracy: 0.880000  [11550/45000]\n",
            "cost: 0.204192, accuracy: 0.900000  [12050/45000]\n",
            "cost: 0.141268, accuracy: 0.960000  [12550/45000]\n",
            "cost: 0.338715, accuracy: 0.880000  [13050/45000]\n",
            "cost: 0.188814, accuracy: 0.920000  [13550/45000]\n",
            "cost: 0.274552, accuracy: 0.880000  [14050/45000]\n",
            "cost: 0.358994, accuracy: 0.880000  [14550/45000]\n",
            "cost: 0.198554, accuracy: 0.900000  [15050/45000]\n",
            "cost: 0.263435, accuracy: 0.880000  [15550/45000]\n",
            "cost: 0.239128, accuracy: 0.900000  [16050/45000]\n",
            "cost: 0.215507, accuracy: 0.920000  [16550/45000]\n",
            "cost: 0.215424, accuracy: 0.900000  [17050/45000]\n",
            "cost: 0.349920, accuracy: 0.840000  [17550/45000]\n",
            "cost: 0.334830, accuracy: 0.860000  [18050/45000]\n",
            "cost: 0.227999, accuracy: 0.940000  [18550/45000]\n",
            "cost: 0.120438, accuracy: 0.960000  [19050/45000]\n",
            "cost: 0.263811, accuracy: 0.900000  [19550/45000]\n",
            "cost: 0.263223, accuracy: 0.920000  [20050/45000]\n",
            "cost: 0.297556, accuracy: 0.900000  [20550/45000]\n",
            "cost: 0.063803, accuracy: 1.000000  [21050/45000]\n",
            "cost: 0.196236, accuracy: 0.900000  [21550/45000]\n",
            "cost: 0.212525, accuracy: 0.940000  [22050/45000]\n",
            "cost: 0.192290, accuracy: 0.920000  [22550/45000]\n",
            "cost: 0.161091, accuracy: 0.920000  [23050/45000]\n",
            "cost: 0.208362, accuracy: 0.900000  [23550/45000]\n",
            "cost: 0.247335, accuracy: 0.900000  [24050/45000]\n",
            "cost: 0.148844, accuracy: 0.980000  [24550/45000]\n",
            "cost: 0.365349, accuracy: 0.840000  [25050/45000]\n",
            "cost: 0.231994, accuracy: 0.920000  [25550/45000]\n",
            "cost: 0.155834, accuracy: 0.940000  [26050/45000]\n",
            "cost: 0.166462, accuracy: 0.900000  [26550/45000]\n",
            "cost: 0.141398, accuracy: 0.960000  [27050/45000]\n",
            "cost: 0.108196, accuracy: 1.000000  [27550/45000]\n",
            "cost: 0.242066, accuracy: 0.880000  [28050/45000]\n",
            "cost: 0.124790, accuracy: 0.960000  [28550/45000]\n",
            "cost: 0.165682, accuracy: 0.940000  [29050/45000]\n",
            "cost: 0.144169, accuracy: 0.960000  [29550/45000]\n",
            "cost: 0.232130, accuracy: 0.940000  [30050/45000]\n",
            "cost: 0.255033, accuracy: 0.880000  [30550/45000]\n",
            "cost: 0.151838, accuracy: 0.940000  [31050/45000]\n",
            "cost: 0.303716, accuracy: 0.840000  [31550/45000]\n",
            "cost: 0.107335, accuracy: 0.960000  [32050/45000]\n",
            "cost: 0.272645, accuracy: 0.900000  [32550/45000]\n",
            "cost: 0.260543, accuracy: 0.860000  [33050/45000]\n",
            "cost: 0.109627, accuracy: 0.960000  [33550/45000]\n",
            "cost: 0.312639, accuracy: 0.880000  [34050/45000]\n",
            "cost: 0.211985, accuracy: 0.940000  [34550/45000]\n",
            "cost: 0.313114, accuracy: 0.860000  [35050/45000]\n",
            "cost: 0.278211, accuracy: 0.860000  [35550/45000]\n",
            "cost: 0.122509, accuracy: 0.960000  [36050/45000]\n",
            "cost: 0.201948, accuracy: 0.920000  [36550/45000]\n",
            "cost: 0.201360, accuracy: 0.920000  [37050/45000]\n",
            "cost: 0.249220, accuracy: 0.900000  [37550/45000]\n",
            "cost: 0.209830, accuracy: 0.900000  [38050/45000]\n",
            "cost: 0.228182, accuracy: 0.920000  [38550/45000]\n",
            "cost: 0.297420, accuracy: 0.880000  [39050/45000]\n",
            "cost: 0.214879, accuracy: 0.940000  [39550/45000]\n",
            "cost: 0.239975, accuracy: 0.880000  [40050/45000]\n",
            "cost: 0.118382, accuracy: 0.980000  [40550/45000]\n",
            "cost: 0.201569, accuracy: 0.940000  [41050/45000]\n",
            "cost: 0.367992, accuracy: 0.860000  [41550/45000]\n",
            "cost: 0.196036, accuracy: 0.880000  [42050/45000]\n",
            "cost: 0.270751, accuracy: 0.880000  [42550/45000]\n",
            "cost: 0.235819, accuracy: 0.900000  [43050/45000]\n",
            "cost: 0.201455, accuracy: 0.980000  [43550/45000]\n",
            "cost: 0.243892, accuracy: 0.900000  [44050/45000]\n",
            "cost: 0.393363, accuracy: 0.860000  [44550/45000]\n",
            "cost: 0.180229, accuracy: 0.960000  [   50/45000]\n",
            "cost: 0.180527, accuracy: 0.920000  [  550/45000]\n",
            "cost: 0.120237, accuracy: 0.960000  [ 1050/45000]\n",
            "cost: 0.231930, accuracy: 0.920000  [ 1550/45000]\n",
            "cost: 0.187032, accuracy: 0.940000  [ 2050/45000]\n",
            "cost: 0.229387, accuracy: 0.900000  [ 2550/45000]\n",
            "cost: 0.234134, accuracy: 0.880000  [ 3050/45000]\n",
            "cost: 0.200816, accuracy: 0.900000  [ 3550/45000]\n",
            "cost: 0.200235, accuracy: 0.960000  [ 4050/45000]\n",
            "cost: 0.181644, accuracy: 0.900000  [ 4550/45000]\n",
            "cost: 0.176379, accuracy: 0.920000  [ 5050/45000]\n",
            "cost: 0.080943, accuracy: 0.980000  [ 5550/45000]\n",
            "cost: 0.178948, accuracy: 0.920000  [ 6050/45000]\n",
            "cost: 0.137627, accuracy: 0.960000  [ 6550/45000]\n",
            "cost: 0.163533, accuracy: 0.940000  [ 7050/45000]\n",
            "cost: 0.100211, accuracy: 0.960000  [ 7550/45000]\n",
            "cost: 0.237825, accuracy: 0.900000  [ 8050/45000]\n",
            "cost: 0.196997, accuracy: 0.920000  [ 8550/45000]\n",
            "cost: 0.221698, accuracy: 0.940000  [ 9050/45000]\n",
            "cost: 0.144514, accuracy: 0.960000  [ 9550/45000]\n",
            "cost: 0.200230, accuracy: 0.920000  [10050/45000]\n",
            "cost: 0.219623, accuracy: 0.900000  [10550/45000]\n",
            "cost: 0.157733, accuracy: 0.920000  [11050/45000]\n",
            "cost: 0.246629, accuracy: 0.920000  [11550/45000]\n",
            "cost: 0.101106, accuracy: 0.980000  [12050/45000]\n",
            "cost: 0.083801, accuracy: 0.960000  [12550/45000]\n",
            "cost: 0.258742, accuracy: 0.900000  [13050/45000]\n",
            "cost: 0.087284, accuracy: 0.980000  [13550/45000]\n",
            "cost: 0.136315, accuracy: 0.960000  [14050/45000]\n",
            "cost: 0.152271, accuracy: 0.940000  [14550/45000]\n",
            "cost: 0.242318, accuracy: 0.900000  [15050/45000]\n",
            "cost: 0.176973, accuracy: 0.900000  [15550/45000]\n",
            "cost: 0.132785, accuracy: 0.960000  [16050/45000]\n",
            "cost: 0.141420, accuracy: 0.940000  [16550/45000]\n",
            "cost: 0.068604, accuracy: 0.980000  [17050/45000]\n",
            "cost: 0.185695, accuracy: 0.940000  [17550/45000]\n",
            "cost: 0.165569, accuracy: 0.960000  [18050/45000]\n",
            "cost: 0.177802, accuracy: 0.920000  [18550/45000]\n",
            "cost: 0.246098, accuracy: 0.860000  [19050/45000]\n",
            "cost: 0.164830, accuracy: 0.920000  [19550/45000]\n",
            "cost: 0.188932, accuracy: 0.920000  [20050/45000]\n",
            "cost: 0.264213, accuracy: 0.940000  [20550/45000]\n",
            "cost: 0.163658, accuracy: 0.960000  [21050/45000]\n",
            "cost: 0.201163, accuracy: 0.920000  [21550/45000]\n",
            "cost: 0.139116, accuracy: 0.980000  [22050/45000]\n",
            "cost: 0.266795, accuracy: 0.860000  [22550/45000]\n",
            "cost: 0.158327, accuracy: 0.960000  [23050/45000]\n",
            "cost: 0.116454, accuracy: 0.960000  [23550/45000]\n",
            "cost: 0.197707, accuracy: 0.900000  [24050/45000]\n",
            "cost: 0.079940, accuracy: 0.980000  [24550/45000]\n",
            "cost: 0.233919, accuracy: 0.860000  [25050/45000]\n",
            "cost: 0.218169, accuracy: 0.880000  [25550/45000]\n",
            "cost: 0.137700, accuracy: 0.940000  [26050/45000]\n",
            "cost: 0.197047, accuracy: 0.860000  [26550/45000]\n",
            "cost: 0.259824, accuracy: 0.880000  [27050/45000]\n",
            "cost: 0.301891, accuracy: 0.880000  [27550/45000]\n",
            "cost: 0.145426, accuracy: 0.920000  [28050/45000]\n",
            "cost: 0.134899, accuracy: 0.940000  [28550/45000]\n",
            "cost: 0.334745, accuracy: 0.840000  [29050/45000]\n",
            "cost: 0.236364, accuracy: 0.900000  [29550/45000]\n",
            "cost: 0.217103, accuracy: 0.880000  [30050/45000]\n",
            "cost: 0.325430, accuracy: 0.900000  [30550/45000]\n",
            "cost: 0.206685, accuracy: 0.900000  [31050/45000]\n",
            "cost: 0.155957, accuracy: 0.940000  [31550/45000]\n",
            "cost: 0.222357, accuracy: 0.880000  [32050/45000]\n",
            "cost: 0.233303, accuracy: 0.900000  [32550/45000]\n",
            "cost: 0.190714, accuracy: 0.940000  [33050/45000]\n",
            "cost: 0.305508, accuracy: 0.900000  [33550/45000]\n",
            "cost: 0.196684, accuracy: 0.920000  [34050/45000]\n",
            "cost: 0.167028, accuracy: 0.960000  [34550/45000]\n",
            "cost: 0.155263, accuracy: 0.940000  [35050/45000]\n",
            "cost: 0.232434, accuracy: 0.920000  [35550/45000]\n",
            "cost: 0.303607, accuracy: 0.920000  [36050/45000]\n",
            "cost: 0.226499, accuracy: 0.920000  [36550/45000]\n",
            "cost: 0.145462, accuracy: 0.960000  [37050/45000]\n",
            "cost: 0.166890, accuracy: 0.960000  [37550/45000]\n",
            "cost: 0.113143, accuracy: 0.960000  [38050/45000]\n",
            "cost: 0.279783, accuracy: 0.840000  [38550/45000]\n",
            "cost: 0.321361, accuracy: 0.840000  [39050/45000]\n",
            "cost: 0.170162, accuracy: 0.920000  [39550/45000]\n",
            "cost: 0.136685, accuracy: 0.940000  [40050/45000]\n",
            "cost: 0.190869, accuracy: 0.960000  [40550/45000]\n",
            "cost: 0.287354, accuracy: 0.860000  [41050/45000]\n",
            "cost: 0.175925, accuracy: 0.920000  [41550/45000]\n",
            "cost: 0.262464, accuracy: 0.880000  [42050/45000]\n",
            "cost: 0.239911, accuracy: 0.900000  [42550/45000]\n",
            "cost: 0.172676, accuracy: 0.940000  [43050/45000]\n",
            "cost: 0.244778, accuracy: 0.880000  [43550/45000]\n",
            "cost: 0.299744, accuracy: 0.840000  [44050/45000]\n",
            "cost: 0.114902, accuracy: 0.960000  [44550/45000]\n",
            "cost: 0.232983, accuracy: 0.920000  [   50/45000]\n",
            "cost: 0.207750, accuracy: 0.960000  [  550/45000]\n",
            "cost: 0.124798, accuracy: 0.980000  [ 1050/45000]\n",
            "cost: 0.150124, accuracy: 0.940000  [ 1550/45000]\n",
            "cost: 0.144149, accuracy: 0.960000  [ 2050/45000]\n",
            "cost: 0.270570, accuracy: 0.900000  [ 2550/45000]\n",
            "cost: 0.136040, accuracy: 0.940000  [ 3050/45000]\n",
            "cost: 0.175121, accuracy: 0.940000  [ 3550/45000]\n",
            "cost: 0.273813, accuracy: 0.900000  [ 4050/45000]\n",
            "cost: 0.211954, accuracy: 0.900000  [ 4550/45000]\n",
            "cost: 0.175510, accuracy: 0.880000  [ 5050/45000]\n",
            "cost: 0.102413, accuracy: 0.980000  [ 5550/45000]\n",
            "cost: 0.193587, accuracy: 0.940000  [ 6050/45000]\n",
            "cost: 0.188316, accuracy: 0.900000  [ 6550/45000]\n",
            "cost: 0.165495, accuracy: 0.940000  [ 7050/45000]\n",
            "cost: 0.095120, accuracy: 0.980000  [ 7550/45000]\n",
            "cost: 0.083028, accuracy: 0.960000  [ 8050/45000]\n",
            "cost: 0.120264, accuracy: 0.960000  [ 8550/45000]\n",
            "cost: 0.190222, accuracy: 0.940000  [ 9050/45000]\n",
            "cost: 0.194591, accuracy: 0.940000  [ 9550/45000]\n",
            "cost: 0.348420, accuracy: 0.860000  [10050/45000]\n",
            "cost: 0.188824, accuracy: 0.940000  [10550/45000]\n",
            "cost: 0.111345, accuracy: 0.960000  [11050/45000]\n",
            "cost: 0.106479, accuracy: 0.980000  [11550/45000]\n",
            "cost: 0.211291, accuracy: 0.920000  [12050/45000]\n",
            "cost: 0.114975, accuracy: 0.980000  [12550/45000]\n",
            "cost: 0.207260, accuracy: 0.900000  [13050/45000]\n",
            "cost: 0.097364, accuracy: 0.980000  [13550/45000]\n",
            "cost: 0.098675, accuracy: 0.980000  [14050/45000]\n",
            "cost: 0.109726, accuracy: 0.960000  [14550/45000]\n",
            "cost: 0.145470, accuracy: 0.920000  [15050/45000]\n",
            "cost: 0.172199, accuracy: 0.900000  [15550/45000]\n",
            "cost: 0.152125, accuracy: 0.920000  [16050/45000]\n",
            "cost: 0.117889, accuracy: 1.000000  [16550/45000]\n",
            "cost: 0.158805, accuracy: 0.920000  [17050/45000]\n",
            "cost: 0.169794, accuracy: 0.920000  [17550/45000]\n",
            "cost: 0.257065, accuracy: 0.920000  [18050/45000]\n",
            "cost: 0.131721, accuracy: 0.960000  [18550/45000]\n",
            "cost: 0.078622, accuracy: 1.000000  [19050/45000]\n",
            "cost: 0.180633, accuracy: 0.920000  [19550/45000]\n",
            "cost: 0.230931, accuracy: 0.920000  [20050/45000]\n",
            "cost: 0.104990, accuracy: 0.980000  [20550/45000]\n",
            "cost: 0.121798, accuracy: 0.920000  [21050/45000]\n",
            "cost: 0.137101, accuracy: 0.960000  [21550/45000]\n",
            "cost: 0.190636, accuracy: 0.960000  [22050/45000]\n",
            "cost: 0.069391, accuracy: 0.980000  [22550/45000]\n",
            "cost: 0.198023, accuracy: 0.920000  [23050/45000]\n",
            "cost: 0.123970, accuracy: 0.960000  [23550/45000]\n",
            "cost: 0.206262, accuracy: 0.900000  [24050/45000]\n",
            "cost: 0.123229, accuracy: 0.960000  [24550/45000]\n",
            "cost: 0.159511, accuracy: 0.940000  [25050/45000]\n",
            "cost: 0.124038, accuracy: 0.940000  [25550/45000]\n",
            "cost: 0.148422, accuracy: 0.960000  [26050/45000]\n",
            "cost: 0.089969, accuracy: 0.960000  [26550/45000]\n",
            "cost: 0.310117, accuracy: 0.800000  [27050/45000]\n",
            "cost: 0.203289, accuracy: 0.920000  [27550/45000]\n",
            "cost: 0.177276, accuracy: 0.920000  [28050/45000]\n",
            "cost: 0.117285, accuracy: 0.960000  [28550/45000]\n",
            "cost: 0.097640, accuracy: 1.000000  [29050/45000]\n",
            "cost: 0.159071, accuracy: 0.920000  [29550/45000]\n",
            "cost: 0.078782, accuracy: 0.980000  [30050/45000]\n",
            "cost: 0.207194, accuracy: 0.920000  [30550/45000]\n",
            "cost: 0.103549, accuracy: 0.980000  [31050/45000]\n",
            "cost: 0.110986, accuracy: 0.980000  [31550/45000]\n",
            "cost: 0.222526, accuracy: 0.900000  [32050/45000]\n",
            "cost: 0.151300, accuracy: 0.920000  [32550/45000]\n",
            "cost: 0.114010, accuracy: 0.980000  [33050/45000]\n",
            "cost: 0.209757, accuracy: 0.920000  [33550/45000]\n",
            "cost: 0.047013, accuracy: 0.980000  [34050/45000]\n",
            "cost: 0.316713, accuracy: 0.880000  [34550/45000]\n",
            "cost: 0.210394, accuracy: 0.880000  [35050/45000]\n",
            "cost: 0.100497, accuracy: 0.960000  [35550/45000]\n",
            "cost: 0.091365, accuracy: 0.960000  [36050/45000]\n",
            "cost: 0.179294, accuracy: 0.920000  [36550/45000]\n",
            "cost: 0.142226, accuracy: 0.940000  [37050/45000]\n",
            "cost: 0.157113, accuracy: 0.940000  [37550/45000]\n",
            "cost: 0.192481, accuracy: 0.940000  [38050/45000]\n",
            "cost: 0.201755, accuracy: 0.920000  [38550/45000]\n",
            "cost: 0.128403, accuracy: 1.000000  [39050/45000]\n",
            "cost: 0.189099, accuracy: 0.920000  [39550/45000]\n",
            "cost: 0.161138, accuracy: 0.940000  [40050/45000]\n",
            "cost: 0.283479, accuracy: 0.880000  [40550/45000]\n",
            "cost: 0.080619, accuracy: 0.960000  [41050/45000]\n",
            "cost: 0.189547, accuracy: 0.920000  [41550/45000]\n",
            "cost: 0.208678, accuracy: 0.960000  [42050/45000]\n",
            "cost: 0.298872, accuracy: 0.920000  [42550/45000]\n",
            "cost: 0.117409, accuracy: 0.960000  [43050/45000]\n",
            "cost: 0.133990, accuracy: 0.960000  [43550/45000]\n",
            "cost: 0.114364, accuracy: 0.960000  [44050/45000]\n",
            "cost: 0.128393, accuracy: 0.960000  [44550/45000]\n",
            "cost: 0.212656, accuracy: 0.880000  [   50/45000]\n",
            "cost: 0.081494, accuracy: 0.980000  [  550/45000]\n",
            "cost: 0.087509, accuracy: 0.980000  [ 1050/45000]\n",
            "cost: 0.141873, accuracy: 0.960000  [ 1550/45000]\n",
            "cost: 0.113643, accuracy: 0.980000  [ 2050/45000]\n",
            "cost: 0.134915, accuracy: 0.940000  [ 2550/45000]\n",
            "cost: 0.222356, accuracy: 0.920000  [ 3050/45000]\n",
            "cost: 0.067583, accuracy: 0.960000  [ 3550/45000]\n",
            "cost: 0.098371, accuracy: 0.940000  [ 4050/45000]\n",
            "cost: 0.151608, accuracy: 0.940000  [ 4550/45000]\n",
            "cost: 0.180708, accuracy: 0.920000  [ 5050/45000]\n",
            "cost: 0.127627, accuracy: 0.920000  [ 5550/45000]\n",
            "cost: 0.160501, accuracy: 0.940000  [ 6050/45000]\n",
            "cost: 0.264798, accuracy: 0.900000  [ 6550/45000]\n",
            "cost: 0.134303, accuracy: 0.980000  [ 7050/45000]\n",
            "cost: 0.146352, accuracy: 0.920000  [ 7550/45000]\n",
            "cost: 0.225751, accuracy: 0.960000  [ 8050/45000]\n",
            "cost: 0.048950, accuracy: 0.980000  [ 8550/45000]\n",
            "cost: 0.169396, accuracy: 0.940000  [ 9050/45000]\n",
            "cost: 0.059780, accuracy: 1.000000  [ 9550/45000]\n",
            "cost: 0.171494, accuracy: 0.900000  [10050/45000]\n",
            "cost: 0.192807, accuracy: 0.900000  [10550/45000]\n",
            "cost: 0.042657, accuracy: 1.000000  [11050/45000]\n",
            "cost: 0.162569, accuracy: 0.900000  [11550/45000]\n",
            "cost: 0.174105, accuracy: 0.940000  [12050/45000]\n",
            "cost: 0.064236, accuracy: 0.980000  [12550/45000]\n",
            "cost: 0.127992, accuracy: 0.940000  [13050/45000]\n",
            "cost: 0.176178, accuracy: 0.900000  [13550/45000]\n",
            "cost: 0.134426, accuracy: 0.980000  [14050/45000]\n",
            "cost: 0.136076, accuracy: 0.960000  [14550/45000]\n",
            "cost: 0.050024, accuracy: 1.000000  [15050/45000]\n",
            "cost: 0.169913, accuracy: 0.920000  [15550/45000]\n",
            "cost: 0.176823, accuracy: 0.920000  [16050/45000]\n",
            "cost: 0.106638, accuracy: 0.940000  [16550/45000]\n",
            "cost: 0.130261, accuracy: 0.940000  [17050/45000]\n",
            "cost: 0.058838, accuracy: 1.000000  [17550/45000]\n",
            "cost: 0.087791, accuracy: 0.980000  [18050/45000]\n",
            "cost: 0.097350, accuracy: 0.960000  [18550/45000]\n",
            "cost: 0.172664, accuracy: 0.920000  [19050/45000]\n",
            "cost: 0.132026, accuracy: 0.940000  [19550/45000]\n",
            "cost: 0.082182, accuracy: 0.980000  [20050/45000]\n",
            "cost: 0.148420, accuracy: 0.940000  [20550/45000]\n",
            "cost: 0.099961, accuracy: 0.960000  [21050/45000]\n",
            "cost: 0.170886, accuracy: 0.940000  [21550/45000]\n",
            "cost: 0.142277, accuracy: 0.960000  [22050/45000]\n",
            "cost: 0.054677, accuracy: 0.980000  [22550/45000]\n",
            "cost: 0.058236, accuracy: 1.000000  [23050/45000]\n",
            "cost: 0.182997, accuracy: 0.920000  [23550/45000]\n",
            "cost: 0.190875, accuracy: 0.920000  [24050/45000]\n",
            "cost: 0.146964, accuracy: 0.940000  [24550/45000]\n",
            "cost: 0.136776, accuracy: 0.960000  [25050/45000]\n",
            "cost: 0.059812, accuracy: 0.980000  [25550/45000]\n",
            "cost: 0.113372, accuracy: 0.960000  [26050/45000]\n",
            "cost: 0.078722, accuracy: 1.000000  [26550/45000]\n",
            "cost: 0.161583, accuracy: 0.920000  [27050/45000]\n",
            "cost: 0.310302, accuracy: 0.880000  [27550/45000]\n",
            "cost: 0.107808, accuracy: 0.940000  [28050/45000]\n",
            "cost: 0.208237, accuracy: 0.900000  [28550/45000]\n",
            "cost: 0.199187, accuracy: 0.920000  [29050/45000]\n",
            "cost: 0.167578, accuracy: 0.900000  [29550/45000]\n",
            "cost: 0.100678, accuracy: 0.980000  [30050/45000]\n",
            "cost: 0.165927, accuracy: 0.880000  [30550/45000]\n",
            "cost: 0.108674, accuracy: 0.940000  [31050/45000]\n",
            "cost: 0.141941, accuracy: 0.940000  [31550/45000]\n",
            "cost: 0.156972, accuracy: 0.960000  [32050/45000]\n",
            "cost: 0.109984, accuracy: 0.960000  [32550/45000]\n",
            "cost: 0.084010, accuracy: 0.980000  [33050/45000]\n",
            "cost: 0.063854, accuracy: 1.000000  [33550/45000]\n",
            "cost: 0.195376, accuracy: 0.920000  [34050/45000]\n",
            "cost: 0.163255, accuracy: 0.880000  [34550/45000]\n",
            "cost: 0.104512, accuracy: 0.960000  [35050/45000]\n",
            "cost: 0.197411, accuracy: 0.900000  [35550/45000]\n",
            "cost: 0.116406, accuracy: 0.920000  [36050/45000]\n",
            "cost: 0.110588, accuracy: 0.960000  [36550/45000]\n",
            "cost: 0.213575, accuracy: 0.920000  [37050/45000]\n",
            "cost: 0.172621, accuracy: 0.920000  [37550/45000]\n",
            "cost: 0.104352, accuracy: 0.940000  [38050/45000]\n",
            "cost: 0.184126, accuracy: 0.940000  [38550/45000]\n",
            "cost: 0.215762, accuracy: 0.920000  [39050/45000]\n",
            "cost: 0.078566, accuracy: 1.000000  [39550/45000]\n",
            "cost: 0.120555, accuracy: 0.940000  [40050/45000]\n",
            "cost: 0.127678, accuracy: 0.940000  [40550/45000]\n",
            "cost: 0.157377, accuracy: 0.940000  [41050/45000]\n",
            "cost: 0.102176, accuracy: 0.960000  [41550/45000]\n",
            "cost: 0.082956, accuracy: 0.940000  [42050/45000]\n",
            "cost: 0.178013, accuracy: 0.900000  [42550/45000]\n",
            "cost: 0.130796, accuracy: 0.980000  [43050/45000]\n",
            "cost: 0.163625, accuracy: 0.940000  [43550/45000]\n",
            "cost: 0.075180, accuracy: 0.980000  [44050/45000]\n",
            "cost: 0.190353, accuracy: 0.940000  [44550/45000]\n",
            "cost: 0.118349, accuracy: 0.980000  [   50/45000]\n",
            "cost: 0.070835, accuracy: 0.960000  [  550/45000]\n",
            "cost: 0.044551, accuracy: 1.000000  [ 1050/45000]\n",
            "cost: 0.086149, accuracy: 0.960000  [ 1550/45000]\n",
            "cost: 0.016004, accuracy: 1.000000  [ 2050/45000]\n",
            "cost: 0.095978, accuracy: 0.980000  [ 2550/45000]\n",
            "cost: 0.081458, accuracy: 0.980000  [ 3050/45000]\n",
            "cost: 0.115030, accuracy: 0.960000  [ 3550/45000]\n",
            "cost: 0.048973, accuracy: 1.000000  [ 4050/45000]\n",
            "cost: 0.156259, accuracy: 0.920000  [ 4550/45000]\n",
            "cost: 0.132029, accuracy: 0.940000  [ 5050/45000]\n",
            "cost: 0.189410, accuracy: 0.920000  [ 5550/45000]\n",
            "cost: 0.057952, accuracy: 1.000000  [ 6050/45000]\n",
            "cost: 0.157116, accuracy: 0.960000  [ 6550/45000]\n",
            "cost: 0.099560, accuracy: 0.940000  [ 7050/45000]\n",
            "cost: 0.083084, accuracy: 0.960000  [ 7550/45000]\n",
            "cost: 0.193811, accuracy: 0.920000  [ 8050/45000]\n",
            "cost: 0.045095, accuracy: 0.980000  [ 8550/45000]\n",
            "cost: 0.035162, accuracy: 1.000000  [ 9050/45000]\n",
            "cost: 0.150515, accuracy: 0.940000  [ 9550/45000]\n",
            "cost: 0.102815, accuracy: 0.980000  [10050/45000]\n",
            "cost: 0.226184, accuracy: 0.940000  [10550/45000]\n",
            "cost: 0.114031, accuracy: 0.940000  [11050/45000]\n",
            "cost: 0.183343, accuracy: 0.960000  [11550/45000]\n",
            "cost: 0.096902, accuracy: 0.940000  [12050/45000]\n",
            "cost: 0.078716, accuracy: 0.980000  [12550/45000]\n",
            "cost: 0.176390, accuracy: 0.900000  [13050/45000]\n",
            "cost: 0.089486, accuracy: 0.960000  [13550/45000]\n",
            "cost: 0.054201, accuracy: 1.000000  [14050/45000]\n",
            "cost: 0.034498, accuracy: 1.000000  [14550/45000]\n",
            "cost: 0.056429, accuracy: 0.980000  [15050/45000]\n",
            "cost: 0.050385, accuracy: 1.000000  [15550/45000]\n",
            "cost: 0.090818, accuracy: 0.980000  [16050/45000]\n",
            "cost: 0.049282, accuracy: 1.000000  [16550/45000]\n",
            "cost: 0.121650, accuracy: 0.960000  [17050/45000]\n",
            "cost: 0.067790, accuracy: 0.980000  [17550/45000]\n",
            "cost: 0.081845, accuracy: 0.960000  [18050/45000]\n",
            "cost: 0.162787, accuracy: 0.940000  [18550/45000]\n",
            "cost: 0.094383, accuracy: 0.980000  [19050/45000]\n",
            "cost: 0.116801, accuracy: 0.960000  [19550/45000]\n",
            "cost: 0.089147, accuracy: 0.980000  [20050/45000]\n",
            "cost: 0.063364, accuracy: 0.960000  [20550/45000]\n",
            "cost: 0.056968, accuracy: 1.000000  [21050/45000]\n",
            "cost: 0.060086, accuracy: 0.980000  [21550/45000]\n",
            "cost: 0.114444, accuracy: 0.960000  [22050/45000]\n",
            "cost: 0.046877, accuracy: 0.980000  [22550/45000]\n",
            "cost: 0.111836, accuracy: 0.960000  [23050/45000]\n",
            "cost: 0.066990, accuracy: 0.960000  [23550/45000]\n",
            "cost: 0.088190, accuracy: 0.960000  [24050/45000]\n",
            "cost: 0.144804, accuracy: 0.900000  [24550/45000]\n",
            "cost: 0.190714, accuracy: 0.920000  [25050/45000]\n",
            "cost: 0.102046, accuracy: 0.960000  [25550/45000]\n",
            "cost: 0.107112, accuracy: 0.960000  [26050/45000]\n",
            "cost: 0.087115, accuracy: 0.980000  [26550/45000]\n",
            "cost: 0.019943, accuracy: 1.000000  [27050/45000]\n",
            "cost: 0.191116, accuracy: 0.900000  [27550/45000]\n",
            "cost: 0.159241, accuracy: 0.940000  [28050/45000]\n",
            "cost: 0.201850, accuracy: 0.920000  [28550/45000]\n",
            "cost: 0.079812, accuracy: 0.980000  [29050/45000]\n",
            "cost: 0.086433, accuracy: 0.960000  [29550/45000]\n",
            "cost: 0.058332, accuracy: 0.980000  [30050/45000]\n",
            "cost: 0.088926, accuracy: 0.960000  [30550/45000]\n",
            "cost: 0.067493, accuracy: 0.980000  [31050/45000]\n",
            "cost: 0.198670, accuracy: 0.880000  [31550/45000]\n",
            "cost: 0.129404, accuracy: 0.960000  [32050/45000]\n",
            "cost: 0.089924, accuracy: 0.960000  [32550/45000]\n",
            "cost: 0.043934, accuracy: 1.000000  [33050/45000]\n",
            "cost: 0.105834, accuracy: 0.940000  [33550/45000]\n",
            "cost: 0.171519, accuracy: 0.920000  [34050/45000]\n",
            "cost: 0.207443, accuracy: 0.880000  [34550/45000]\n",
            "cost: 0.012619, accuracy: 1.000000  [35050/45000]\n",
            "cost: 0.149433, accuracy: 0.920000  [35550/45000]\n",
            "cost: 0.165688, accuracy: 0.920000  [36050/45000]\n",
            "cost: 0.201536, accuracy: 0.920000  [36550/45000]\n",
            "cost: 0.056522, accuracy: 0.980000  [37050/45000]\n",
            "cost: 0.085622, accuracy: 0.960000  [37550/45000]\n",
            "cost: 0.038054, accuracy: 1.000000  [38050/45000]\n",
            "cost: 0.143941, accuracy: 0.960000  [38550/45000]\n",
            "cost: 0.098500, accuracy: 0.960000  [39050/45000]\n",
            "cost: 0.090665, accuracy: 0.960000  [39550/45000]\n",
            "cost: 0.180683, accuracy: 0.900000  [40050/45000]\n",
            "cost: 0.044897, accuracy: 0.980000  [40550/45000]\n",
            "cost: 0.075561, accuracy: 0.940000  [41050/45000]\n",
            "cost: 0.037052, accuracy: 1.000000  [41550/45000]\n",
            "cost: 0.092247, accuracy: 0.960000  [42050/45000]\n",
            "cost: 0.201833, accuracy: 0.940000  [42550/45000]\n",
            "cost: 0.143871, accuracy: 0.920000  [43050/45000]\n",
            "cost: 0.073816, accuracy: 0.960000  [43550/45000]\n",
            "cost: 0.082066, accuracy: 0.960000  [44050/45000]\n",
            "cost: 0.111511, accuracy: 0.940000  [44550/45000]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy: 85.86, best accuracy: 86.44\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import InputLayer\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn import model_selection\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "device=torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "\n",
        "data=pd.read_csv(\"xaa\",encoding=\"utf-8\")\n",
        "\n",
        "word2vec_model = KeyedVectors.load_word2vec_format(\"drive/MyDrive/Colab Notebooks/GoogleNews-vectors-negative300.bin.gz\", binary=True, limit=20000)\n",
        "#print(word2vec_model.get_index(\"the\"))\n",
        "vect  = CountVectorizer(stop_words=\"english\",max_df=0.7)\n",
        "#corpus = vect.fit_transform(data[\"text\"])\n",
        "#train_corpus, test_corpus, train_label, test_label = model_selection.train_test_split(data[\"text\"],data[\"label\"],test_size=0.4)\n",
        "#Encoder = LabelEncoder()\n",
        "#train_label = Encoder.fit_transform(train_label)\n",
        "#test_label = Encoder.fit_transform(test_label)\n",
        "#train_corpus_vect=vect.transform(train_corpus)\n",
        "#test_corpus_vect=vect.transform(test_corpus)\n",
        "\n",
        "# Initalise vect.vocabulary_\n",
        "vect.fit_transform(data[\"text\"])\n",
        "\n",
        "maxlen=0\n",
        "\n",
        "def transform(text,vect):\n",
        "    global maxlen\n",
        "    global word2vec_model\n",
        "    #d=vect.vocabulary_\n",
        "    d=word2vec_model\n",
        "    p=vect.build_preprocessor()\n",
        "    t=vect.build_tokenizer()\n",
        "    vec_list=[]\n",
        "    for doc in text:\n",
        "        tokens=t(p(doc))\n",
        "        doc_vec=np.array([d.get_index(token) for token in tokens if token in d])\n",
        "        s=len(doc_vec)\n",
        "        if s>maxlen:\n",
        "            maxlen=s\n",
        "        #doc_vec=sequence.pad_sequences(doc_vec,maxlen=maxlen)\n",
        "        vec_list.append(doc_vec)\n",
        "    vec_list=sequence.pad_sequences(vec_list,maxlen=maxlen,padding=\"post\")\n",
        "    corpus_vec=np.vstack(vec_list)\n",
        "    #return nn.functional.normalize(torch.tensor(corpus_vec).float())\n",
        "    return torch.tensor(corpus_vec)\n",
        "    #return torch.tensor(corpus_vec).float()\n",
        "\n",
        "# print(corpus_vec)\n",
        "\n",
        "bsize=50\n",
        "epochs=10\n",
        "lr=5e-4\n",
        "embed_dim=300\n",
        "\n",
        "class corpus(Dataset):\n",
        "    def __init__(self,corpus,label,seq):\n",
        "        self.corpus=corpus\n",
        "        self.label=label\n",
        "        self.seq=seq\n",
        "    def __len__(self):\n",
        "        return len(self.corpus)\n",
        "    def __getitem__(self,idx):\n",
        "        return self.corpus[idx],self.label[idx]\n",
        "\n",
        "class lstm(nn.Module):\n",
        "    def __init__(self,input_size,hidden_size,seq):\n",
        "        super(lstm,self).__init__()\n",
        "        self.input_size=input_size\n",
        "        self.hidden_size=hidden_size\n",
        "        self.seq=seq\n",
        "        self.rnn=True\n",
        "        self.lstm=nn.LSTM(input_size,hidden_size,batch_first=True,num_layers=1)\n",
        "        self.fc=nn.Linear(self.hidden_size*seq,1)\n",
        "        #self.fc=nn.Linear(self.hidden_size,1)\n",
        "        #self.embed=nn.Embedding(len(vect.vocabulary_),input_size)\n",
        "        self.embed=nn.Embedding.from_pretrained(torch.from_numpy(word2vec_model.vectors),freeze=True)\n",
        "\n",
        "    def forward(self,x,h0=None,c0=None):\n",
        "        x=self.embed(x)\n",
        "        if h0==None and c0==None:\n",
        "            x, (hn,cn) = self.lstm(x)\n",
        "        else:\n",
        "            x, (hn,cn) = self.lstm(x,(h0,c0))\n",
        "        #print(x[:,-1,:].shape)\n",
        "        #x = torch.flatten(x[:,-1,:],1)\n",
        "        # Flatten like this so that all information from previous time steps is fed into fully connected layer\n",
        "        x=torch.flatten(x,1)\n",
        "        x = self.fc(x)\n",
        "        return nn.Sigmoid()(x), hn,cn\n",
        "        #return nn.Softmax()(x), hn,cn\n",
        "\n",
        "class dense(nn.Module):\n",
        "    def __init__(self,seq,vocab,embed_dim):\n",
        "        super(dense,self).__init__()\n",
        "        self.seq=seq\n",
        "        self.rnn=False\n",
        "        self.network=nn.Sequential(\n",
        "            nn.Linear(embed_dim*seq,360),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(360,180),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(180,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.embed=nn.Embedding(vocab,embed_dim)\n",
        "\n",
        "    def forward(self,x):\n",
        "        #print(x.shape)\n",
        "        x=self.embed(x)\n",
        "        x=torch.flatten(x,1)\n",
        "        #x=torch.transpose(x,1,2)\n",
        "        #x=torch.flatten(x,1)\n",
        "        return self.network(x)\n",
        "\n",
        "vocab=len(vect.vocabulary_)\n",
        "Encoder = LabelEncoder()\n",
        "corpus_vec = transform(data[\"text\"],vect)\n",
        "train_corpus_vec, test_corpus_vec, train_label, test_label = model_selection.train_test_split(corpus_vec,data[\"label\"],test_size=0.1)\n",
        "train_label = torch.from_numpy(Encoder.fit_transform(train_label)).float()\n",
        "test_label = torch.from_numpy(Encoder.fit_transform(test_label)).float()\n",
        "#train_corpus_vec = transform(train_corpus,vect)\n",
        "#test_corpus_vec = transform(test_corpus,vect)\n",
        "lstm_classifier=lstm(embed_dim,200,maxlen)\n",
        "loss_fn=nn.BCELoss()\n",
        "#loss_fn=nn.BCEWithLogitsLoss()\n",
        "#loss_fn=nn.MSELoss()\n",
        "optimizer=torch.optim.Adam(lstm_classifier.parameters(),lr=lr)\n",
        "#optimizer=torch.optim.SGD(lstm_classifier.parameters(),lr=lr)\n",
        "#print(len(lstm_classifier(torch.reshape(train_corpus_vec[0],(bsize,maxlen,1)))))\n",
        "\n",
        "dense_classifier=dense(maxlen,vocab,16)\n",
        "\n",
        "c=lstm_classifier\n",
        "\n",
        "c=c.to(device)\n",
        "\n",
        "train_dataloader=DataLoader(corpus(train_corpus_vec,train_label,maxlen),batch_size=bsize,shuffle=True)\n",
        "test_dataloader=DataLoader(corpus(test_corpus_vec,test_label,maxlen),batch_size=bsize,shuffle=True)\n",
        "\n",
        "def train(dataloader,model,loss_fn,optimizer):\n",
        "    hn,cn=None,None\n",
        "    size=len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch,(x,y) in enumerate(dataloader):\n",
        "        x,y=x.to(device),y.to(device)\n",
        "        #print(torch.sum(x[0]!=0))\n",
        "        if model.rnn==True:\n",
        "            #pred,hn,cn=model(x.reshape(bsize,dataloader.dataset.seq,1).to(device),hn,cn)\n",
        "            pred,hn,cn=model(x,hn,cn)\n",
        "        else:\n",
        "            pred=model(x)\n",
        "        #pred=model(x)\n",
        "        cost=loss_fn(pred.flatten(),y)\n",
        "        cost.backward()\n",
        "        #if model.rnn==True:\n",
        "        #    print(model.lstm.weight_ih_l0.grad[0][0])\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if model.rnn==True:\n",
        "            hn=hn.detach()\n",
        "            cn=cn.detach()\n",
        "        if batch % 10 == 0:\n",
        "            cost_val, current = cost.item(), batch * bsize + len(x)\n",
        "            print(f\"cost: {cost_val:>7f}, accuracy: {(torch.round(pred.flatten())==y).sum().item()/bsize:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x,y=x.to(device),y.to(device)\n",
        "            if model.rnn==True:\n",
        "                #pred,_,_=model(x.reshape(bsize,dataloader.dataset.seq,1).to(device))\n",
        "                pred,_,_=model(x)\n",
        "            else:\n",
        "                pred=model(x)\n",
        "            #pred=model(x)\n",
        "            #print(torch.round(pred.flatten()),y)\n",
        "            test_loss += loss_fn(pred.flatten(), y).item()\n",
        "            ncorrect = (torch.round(pred.flatten()) == y).sum().item()\n",
        "            #print(ncorrect)\n",
        "            correct += ncorrect\n",
        "\n",
        "    #print(correct,size)\n",
        "    #print(f\"Test Error: \\n Accuracy: {(100*correct/size):>0.1f}%, Avg loss: {100*test_loss/size:>8f} \\n\")\n",
        "    return 100*correct/size\n",
        "\n",
        "#keras_model=Sequential([InputLayer(input_shape=(maxlen,),batch_size=bsize),\n",
        "#                        Embedding(len(vect.vocabulary_),16),\n",
        "#                  Flatten(),\n",
        "#                  Dense(1,activation=\"sigmoid\")])\n",
        "keras_model=Sequential([InputLayer(shape=(maxlen,),batch_size=bsize),\n",
        "                        Embedding(len(word2vec_model),embed_dim),\n",
        "                        LSTM(20,return_sequences=True),\n",
        "                  Flatten(),\n",
        "                  Dense(1,activation=\"sigmoid\")])\n",
        "keras_model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
        "keras_model.summary()\n",
        "\n",
        "use_torch=True\n",
        "\n",
        "x=np.arange(epochs)\n",
        "y=np.zeros(epochs)\n",
        "#print(corpus_vec[0])\n",
        "if use_torch==True:\n",
        "    for epoch in range(epochs):\n",
        "        train(train_dataloader,c,loss_fn,optimizer)\n",
        "        y[epoch] = test_loop(test_dataloader,c,loss_fn)\n",
        "    plt.plot(x,y)\n",
        "    plt.show()\n",
        "    print(f\"Final accuracy: {y[-1]:.2f}, best accuracy: {y[np.argmax(y)]:.2f}\")\n",
        "else:\n",
        "    keras_model.fit(train_corpus_vec,train_label,batch_size=bsize,epochs=epochs,validation_data=(test_corpus_vec,test_label))\n",
        "    keras_model.evaluate(test_corpus_vec,test_label,batch_size=bsize)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "cy5ekCL0mHOK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "outputId": "e39f3001-2f32-4ce6-fab0-09c8942b7283"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m118.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "scipy"
                ]
              },
              "id": "582aa023f51e442fa9099689695ad27f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vx6v266o_lK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(word2vec_model))\n",
        "word2vec_model[\"horse\"]"
      ],
      "metadata": {
        "id": "0EdHfJkrFBpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93a61279-c917-4845-f223-6b7729de06f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5.34057617e-04,  3.11279297e-02,  5.03540039e-03, -9.17968750e-02,\n",
              "       -8.36181641e-03, -1.66015625e-01,  3.93066406e-02,  2.97851562e-02,\n",
              "        1.69921875e-01, -2.04101562e-01,  2.41210938e-01, -3.04687500e-01,\n",
              "       -2.24609375e-02, -3.71093750e-01, -5.61523438e-02,  1.51367188e-01,\n",
              "       -1.21582031e-01,  3.41796875e-01,  3.05175781e-02, -2.94921875e-01,\n",
              "        6.54296875e-02, -9.27734375e-02,  1.49414062e-01,  8.15429688e-02,\n",
              "       -6.93359375e-02,  1.98242188e-01, -1.66015625e-01,  2.00195312e-01,\n",
              "        1.16699219e-01, -3.69140625e-01, -2.48046875e-01,  1.25976562e-01,\n",
              "        3.59375000e-01,  1.51367188e-01, -7.76367188e-02,  2.91015625e-01,\n",
              "       -1.74560547e-02, -1.21093750e-01, -1.00097656e-01,  1.43554688e-01,\n",
              "        5.92041016e-03,  2.35595703e-02,  3.20312500e-01,  1.82617188e-01,\n",
              "        9.52148438e-02,  9.22851562e-02,  8.30078125e-02, -1.33789062e-01,\n",
              "        9.57031250e-02,  1.66992188e-01,  1.87988281e-02, -2.79541016e-02,\n",
              "       -1.03149414e-02,  1.11816406e-01, -8.10546875e-02,  1.79687500e-01,\n",
              "        8.34960938e-02, -5.90820312e-02,  1.70898438e-01,  1.68457031e-02,\n",
              "        8.74023438e-02,  9.03320312e-02,  9.22851562e-02, -9.76562500e-02,\n",
              "       -2.39257812e-02, -2.07031250e-01, -1.21459961e-02,  1.44531250e-01,\n",
              "        6.20117188e-02, -1.44042969e-02,  2.81250000e-01,  6.83593750e-02,\n",
              "       -2.12890625e-01,  4.68750000e-02, -1.00097656e-01, -1.35742188e-01,\n",
              "        7.47070312e-02, -2.69531250e-01,  7.56835938e-02, -5.51757812e-02,\n",
              "       -2.01416016e-02, -3.80859375e-01,  1.67968750e-01, -3.90625000e-01,\n",
              "        5.54199219e-02,  2.23632812e-01, -6.28662109e-03,  7.76367188e-02,\n",
              "        2.03125000e-01, -1.04980469e-01, -3.12500000e-02,  3.53515625e-01,\n",
              "        1.54296875e-01, -1.25976562e-01, -2.24609375e-02, -3.80859375e-01,\n",
              "        3.12500000e-01,  2.12890625e-01,  1.97265625e-01, -4.49218750e-01,\n",
              "       -9.22851562e-02, -2.94921875e-01,  2.21679688e-01, -2.58789062e-02,\n",
              "        1.38671875e-01,  7.37304688e-02, -1.10839844e-01, -3.00781250e-01,\n",
              "        1.29882812e-01, -1.74804688e-01,  1.37939453e-02,  7.51953125e-02,\n",
              "        2.98828125e-01, -7.61718750e-02,  9.76562500e-02, -2.53906250e-01,\n",
              "       -8.69140625e-02, -9.86328125e-02, -4.49218750e-02,  1.00585938e-01,\n",
              "       -1.68945312e-01,  1.06933594e-01, -1.07910156e-01, -3.57421875e-01,\n",
              "       -3.58886719e-02, -3.24707031e-02,  1.62109375e-01, -1.17187500e-01,\n",
              "       -1.49414062e-01,  2.31933594e-02, -4.14062500e-01,  2.85644531e-02,\n",
              "       -2.57812500e-01, -6.59179688e-02, -4.80957031e-02, -1.87500000e-01,\n",
              "        5.10253906e-02,  7.86132812e-02, -7.56835938e-02,  3.20312500e-01,\n",
              "        7.08007812e-02, -1.08886719e-01, -3.03955078e-02,  1.53320312e-01,\n",
              "        8.93554688e-02,  8.83789062e-02, -2.01416016e-02, -1.66992188e-01,\n",
              "       -8.88671875e-02,  7.20214844e-03,  4.88281250e-01,  7.66601562e-02,\n",
              "        6.68945312e-02, -4.34570312e-02, -2.41210938e-01, -9.71679688e-02,\n",
              "        7.47070312e-02, -3.24218750e-01, -3.14453125e-01, -8.74023438e-02,\n",
              "       -8.20312500e-02,  2.33154297e-02,  2.55859375e-01, -1.06445312e-01,\n",
              "        1.13769531e-01, -4.71191406e-02, -4.83398438e-02, -4.31640625e-01,\n",
              "        3.41796875e-02,  1.66015625e-02, -6.00585938e-02, -1.82617188e-01,\n",
              "       -3.56445312e-02, -8.59375000e-02, -5.88378906e-02,  1.19628906e-01,\n",
              "        1.33789062e-01, -3.73535156e-02, -1.79687500e-01,  8.59375000e-02,\n",
              "       -2.63671875e-01, -3.36914062e-02, -2.22167969e-02,  9.52148438e-02,\n",
              "        1.05468750e-01, -1.63085938e-01,  2.65625000e-01,  1.77734375e-01,\n",
              "       -2.37304688e-01,  1.58203125e-01, -4.00390625e-02, -5.71289062e-02,\n",
              "        1.51367188e-01,  1.52343750e-01,  2.71484375e-01, -2.13867188e-01,\n",
              "        1.81884766e-02, -4.98046875e-02, -2.65625000e-01, -1.05468750e-01,\n",
              "        4.98046875e-02, -5.54687500e-01, -2.91015625e-01, -9.61914062e-02,\n",
              "       -9.52148438e-02, -4.17480469e-02, -2.57812500e-01, -6.25000000e-02,\n",
              "        9.37500000e-02, -2.09960938e-02, -2.48046875e-01,  4.51660156e-02,\n",
              "        1.56250000e-01, -1.74804688e-01, -1.37695312e-01,  1.54296875e-01,\n",
              "       -3.94531250e-01, -1.66992188e-01,  2.07031250e-01,  1.55273438e-01,\n",
              "        2.08984375e-01,  2.69531250e-01,  3.26171875e-01, -1.24023438e-01,\n",
              "        1.64794922e-02, -5.05371094e-02,  3.10546875e-01, -3.24707031e-02,\n",
              "        3.41796875e-01, -9.96093750e-02,  4.15039062e-02, -3.06640625e-01,\n",
              "        4.83398438e-02,  2.08007812e-01, -4.10156250e-01, -7.22656250e-02,\n",
              "        1.09863281e-01, -2.67578125e-01,  2.28515625e-01, -1.02050781e-01,\n",
              "        1.83593750e-01, -2.44140625e-01,  2.21679688e-01,  2.01171875e-01,\n",
              "        3.10546875e-01, -2.14843750e-01, -3.41796875e-02,  2.57812500e-01,\n",
              "        2.53906250e-02,  7.66601562e-02, -1.18164062e-01,  2.41210938e-01,\n",
              "       -9.61914062e-02, -1.16699219e-01, -8.34960938e-02,  2.37304688e-01,\n",
              "       -1.10839844e-01,  6.73828125e-02, -4.35546875e-01, -3.32641602e-03,\n",
              "       -1.27929688e-01, -6.53076172e-03,  2.72216797e-02,  2.83203125e-01,\n",
              "       -4.19921875e-02,  8.00781250e-02,  1.12792969e-01, -2.73437500e-01,\n",
              "       -2.63671875e-01,  2.04101562e-01, -8.54492188e-02,  1.91497803e-03,\n",
              "       -2.04101562e-01,  6.83593750e-02, -6.54296875e-02, -1.35742188e-01,\n",
              "       -2.57568359e-02, -3.96484375e-01, -2.94189453e-02,  4.33593750e-01,\n",
              "        6.03027344e-02,  3.41796875e-03,  1.74804688e-01,  6.44531250e-02,\n",
              "       -3.97949219e-02, -7.32421875e-02, -1.50390625e-01,  2.56347656e-02,\n",
              "        2.91015625e-01,  2.61718750e-01, -2.32421875e-01,  1.13769531e-01,\n",
              "       -2.53906250e-01,  1.75781250e-01,  1.89453125e-01,  2.65625000e-01,\n",
              "        1.66015625e-01,  2.85156250e-01, -1.63085938e-01,  6.07910156e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Md7zxWTk9AhSbKVVPH3m-w-W4QP5_WDX",
      "authorship_tag": "ABX9TyMP/GxX0BXLY2LeyK77p2py",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}