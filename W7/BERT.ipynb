{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNl0HZ0IJ1LBRp/cHzjt6SX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nwon24/nlp/blob/main/W7/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning a BERT model for text classification\n",
        "\n",
        "BERT (Bidirectional encoder representations from transformers) is a special kind of transfomer architecture that has wide applications in NLP."
      ],
      "metadata": {
        "id": "wzYrLChnL-Rs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "BEDRmYUCMmAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torch.optim import Adam\n",
        "from torch.nn import BCELoss\n",
        "from torch import accelerator\n",
        "from torch import tensor\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import AutoTokenizer\n"
      ],
      "metadata": {
        "id": "W_Wnu63fMn1O"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here it's handy as well to set a variable that will hold the device we want to train the model on. That way we don't have to manually change the device if we no longer have  GPU."
      ],
      "metadata": {
        "id": "W9NgZIrKSmUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device=accelerator.current_accelerator().type if accelerator.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "-Iss-FeXSvwE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "m_GTy090VnTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=8\n",
        "lr=1e-5\n",
        "epochs=5"
      ],
      "metadata": {
        "id": "-6gFEiHYVpq_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation and tokenisation\n",
        "\n",
        "Our corpus is a CSV file of Amazon reviews eith three columns: label, title of the review, and the text of the review itself.\n",
        "\n",
        "For the tokeniser, we are lucky to have a pretrained tokeniser from `AutoTokenizer`. This tokenizer returns the tokens of the corpus in a special dictionary, which will be used in the `DataLoader` class to feed the input correctly into the model. The output of the tokenizer is a dictionary with keys `input_ids`, `token_type_ids`, and `attention_mask`, so we can't just use our homeegrown tokenizer or that of `CounntVectorizer` anymore. We can, however, still use the `LabelEncoder` from `sklearn` to convert our labels into ones and zeroes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LUvE2iKaNWar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_file=\"xaa\"\n",
        "data=pd.read_csv(corpus_file,encoding=\"utf-8\")\n",
        "trainX,testX,trainY,testY=train_test_split(data[\"text\"],data[\"label\"],test_size=0.1)\n",
        "\n",
        "label_encoder=LabelEncoder()\n",
        "\n",
        "tokenizer=AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
        "train_tokens=tokenizer(list(trainX),truncation=True,padding=True)\n",
        "test_tokens=tokenizer(list(testX),truncation=True,padding=True)\n",
        "train_labels=label_encoder.fit_transform(trainY)\n",
        "test_labels=label_encoder.transform(testY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRf3_aC-NdaI",
        "outputId": "8cc66575-5842-4c8f-cac6-e5177a9291f8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the data\n",
        "\n",
        "Models implemented in `torch` can be trained by feeding input data through the `DataLoader` and `DataSet` classes. This ensures that the data comes in the specified batches and that the data is also shuffled around.\n",
        "\n",
        "To implement our own class that inherits from `DataLoader`, we just need to define a copy of basic methods: a method to return the amount of data we have, and a method to return the both the input and and the corresponding output given an index. In this case we have included the labels in the dictionary that is returned, but we could also have returned the labels separately."
      ],
      "metadata": {
        "id": "Xb8wJ9vAPcNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class token_data(Dataset):\n",
        "    def __init__(self,data,tokens,labels):\n",
        "        # self.data will hold raw data (not tokenized) for __len__ method\n",
        "        self.data=data\n",
        "        self.tokens=tokens\n",
        "        self.labels=labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        item={}\n",
        "        for k,v in self.tokens.items():\n",
        "            item[k]=tensor(v[idx])\n",
        "        item[\"labels\"]=tensor(self.labels[idx]).float()\n",
        "        return item"
      ],
      "metadata": {
        "id": "USvxNkJaP8u6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our  own `token_data` class, all that is required is to wrap our data using the `DataLoader` class."
      ],
      "metadata": {
        "id": "izjxJ0mnSSpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader=DataLoader(token_data(trainX,train_tokens,train_labels),shuffle=True,batch_size=batch_size)\n",
        "test_loader=DataLoader(token_data(testX,test_tokens,test_labels),shuffle=True,batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Lzm5qgBLSi_i"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the pretrained BERT model\n",
        "\n",
        "We use binary cross entropy loss because our classification task is binary---the reviews are either positive or negative."
      ],
      "metadata": {
        "id": "-vQdA4SWSMD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=BertForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\").to(device)\n",
        "optimizer=Adam(model.parameters(),lr=lr)\n",
        "loss_fn=BCELoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v84aJ7X8TNOV",
        "outputId": "d4af7cc0-c84e-454b-facc-f8182be370d6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n"
      ],
      "metadata": {
        "id": "dGPPDMWYVc3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i,batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids=batch[\"input_ids\"].to(device)\n",
        "        attention_mask=batch[\"attention_mask\"].to(device)\n",
        "        labels=batch[\"labels\"].to(device)\n",
        "\n",
        "        prediction=model(input_ids,attention_mask=attention_mask)\n",
        "        loss=loss_fn(prediction.logits.max(1)[0],labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_loss=loss.item()\n",
        "        print(f\"Batch {i}, loss {batch_loss/batch_size:.2f}\")\n",
        "    print(f\"Epoch {epoch}, loss {batch_loss/batch_size:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaQgYCkDV335",
        "outputId": "938a887a-27cc-458e-ceab-3569726ac271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0, loss 0.09\n",
            "Batch 1, loss 0.10\n",
            "Batch 2, loss 0.10\n",
            "Batch 3, loss 0.10\n",
            "Batch 4, loss 0.09\n"
          ]
        }
      ]
    }
  ]
}