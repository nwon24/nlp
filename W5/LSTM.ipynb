{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nwon24/nlp/blob/main/W5/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWAcH6mWYxkE",
        "outputId": "1df441c8-5c6b-4968-cc78-88160213e1f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost: 0.682545, accuracy: 0.580000  [   50/16000]\n",
            "cost: 0.685732, accuracy: 0.540000  [  550/16000]\n",
            "cost: 0.697085, accuracy: 0.480000  [ 1050/16000]\n",
            "cost: 0.699705, accuracy: 0.500000  [ 1550/16000]\n",
            "cost: 0.696656, accuracy: 0.480000  [ 2050/16000]\n",
            "cost: 0.699588, accuracy: 0.480000  [ 2550/16000]\n",
            "cost: 0.688086, accuracy: 0.500000  [ 3050/16000]\n",
            "cost: 0.692784, accuracy: 0.600000  [ 3550/16000]\n",
            "cost: 0.700829, accuracy: 0.440000  [ 4050/16000]\n",
            "cost: 0.696718, accuracy: 0.500000  [ 4550/16000]\n",
            "cost: 0.695934, accuracy: 0.500000  [ 5050/16000]\n",
            "cost: 0.694003, accuracy: 0.500000  [ 5550/16000]\n",
            "cost: 0.702279, accuracy: 0.460000  [ 6050/16000]\n",
            "cost: 0.707941, accuracy: 0.380000  [ 6550/16000]\n",
            "cost: 0.686886, accuracy: 0.560000  [ 7050/16000]\n",
            "cost: 0.696108, accuracy: 0.460000  [ 7550/16000]\n",
            "cost: 0.695306, accuracy: 0.500000  [ 8050/16000]\n",
            "cost: 0.690653, accuracy: 0.560000  [ 8550/16000]\n",
            "cost: 0.699969, accuracy: 0.520000  [ 9050/16000]\n",
            "cost: 0.686982, accuracy: 0.540000  [ 9550/16000]\n",
            "cost: 0.699763, accuracy: 0.400000  [10050/16000]\n",
            "cost: 0.688641, accuracy: 0.580000  [10550/16000]\n",
            "cost: 0.688019, accuracy: 0.560000  [11050/16000]\n",
            "cost: 0.689778, accuracy: 0.540000  [11550/16000]\n",
            "cost: 0.695193, accuracy: 0.480000  [12050/16000]\n",
            "cost: 0.707834, accuracy: 0.400000  [12550/16000]\n",
            "cost: 0.692222, accuracy: 0.520000  [13050/16000]\n",
            "cost: 0.693990, accuracy: 0.500000  [13550/16000]\n",
            "cost: 0.704805, accuracy: 0.440000  [14050/16000]\n",
            "cost: 0.691926, accuracy: 0.560000  [14550/16000]\n",
            "cost: 0.697179, accuracy: 0.500000  [15050/16000]\n",
            "cost: 0.685861, accuracy: 0.560000  [15550/16000]\n",
            "cost: 0.698061, accuracy: 0.480000  [   50/16000]\n",
            "cost: 0.673937, accuracy: 0.680000  [  550/16000]\n",
            "cost: 0.684314, accuracy: 0.620000  [ 1050/16000]\n",
            "cost: 0.694659, accuracy: 0.500000  [ 1550/16000]\n",
            "cost: 0.691405, accuracy: 0.580000  [ 2050/16000]\n",
            "cost: 0.703215, accuracy: 0.380000  [ 2550/16000]\n",
            "cost: 0.677997, accuracy: 0.620000  [ 3050/16000]\n",
            "cost: 0.709058, accuracy: 0.400000  [ 3550/16000]\n",
            "cost: 0.704293, accuracy: 0.500000  [ 4050/16000]\n",
            "cost: 0.698813, accuracy: 0.580000  [ 4550/16000]\n",
            "cost: 0.708702, accuracy: 0.400000  [ 5050/16000]\n",
            "cost: 0.695628, accuracy: 0.480000  [ 5550/16000]\n",
            "cost: 0.707047, accuracy: 0.400000  [ 6050/16000]\n",
            "cost: 0.687569, accuracy: 0.580000  [ 6550/16000]\n",
            "cost: 0.686088, accuracy: 0.600000  [ 7050/16000]\n",
            "cost: 0.705066, accuracy: 0.380000  [ 7550/16000]\n",
            "cost: 0.693063, accuracy: 0.520000  [ 8050/16000]\n",
            "cost: 0.690161, accuracy: 0.480000  [ 8550/16000]\n",
            "cost: 0.699548, accuracy: 0.420000  [ 9050/16000]\n",
            "cost: 0.707745, accuracy: 0.360000  [ 9550/16000]\n",
            "cost: 0.682636, accuracy: 0.540000  [10050/16000]\n",
            "cost: 0.690990, accuracy: 0.520000  [10550/16000]\n",
            "cost: 0.682890, accuracy: 0.560000  [11050/16000]\n",
            "cost: 0.697715, accuracy: 0.520000  [11550/16000]\n",
            "cost: 0.690021, accuracy: 0.560000  [12050/16000]\n",
            "cost: 0.681414, accuracy: 0.640000  [12550/16000]\n",
            "cost: 0.703453, accuracy: 0.420000  [13050/16000]\n",
            "cost: 0.698988, accuracy: 0.380000  [13550/16000]\n",
            "cost: 0.704526, accuracy: 0.440000  [14050/16000]\n",
            "cost: 0.691659, accuracy: 0.500000  [14550/16000]\n",
            "cost: 0.689685, accuracy: 0.540000  [15050/16000]\n",
            "cost: 0.695620, accuracy: 0.500000  [15550/16000]\n",
            "cost: 0.687670, accuracy: 0.600000  [   50/16000]\n",
            "cost: 0.687968, accuracy: 0.560000  [  550/16000]\n",
            "cost: 0.685261, accuracy: 0.580000  [ 1050/16000]\n",
            "cost: 0.705468, accuracy: 0.400000  [ 1550/16000]\n",
            "cost: 0.693150, accuracy: 0.500000  [ 2050/16000]\n",
            "cost: 0.696640, accuracy: 0.500000  [ 2550/16000]\n",
            "cost: 0.697245, accuracy: 0.500000  [ 3050/16000]\n",
            "cost: 0.682242, accuracy: 0.620000  [ 3550/16000]\n",
            "cost: 0.689893, accuracy: 0.500000  [ 4050/16000]\n",
            "cost: 0.706846, accuracy: 0.380000  [ 4550/16000]\n",
            "cost: 0.672903, accuracy: 0.700000  [ 5050/16000]\n",
            "cost: 0.686017, accuracy: 0.560000  [ 5550/16000]\n",
            "cost: 0.685137, accuracy: 0.580000  [ 6050/16000]\n",
            "cost: 0.677283, accuracy: 0.620000  [ 6550/16000]\n",
            "cost: 0.692555, accuracy: 0.460000  [ 7050/16000]\n",
            "cost: 0.711315, accuracy: 0.380000  [ 7550/16000]\n",
            "cost: 0.684997, accuracy: 0.580000  [ 8050/16000]\n",
            "cost: 0.698515, accuracy: 0.480000  [ 8550/16000]\n",
            "cost: 0.691741, accuracy: 0.520000  [ 9050/16000]\n",
            "cost: 0.691774, accuracy: 0.500000  [ 9550/16000]\n",
            "cost: 0.683049, accuracy: 0.580000  [10050/16000]\n",
            "cost: 0.690820, accuracy: 0.580000  [10550/16000]\n",
            "cost: 0.692959, accuracy: 0.540000  [11050/16000]\n",
            "cost: 0.684362, accuracy: 0.540000  [11550/16000]\n",
            "cost: 0.687074, accuracy: 0.600000  [12050/16000]\n",
            "cost: 0.700644, accuracy: 0.420000  [12550/16000]\n",
            "cost: 0.701194, accuracy: 0.440000  [13050/16000]\n",
            "cost: 0.691974, accuracy: 0.540000  [13550/16000]\n",
            "cost: 0.695251, accuracy: 0.520000  [14050/16000]\n",
            "cost: 0.699134, accuracy: 0.440000  [14550/16000]\n",
            "cost: 0.684648, accuracy: 0.620000  [15050/16000]\n",
            "cost: 0.709106, accuracy: 0.380000  [15550/16000]\n",
            "cost: 0.686231, accuracy: 0.580000  [   50/16000]\n",
            "cost: 0.692187, accuracy: 0.500000  [  550/16000]\n",
            "cost: 0.693380, accuracy: 0.480000  [ 1050/16000]\n",
            "cost: 0.707389, accuracy: 0.400000  [ 1550/16000]\n",
            "cost: 0.684461, accuracy: 0.580000  [ 2050/16000]\n",
            "cost: 0.691970, accuracy: 0.520000  [ 2550/16000]\n",
            "cost: 0.698448, accuracy: 0.460000  [ 3050/16000]\n",
            "cost: 0.692741, accuracy: 0.520000  [ 3550/16000]\n",
            "cost: 0.694533, accuracy: 0.500000  [ 4050/16000]\n",
            "cost: 0.684373, accuracy: 0.580000  [ 4550/16000]\n",
            "cost: 0.696329, accuracy: 0.500000  [ 5050/16000]\n",
            "cost: 0.713976, accuracy: 0.360000  [ 5550/16000]\n",
            "cost: 0.703011, accuracy: 0.440000  [ 6050/16000]\n",
            "cost: 0.690683, accuracy: 0.520000  [ 6550/16000]\n",
            "cost: 0.690266, accuracy: 0.500000  [ 7050/16000]\n",
            "cost: 0.688029, accuracy: 0.540000  [ 7550/16000]\n",
            "cost: 0.695580, accuracy: 0.500000  [ 8050/16000]\n",
            "cost: 0.704199, accuracy: 0.400000  [ 8550/16000]\n",
            "cost: 0.682046, accuracy: 0.620000  [ 9050/16000]\n",
            "cost: 0.691458, accuracy: 0.500000  [ 9550/16000]\n",
            "cost: 0.698226, accuracy: 0.480000  [10050/16000]\n",
            "cost: 0.700761, accuracy: 0.420000  [10550/16000]\n",
            "cost: 0.691872, accuracy: 0.480000  [11050/16000]\n",
            "cost: 0.683336, accuracy: 0.580000  [11550/16000]\n",
            "cost: 0.695828, accuracy: 0.500000  [12050/16000]\n",
            "cost: 0.697855, accuracy: 0.540000  [12550/16000]\n",
            "cost: 0.689774, accuracy: 0.500000  [13050/16000]\n",
            "cost: 0.703456, accuracy: 0.420000  [13550/16000]\n",
            "cost: 0.689255, accuracy: 0.500000  [14050/16000]\n",
            "cost: 0.699468, accuracy: 0.440000  [14550/16000]\n",
            "cost: 0.693967, accuracy: 0.540000  [15050/16000]\n",
            "cost: 0.705236, accuracy: 0.400000  [15550/16000]\n",
            "cost: 0.681437, accuracy: 0.640000  [   50/16000]\n",
            "cost: 0.708257, accuracy: 0.440000  [  550/16000]\n",
            "cost: 0.685510, accuracy: 0.600000  [ 1050/16000]\n",
            "cost: 0.691577, accuracy: 0.520000  [ 1550/16000]\n",
            "cost: 0.694157, accuracy: 0.560000  [ 2050/16000]\n",
            "cost: 0.682606, accuracy: 0.600000  [ 2550/16000]\n",
            "cost: 0.707722, accuracy: 0.380000  [ 3050/16000]\n",
            "cost: 0.692220, accuracy: 0.540000  [ 3550/16000]\n",
            "cost: 0.705299, accuracy: 0.460000  [ 4050/16000]\n",
            "cost: 0.686251, accuracy: 0.640000  [ 4550/16000]\n",
            "cost: 0.700557, accuracy: 0.500000  [ 5050/16000]\n",
            "cost: 0.688082, accuracy: 0.540000  [ 5550/16000]\n",
            "cost: 0.700319, accuracy: 0.460000  [ 6050/16000]\n",
            "cost: 0.705905, accuracy: 0.380000  [ 6550/16000]\n",
            "cost: 0.699778, accuracy: 0.480000  [ 7050/16000]\n",
            "cost: 0.690517, accuracy: 0.580000  [ 7550/16000]\n",
            "cost: 0.688373, accuracy: 0.540000  [ 8050/16000]\n",
            "cost: 0.690338, accuracy: 0.560000  [ 8550/16000]\n",
            "cost: 0.686755, accuracy: 0.560000  [ 9050/16000]\n",
            "cost: 0.680816, accuracy: 0.600000  [ 9550/16000]\n",
            "cost: 0.689105, accuracy: 0.500000  [10050/16000]\n",
            "cost: 0.683188, accuracy: 0.580000  [10550/16000]\n",
            "cost: 0.682699, accuracy: 0.600000  [11050/16000]\n",
            "cost: 0.700112, accuracy: 0.480000  [11550/16000]\n",
            "cost: 0.697409, accuracy: 0.500000  [12050/16000]\n",
            "cost: 0.700126, accuracy: 0.440000  [12550/16000]\n",
            "cost: 0.687927, accuracy: 0.560000  [13050/16000]\n",
            "cost: 0.688462, accuracy: 0.540000  [13550/16000]\n",
            "cost: 0.678352, accuracy: 0.620000  [14050/16000]\n",
            "cost: 0.688934, accuracy: 0.560000  [14550/16000]\n",
            "cost: 0.702464, accuracy: 0.420000  [15050/16000]\n",
            "cost: 0.685259, accuracy: 0.560000  [15550/16000]\n",
            "cost: 0.688942, accuracy: 0.560000  [   50/16000]\n",
            "cost: 0.687241, accuracy: 0.560000  [  550/16000]\n",
            "cost: 0.704133, accuracy: 0.400000  [ 1050/16000]\n",
            "cost: 0.693943, accuracy: 0.520000  [ 1550/16000]\n",
            "cost: 0.704074, accuracy: 0.400000  [ 2050/16000]\n",
            "cost: 0.688623, accuracy: 0.580000  [ 2550/16000]\n",
            "cost: 0.687155, accuracy: 0.540000  [ 3050/16000]\n",
            "cost: 0.706866, accuracy: 0.340000  [ 3550/16000]\n",
            "cost: 0.702220, accuracy: 0.440000  [ 4050/16000]\n",
            "cost: 0.692884, accuracy: 0.560000  [ 4550/16000]\n",
            "cost: 0.680584, accuracy: 0.620000  [ 5050/16000]\n",
            "cost: 0.691825, accuracy: 0.460000  [ 5550/16000]\n",
            "cost: 0.693502, accuracy: 0.500000  [ 6050/16000]\n",
            "cost: 0.698954, accuracy: 0.420000  [ 6550/16000]\n",
            "cost: 0.693346, accuracy: 0.520000  [ 7050/16000]\n",
            "cost: 0.701785, accuracy: 0.420000  [ 7550/16000]\n",
            "cost: 0.675234, accuracy: 0.600000  [ 8050/16000]\n",
            "cost: 0.684521, accuracy: 0.540000  [ 8550/16000]\n",
            "cost: 0.707016, accuracy: 0.400000  [ 9050/16000]\n",
            "cost: 0.692490, accuracy: 0.520000  [ 9550/16000]\n",
            "cost: 0.694941, accuracy: 0.500000  [10050/16000]\n",
            "cost: 0.692326, accuracy: 0.480000  [10550/16000]\n",
            "cost: 0.690152, accuracy: 0.560000  [11050/16000]\n",
            "cost: 0.705620, accuracy: 0.440000  [11550/16000]\n",
            "cost: 0.699205, accuracy: 0.540000  [12050/16000]\n",
            "cost: 0.695904, accuracy: 0.520000  [12550/16000]\n",
            "cost: 0.692844, accuracy: 0.540000  [13050/16000]\n",
            "cost: 0.693375, accuracy: 0.540000  [13550/16000]\n",
            "cost: 0.686767, accuracy: 0.540000  [14050/16000]\n",
            "cost: 0.697618, accuracy: 0.440000  [14550/16000]\n",
            "cost: 0.686247, accuracy: 0.580000  [15050/16000]\n",
            "cost: 0.706569, accuracy: 0.360000  [15550/16000]\n",
            "cost: 0.697958, accuracy: 0.460000  [   50/16000]\n",
            "cost: 0.690117, accuracy: 0.540000  [  550/16000]\n",
            "cost: 0.680172, accuracy: 0.600000  [ 1050/16000]\n",
            "cost: 0.683225, accuracy: 0.640000  [ 1550/16000]\n",
            "cost: 0.693242, accuracy: 0.460000  [ 2050/16000]\n",
            "cost: 0.677600, accuracy: 0.620000  [ 2550/16000]\n",
            "cost: 0.692190, accuracy: 0.520000  [ 3050/16000]\n",
            "cost: 0.691830, accuracy: 0.500000  [ 3550/16000]\n",
            "cost: 0.694963, accuracy: 0.440000  [ 4050/16000]\n",
            "cost: 0.690437, accuracy: 0.560000  [ 4550/16000]\n",
            "cost: 0.693182, accuracy: 0.460000  [ 5050/16000]\n",
            "cost: 0.681705, accuracy: 0.640000  [ 5550/16000]\n",
            "cost: 0.706231, accuracy: 0.400000  [ 6050/16000]\n",
            "cost: 0.703637, accuracy: 0.440000  [ 6550/16000]\n",
            "cost: 0.684173, accuracy: 0.540000  [ 7050/16000]\n",
            "cost: 0.692932, accuracy: 0.500000  [ 7550/16000]\n",
            "cost: 0.703319, accuracy: 0.400000  [ 8050/16000]\n",
            "cost: 0.694082, accuracy: 0.520000  [ 8550/16000]\n",
            "cost: 0.699694, accuracy: 0.500000  [ 9050/16000]\n",
            "cost: 0.698466, accuracy: 0.440000  [ 9550/16000]\n",
            "cost: 0.693039, accuracy: 0.500000  [10050/16000]\n",
            "cost: 0.689752, accuracy: 0.540000  [10550/16000]\n",
            "cost: 0.685508, accuracy: 0.600000  [11050/16000]\n",
            "cost: 0.678814, accuracy: 0.640000  [11550/16000]\n",
            "cost: 0.694767, accuracy: 0.540000  [12050/16000]\n",
            "cost: 0.696689, accuracy: 0.460000  [12550/16000]\n",
            "cost: 0.689516, accuracy: 0.600000  [13050/16000]\n",
            "cost: 0.674316, accuracy: 0.640000  [13550/16000]\n",
            "cost: 0.694394, accuracy: 0.520000  [14050/16000]\n",
            "cost: 0.685539, accuracy: 0.600000  [14550/16000]\n",
            "cost: 0.697237, accuracy: 0.460000  [15050/16000]\n",
            "cost: 0.706322, accuracy: 0.400000  [15550/16000]\n",
            "cost: 0.676759, accuracy: 0.620000  [   50/16000]\n",
            "cost: 0.695026, accuracy: 0.560000  [  550/16000]\n",
            "cost: 0.704091, accuracy: 0.420000  [ 1050/16000]\n",
            "cost: 0.691271, accuracy: 0.540000  [ 1550/16000]\n",
            "cost: 0.694517, accuracy: 0.500000  [ 2050/16000]\n",
            "cost: 0.699666, accuracy: 0.440000  [ 2550/16000]\n",
            "cost: 0.692941, accuracy: 0.520000  [ 3050/16000]\n",
            "cost: 0.702373, accuracy: 0.460000  [ 3550/16000]\n",
            "cost: 0.687414, accuracy: 0.560000  [ 4050/16000]\n",
            "cost: 0.691621, accuracy: 0.560000  [ 4550/16000]\n",
            "cost: 0.707536, accuracy: 0.400000  [ 5050/16000]\n",
            "cost: 0.694975, accuracy: 0.540000  [ 5550/16000]\n",
            "cost: 0.703697, accuracy: 0.460000  [ 6050/16000]\n",
            "cost: 0.686686, accuracy: 0.600000  [ 6550/16000]\n",
            "cost: 0.686642, accuracy: 0.580000  [ 7050/16000]\n",
            "cost: 0.688797, accuracy: 0.560000  [ 7550/16000]\n",
            "cost: 0.707002, accuracy: 0.380000  [ 8050/16000]\n",
            "cost: 0.692652, accuracy: 0.540000  [ 8550/16000]\n",
            "cost: 0.674459, accuracy: 0.660000  [ 9050/16000]\n",
            "cost: 0.684341, accuracy: 0.600000  [ 9550/16000]\n",
            "cost: 0.687037, accuracy: 0.540000  [10050/16000]\n",
            "cost: 0.680252, accuracy: 0.660000  [10550/16000]\n",
            "cost: 0.700656, accuracy: 0.480000  [11050/16000]\n",
            "cost: 0.688168, accuracy: 0.560000  [11550/16000]\n",
            "cost: 0.711949, accuracy: 0.340000  [12050/16000]\n",
            "cost: 0.681946, accuracy: 0.620000  [12550/16000]\n",
            "cost: 0.688433, accuracy: 0.560000  [13050/16000]\n",
            "cost: 0.678504, accuracy: 0.640000  [13550/16000]\n",
            "cost: 0.699695, accuracy: 0.480000  [14050/16000]\n",
            "cost: 0.687408, accuracy: 0.540000  [14550/16000]\n",
            "cost: 0.705475, accuracy: 0.420000  [15050/16000]\n",
            "cost: 0.697213, accuracy: 0.440000  [15550/16000]\n",
            "cost: 0.699327, accuracy: 0.460000  [   50/16000]\n",
            "cost: 0.695797, accuracy: 0.480000  [  550/16000]\n",
            "cost: 0.690203, accuracy: 0.580000  [ 1050/16000]\n",
            "cost: 0.694702, accuracy: 0.520000  [ 1550/16000]\n",
            "cost: 0.691941, accuracy: 0.460000  [ 2050/16000]\n",
            "cost: 0.692828, accuracy: 0.580000  [ 2550/16000]\n",
            "cost: 0.691357, accuracy: 0.560000  [ 3050/16000]\n",
            "cost: 0.690598, accuracy: 0.560000  [ 3550/16000]\n",
            "cost: 0.688380, accuracy: 0.580000  [ 4050/16000]\n",
            "cost: 0.689752, accuracy: 0.520000  [ 4550/16000]\n",
            "cost: 0.699439, accuracy: 0.420000  [ 5050/16000]\n",
            "cost: 0.688462, accuracy: 0.520000  [ 5550/16000]\n",
            "cost: 0.695877, accuracy: 0.520000  [ 6050/16000]\n",
            "cost: 0.690693, accuracy: 0.580000  [ 6550/16000]\n",
            "cost: 0.700264, accuracy: 0.520000  [ 7050/16000]\n",
            "cost: 0.706782, accuracy: 0.420000  [ 7550/16000]\n",
            "cost: 0.701302, accuracy: 0.480000  [ 8050/16000]\n",
            "cost: 0.685470, accuracy: 0.560000  [ 8550/16000]\n",
            "cost: 0.676511, accuracy: 0.660000  [ 9050/16000]\n",
            "cost: 0.698560, accuracy: 0.480000  [ 9550/16000]\n",
            "cost: 0.711931, accuracy: 0.380000  [10050/16000]\n",
            "cost: 0.707496, accuracy: 0.400000  [10550/16000]\n",
            "cost: 0.695053, accuracy: 0.460000  [11050/16000]\n",
            "cost: 0.694443, accuracy: 0.520000  [11550/16000]\n",
            "cost: 0.694824, accuracy: 0.540000  [12050/16000]\n",
            "cost: 0.709308, accuracy: 0.360000  [12550/16000]\n",
            "cost: 0.686091, accuracy: 0.600000  [13050/16000]\n",
            "cost: 0.700505, accuracy: 0.420000  [13550/16000]\n",
            "cost: 0.700729, accuracy: 0.420000  [14050/16000]\n",
            "cost: 0.698817, accuracy: 0.500000  [14550/16000]\n",
            "cost: 0.694630, accuracy: 0.500000  [15050/16000]\n",
            "cost: 0.699442, accuracy: 0.460000  [15550/16000]\n",
            "cost: 0.698153, accuracy: 0.460000  [   50/16000]\n",
            "cost: 0.690654, accuracy: 0.500000  [  550/16000]\n",
            "cost: 0.695040, accuracy: 0.480000  [ 1050/16000]\n",
            "cost: 0.695835, accuracy: 0.520000  [ 1550/16000]\n",
            "cost: 0.686432, accuracy: 0.540000  [ 2050/16000]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense\n",
        "#from keras.layers import Flatten\n",
        "#from keras.layers import LSTM\n",
        "#from keras.layers import InputLayer\n",
        "from keras.preprocessing import sequence\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn import model_selection\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device=torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "\n",
        "data=pd.read_csv(\"xaa\",encoding=\"utf-8\")\n",
        "\n",
        "vect  = CountVectorizer()\n",
        "#corpus = vect.fit_transform(data[\"text\"])\n",
        "#train_corpus, test_corpus, train_label, test_label = model_selection.train_test_split(data[\"text\"],data[\"label\"],test_size=0.4)\n",
        "#Encoder = LabelEncoder()\n",
        "#train_label = Encoder.fit_transform(train_label)\n",
        "#test_label = Encoder.fit_transform(test_label)\n",
        "#train_corpus_vect=vect.transform(train_corpus)\n",
        "#test_corpus_vect=vect.transform(test_corpus)\n",
        "\n",
        "# Initalise vect.vocabulary_\n",
        "vect.fit_transform(data[\"text\"])\n",
        "\n",
        "maxlen=0\n",
        "\n",
        "def transform(text,vect):\n",
        "    global maxlen\n",
        "    d=vect.vocabulary_\n",
        "    p=vect.build_preprocessor()\n",
        "    t=vect.build_tokenizer()\n",
        "    vec_list=[]\n",
        "    for doc in text:\n",
        "        tokens=t(p(doc))\n",
        "        s=len(tokens)\n",
        "        if s>maxlen:\n",
        "            maxlen=s\n",
        "        doc_vec=np.array([d[token] for token in tokens])\n",
        "        #doc_vec=sequence.pad_sequences(doc_vec,maxlen=maxlen)\n",
        "        vec_list.append(doc_vec)\n",
        "    vec_list=sequence.pad_sequences(vec_list,maxlen=maxlen,padding=\"post\")\n",
        "    corpus_vec=np.vstack(vec_list)\n",
        "    #return nn.functional.normalize(torch.tensor(corpus_vec).float())\n",
        "    return torch.tensor(corpus_vec)\n",
        "    #return torch.tensor(corpus_vec).float()\n",
        "\n",
        "# print(corpus_vec)\n",
        "\n",
        "bsize=50\n",
        "epochs=50\n",
        "lr=1e-3\n",
        "\n",
        "class corpus(Dataset):\n",
        "    def __init__(self,corpus,label,seq):\n",
        "        self.corpus=corpus\n",
        "        self.label=label\n",
        "        self.seq=seq\n",
        "    def __len__(self):\n",
        "        return len(self.corpus)\n",
        "    def __getitem__(self,idx):\n",
        "        return self.corpus[idx],self.label[idx]\n",
        "\n",
        "class lstm(nn.Module):\n",
        "    def __init__(self,input_size,hidden_size,seq):\n",
        "        super(lstm,self).__init__()\n",
        "        self.input_size=input_size\n",
        "        self.hidden_size=hidden_size\n",
        "        self.seq=seq\n",
        "        self.rnn=True\n",
        "        self.lstm=nn.LSTM(input_size,hidden_size,batch_first=True,num_layers=1)\n",
        "        self.fc=nn.Linear(self.hidden_size,1)\n",
        "        self.embed=nn.Embedding(len(vect.vocabulary_),input_size)\n",
        "\n",
        "    def forward(self,x,h0=None,c0=None):\n",
        "        x=self.embed(x)\n",
        "        if h0==None and c0==None:\n",
        "            x, (hn,cn) = self.lstm(x)\n",
        "        else:\n",
        "            x, (hn,cn) = self.lstm(x,(h0,c0))\n",
        "        #print(x[:,-1,:].shape)\n",
        "        #assert False,\"blah\"\n",
        "        x = torch.flatten(x[:,-1,:],1)\n",
        "        x = self.fc(x)\n",
        "        return nn.Sigmoid()(x), hn,cn\n",
        "        #return nn.Softmax()(x), hn,cn\n",
        "\n",
        "class dense(nn.Module):\n",
        "    def __init__(self,seq,vocab,embed_dim):\n",
        "        super(dense,self).__init__()\n",
        "        self.seq=seq\n",
        "        self.rnn=False\n",
        "        self.network=nn.Sequential(\n",
        "#            nn.Embedding(vocab,embed_dim),\n",
        "            nn.Linear(embed_dim*seq,360),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(360,180),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(180,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.embed=nn.Embedding(vocab,embed_dim)\n",
        "\n",
        "    def forward(self,x):\n",
        "        #print(x.shape)\n",
        "        x=self.embed(x)\n",
        "        x=torch.flatten(x,1)\n",
        "        #x=torch.transpose(x,1,2)\n",
        "        #x=torch.flatten(x,1)\n",
        "        return self.network(x)\n",
        "\n",
        "vocab=len(vect.vocabulary_)\n",
        "Encoder = LabelEncoder()\n",
        "corpus_vec = transform(data[\"text\"],vect)\n",
        "train_corpus_vec, test_corpus_vec, train_label, test_label = model_selection.train_test_split(corpus_vec,data[\"label\"],test_size=0.2)\n",
        "train_label = torch.from_numpy(Encoder.fit_transform(train_label)).float()\n",
        "test_label = torch.from_numpy(Encoder.fit_transform(test_label)).float()\n",
        "#train_corpus_vec = transform(train_corpus,vect)\n",
        "#test_corpus_vec = transform(test_corpus,vect)\n",
        "lstm_classifier=lstm(16,20,maxlen)\n",
        "loss_fn=nn.BCELoss()\n",
        "#loss_fn=nn.BCEWithLogitsLoss()\n",
        "#loss_fn=nn.MSELoss()\n",
        "optimizer=torch.optim.Adam(lstm_classifier.parameters(),lr=lr)\n",
        "#optimizer=torch.optim.SGD(lstm_classifier.parameters(),lr=lr)\n",
        "#print(len(lstm_classifier(torch.reshape(train_corpus_vec[0],(bsize,maxlen,1)))))\n",
        "\n",
        "dense_classifier=dense(maxlen,vocab,16)\n",
        "\n",
        "c=dense_classifier\n",
        "\n",
        "train_dataloader=DataLoader(corpus(train_corpus_vec,train_label,maxlen),batch_size=bsize,shuffle=True)\n",
        "test_dataloader=DataLoader(corpus(test_corpus_vec,test_label,maxlen),batch_size=bsize,shuffle=True)\n",
        "\n",
        "def train(dataloader,model,loss_fn,optimizer):\n",
        "    hn,cn=None,None\n",
        "    size=len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch,(x,y) in enumerate(dataloader):\n",
        "        x,y=x.to(device),y.to(device)\n",
        "        #print(torch.sum(x[0]!=0))\n",
        "        if model.rnn==True:\n",
        "            #pred,hn,cn=model(x.reshape(bsize,dataloader.dataset.seq,1).to(device),hn,cn)\n",
        "            pred,hn,cn=model(x,hn,cn)\n",
        "        else:\n",
        "            pred=model(x)\n",
        "        #pred=model(x)\n",
        "        cost=loss_fn(pred.flatten(),y)\n",
        "        cost.backward()\n",
        "        if model.rnn==True:\n",
        "            print(model.lstm.weight_ih_l0.grad[0][0])\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if model.rnn==True:\n",
        "            hn=hn.detach()\n",
        "            cn=cn.detach()\n",
        "        if batch % 10 == 0:\n",
        "            cost_val, current = cost.item(), batch * bsize + len(x)\n",
        "            print(f\"cost: {cost_val:>7f}, accuracy: {(torch.round(pred.flatten())==y).sum().item()/bsize:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x,y=x.to(device),y.to(device)\n",
        "            if model.rnn==True:\n",
        "                pred,_,_=model(x.reshape(bsize,dataloader.dataset.seq,1).to(device))\n",
        "            else:\n",
        "                pred=model(x)\n",
        "            #pred=model(x)\n",
        "            #print(torch.round(pred.flatten()),y)\n",
        "            test_loss += loss_fn(pred.flatten(), y).item()\n",
        "            ncorrect = (torch.round(pred.flatten()) == y).sum().item()\n",
        "            print(ncorrect)\n",
        "            correct += ncorrect\n",
        "\n",
        "    print(correct,size)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct/size):>0.1f}%, Avg loss: {100*test_loss/size:>8f} \\n\")\n",
        "\n",
        "#print(corpus_vec[0])\n",
        "for epoch in range(epochs):\n",
        "    train(train_dataloader,c,loss_fn,optimizer)\n",
        "test_loop(test_dataloader,c,loss_fn)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN56FTOs07ZMeHZ2gpyA9vL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}