{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nwon24/nlp/blob/main/W5/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWAcH6mWYxkE",
        "outputId": "58c263dc-64b3-42bd-c7cc-79f109e6b791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost: 0.687257  [   50/12000]\n",
            "cost: 0.762855  [  550/12000]\n",
            "cost: 0.677522  [ 1050/12000]\n",
            "cost: 0.694661  [ 1550/12000]\n",
            "cost: 0.678861  [ 2050/12000]\n",
            "cost: 0.685298  [ 2550/12000]\n",
            "cost: 0.644602  [ 3050/12000]\n",
            "cost: 0.875158  [ 3550/12000]\n",
            "cost: 0.693387  [ 4050/12000]\n",
            "cost: 0.749722  [ 4550/12000]\n",
            "cost: 0.722870  [ 5050/12000]\n",
            "cost: 0.702108  [ 5550/12000]\n",
            "cost: 0.763407  [ 6050/12000]\n",
            "cost: 0.765638  [ 6550/12000]\n",
            "cost: 0.703042  [ 7050/12000]\n",
            "cost: 0.682852  [ 7550/12000]\n",
            "cost: 0.688694  [ 8050/12000]\n",
            "cost: 0.776498  [ 8550/12000]\n",
            "cost: 0.686863  [ 9050/12000]\n",
            "cost: 0.694928  [ 9550/12000]\n",
            "cost: 0.715820  [10050/12000]\n",
            "cost: 0.703266  [10550/12000]\n",
            "cost: 0.684015  [11050/12000]\n",
            "cost: 0.699546  [11550/12000]\n",
            "cost: 0.722529  [   50/12000]\n",
            "cost: 0.675984  [  550/12000]\n",
            "cost: 0.733863  [ 1050/12000]\n",
            "cost: 0.679385  [ 1550/12000]\n",
            "cost: 0.707996  [ 2050/12000]\n",
            "cost: 0.713310  [ 2550/12000]\n",
            "cost: 0.683178  [ 3050/12000]\n",
            "cost: 0.683671  [ 3550/12000]\n",
            "cost: 0.667226  [ 4050/12000]\n",
            "cost: 0.687310  [ 4550/12000]\n",
            "cost: 0.708482  [ 5050/12000]\n",
            "cost: 0.679861  [ 5550/12000]\n",
            "cost: 0.718778  [ 6050/12000]\n",
            "cost: 0.650671  [ 6550/12000]\n",
            "cost: 0.789528  [ 7050/12000]\n",
            "cost: 0.635287  [ 7550/12000]\n",
            "cost: 0.710201  [ 8050/12000]\n",
            "cost: 0.666852  [ 8550/12000]\n",
            "cost: 0.675208  [ 9050/12000]\n",
            "cost: 0.666843  [ 9550/12000]\n",
            "cost: 0.663363  [10050/12000]\n",
            "cost: 0.668423  [10550/12000]\n",
            "cost: 0.681476  [11050/12000]\n",
            "cost: 0.659664  [11550/12000]\n",
            "cost: 0.720929  [   50/12000]\n",
            "cost: 0.670006  [  550/12000]\n",
            "cost: 0.655598  [ 1050/12000]\n",
            "cost: 0.667417  [ 1550/12000]\n",
            "cost: 0.697404  [ 2050/12000]\n",
            "cost: 0.640529  [ 2550/12000]\n",
            "cost: 0.769978  [ 3050/12000]\n",
            "cost: 0.677187  [ 3550/12000]\n",
            "cost: 0.712083  [ 4050/12000]\n",
            "cost: 0.770116  [ 4550/12000]\n",
            "cost: 0.812134  [ 5050/12000]\n",
            "cost: 0.834934  [ 5550/12000]\n",
            "cost: 1.015397  [ 6050/12000]\n",
            "cost: 0.781288  [ 6550/12000]\n",
            "cost: 0.707566  [ 7050/12000]\n",
            "cost: 0.723480  [ 7550/12000]\n",
            "cost: 0.650076  [ 8050/12000]\n",
            "cost: 0.702427  [ 8550/12000]\n",
            "cost: 0.674760  [ 9050/12000]\n",
            "cost: 0.725480  [ 9550/12000]\n",
            "cost: 0.663815  [10050/12000]\n",
            "cost: 0.731479  [10550/12000]\n",
            "cost: 0.727057  [11050/12000]\n",
            "cost: 0.753056  [11550/12000]\n",
            "31\n",
            "28\n",
            "30\n",
            "26\n",
            "25\n",
            "28\n",
            "27\n",
            "21\n",
            "27\n",
            "26\n",
            "34\n",
            "29\n",
            "22\n",
            "30\n",
            "30\n",
            "27\n",
            "24\n",
            "26\n",
            "25\n",
            "33\n",
            "32\n",
            "23\n",
            "25\n",
            "23\n",
            "28\n",
            "29\n",
            "32\n",
            "19\n",
            "26\n",
            "26\n",
            "27\n",
            "32\n",
            "34\n",
            "30\n",
            "24\n",
            "22\n",
            "29\n",
            "28\n",
            "27\n",
            "22\n",
            "28\n",
            "25\n",
            "31\n",
            "26\n",
            "23\n",
            "23\n",
            "25\n",
            "29\n",
            "26\n",
            "29\n",
            "34\n",
            "25\n",
            "24\n",
            "27\n",
            "23\n",
            "22\n",
            "27\n",
            "24\n",
            "26\n",
            "31\n",
            "27\n",
            "26\n",
            "26\n",
            "33\n",
            "33\n",
            "30\n",
            "28\n",
            "28\n",
            "22\n",
            "26\n",
            "15\n",
            "26\n",
            "33\n",
            "24\n",
            "25\n",
            "25\n",
            "25\n",
            "25\n",
            "25\n",
            "24\n",
            "28\n",
            "30\n",
            "30\n",
            "32\n",
            "29\n",
            "32\n",
            "23\n",
            "29\n",
            "25\n",
            "25\n",
            "25\n",
            "28\n",
            "25\n",
            "26\n",
            "22\n",
            "23\n",
            "26\n",
            "30\n",
            "25\n",
            "25\n",
            "21\n",
            "17\n",
            "30\n",
            "31\n",
            "26\n",
            "25\n",
            "26\n",
            "25\n",
            "17\n",
            "26\n",
            "28\n",
            "24\n",
            "33\n",
            "24\n",
            "25\n",
            "26\n",
            "33\n",
            "31\n",
            "25\n",
            "30\n",
            "32\n",
            "27\n",
            "30\n",
            "21\n",
            "28\n",
            "24\n",
            "25\n",
            "29\n",
            "28\n",
            "26\n",
            "31\n",
            "30\n",
            "22\n",
            "29\n",
            "25\n",
            "21\n",
            "24\n",
            "22\n",
            "29\n",
            "31\n",
            "31\n",
            "25\n",
            "26\n",
            "27\n",
            "25\n",
            "26\n",
            "23\n",
            "27\n",
            "20\n",
            "25\n",
            "30\n",
            "28\n",
            "32\n",
            "29\n",
            "22\n",
            "24\n",
            "33\n",
            "26\n",
            "25\n",
            "27\n",
            "4267 8000\n",
            "Test Error: \n",
            " Accuracy: 53.3%, Avg loss: 0.014122 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense\n",
        "#from keras.layers import Flatten\n",
        "#from keras.layers import LSTM\n",
        "#from keras.layers import InputLayer\n",
        "from keras.preprocessing import sequence\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn import model_selection\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device=torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "\n",
        "data=pd.read_csv(\"xaa\",encoding=\"utf-8\")\n",
        "\n",
        "vect  = CountVectorizer()\n",
        "#corpus = vect.fit_transform(data[\"text\"])\n",
        "#train_corpus, test_corpus, train_label, test_label = model_selection.train_test_split(data[\"text\"],data[\"label\"],test_size=0.4)\n",
        "#Encoder = LabelEncoder()\n",
        "#train_label = Encoder.fit_transform(train_label)\n",
        "#test_label = Encoder.fit_transform(test_label)\n",
        "#train_corpus_vect=vect.transform(train_corpus)\n",
        "#test_corpus_vect=vect.transform(test_corpus)\n",
        "\n",
        "# Initalise vect.vocabulary_\n",
        "vect.fit_transform(data[\"text\"])\n",
        "\n",
        "maxlen=0\n",
        "\n",
        "def transform(text,vect):\n",
        "    global maxlen\n",
        "    d=vect.vocabulary_\n",
        "    p=vect.build_preprocessor()\n",
        "    t=vect.build_tokenizer()\n",
        "    vec_list=[]\n",
        "    for doc in text:\n",
        "        tokens=t(p(doc))\n",
        "        s=len(tokens)\n",
        "        if s>maxlen:\n",
        "            maxlen=s\n",
        "        doc_vec=np.array([d[token] for token in tokens])\n",
        "        #doc_vec=sequence.pad_sequences(doc_vec,maxlen=maxlen)\n",
        "        vec_list.append(doc_vec)\n",
        "    vec_list=sequence.pad_sequences(vec_list,maxlen=maxlen)\n",
        "    corpus_vec=np.vstack(vec_list)\n",
        "    return torch.tensor(corpus_vec).float()\n",
        "\n",
        "# print(corpus_vec)\n",
        "\n",
        "bsize=50\n",
        "epochs=3\n",
        "lr=1e-3\n",
        "\n",
        "class corpus(Dataset):\n",
        "    def __init__(self,corpus,label,seq):\n",
        "        self.corpus=corpus\n",
        "        self.label=label\n",
        "        self.seq=seq\n",
        "    def __len__(self):\n",
        "        return len(self.corpus)\n",
        "    def __getitem__(self,idx):\n",
        "        return self.corpus[idx],self.label[idx]\n",
        "\n",
        "class lstm(nn.Module):\n",
        "    def __init__(self,input_size,hidden_size,seq):\n",
        "        super(lstm,self).__init__()\n",
        "        self.input_size=input_size\n",
        "        self.hidden_size=hidden_size\n",
        "        self.seq=seq\n",
        "        self.lstm=nn.LSTM(input_size,hidden_size,batch_first=True)\n",
        "        self.fc=nn.Linear(self.hidden_size*seq,1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x, (_,_) = self.lstm(x)\n",
        "        x = torch.flatten(x,1)\n",
        "        x = self.fc(x)\n",
        "        return nn.Sigmoid()(x)\n",
        "\n",
        "Encoder = LabelEncoder()\n",
        "corpus_vec = transform(data[\"text\"],vect)\n",
        "train_corpus_vec, test_corpus_vec, train_label, test_label = model_selection.train_test_split(corpus_vec,data[\"label\"],test_size=0.4)\n",
        "train_label = torch.from_numpy(Encoder.fit_transform(train_label)).float()\n",
        "test_label = torch.from_numpy(Encoder.fit_transform(test_label)).float()\n",
        "#train_corpus_vec = transform(train_corpus,vect)\n",
        "#test_corpus_vec = transform(test_corpus,vect)\n",
        "lstm_classifier=lstm(1,100,maxlen)\n",
        "loss_fn=nn.BCELoss()\n",
        "optimizer=torch.optim.Adam(lstm_classifier.parameters(),lr=lr)\n",
        "#print(len(lstm_classifier(torch.reshape(train_corpus_vec[0],(bsize,maxlen,1)))))\n",
        "\n",
        "train_dataloader=DataLoader(corpus(train_corpus_vec,train_label,maxlen),batch_size=bsize,shuffle=True)\n",
        "test_dataloader=DataLoader(corpus(test_corpus_vec,test_label,maxlen),batch_size=bsize,shuffle=True)\n",
        "\n",
        "def train(dataloader,model,loss_fn,optimizer):\n",
        "    size=len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch,(x,y) in enumerate(dataloader):\n",
        "        x,y=x.to(device),y.to(device)\n",
        "        #print(torch.sum(x[0]!=0))\n",
        "        pred=model(x.reshape(bsize,dataloader.dataset.seq,1).to(device))\n",
        "        #pred=model(x)\n",
        "        cost=loss_fn(pred.flatten(),y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % 10 == 0:\n",
        "            cost, current = cost.item(), batch * bsize + len(x)\n",
        "            print(f\"cost: {cost:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x,y=x.to(device),y.to(device)\n",
        "            pred=model(x.reshape(bsize,dataloader.dataset.seq,1).to(device))\n",
        "            #pred=model(x)\n",
        "            #print(torch.round(pred.flatten()),y)\n",
        "            test_loss += loss_fn(pred.flatten(), y).item()\n",
        "            ncorrect = (torch.round(pred.flatten()) == y).sum().item()\n",
        "            print(ncorrect)\n",
        "            correct += ncorrect\n",
        "\n",
        "    print(correct,size)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct/size):>0.1f}%, Avg loss: {test_loss/size:>8f} \\n\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(train_dataloader,lstm_classifier,loss_fn,optimizer)\n",
        "test_loop(test_dataloader,lstm_classifier,loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYNXk2MWyTCE",
        "outputId": "ff60bc2a-a91b-4438-c288-3d1c53ef14ec"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.16.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.14.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMn7PejUzgeJwgPGo3nULhs",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}