{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nwon24/nlp/blob/main/W5/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWAcH6mWYxkE",
        "outputId": "eb977c7d-4ca8-43d3-a6a6-45d3deaf840c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "191\n",
            "191\n",
            "191\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import InputLayer\n",
        "from keras.preprocessing import sequence\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn import model_selection\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "data=pd.read_csv(\"xaa\",encoding=\"utf-8\")\n",
        "\n",
        "vect  = CountVectorizer()\n",
        "#corpus = vect.fit_transform(data[\"text\"])\n",
        "#train_corpus, test_corpus, train_label, test_label = model_selection.train_test_split(data[\"text\"],data[\"label\"],test_size=0.4)\n",
        "#Encoder = LabelEncoder()\n",
        "#train_label = Encoder.fit_transform(train_label)\n",
        "#test_label = Encoder.fit_transform(test_label)\n",
        "#train_corpus_vect=vect.transform(train_corpus)\n",
        "#test_corpus_vect=vect.transform(test_corpus)\n",
        "\n",
        "#import spacy\n",
        "#nlp = spacy.load(\"en_core_web_sm\")\n",
        "#for doc in nlp.pipe(data[\"text\"],disable=[\"tagger\",\"parser\",\"ner\",\"textcat\"]):\n",
        "#    # Do something with the doc here\n",
        "#    print(doc.text)\n",
        "\n",
        "corpus = vect.fit_transform(data[\"text\"])\n",
        "\n",
        "maxlen=0\n",
        "\n",
        "def transform(text,vect):\n",
        "    global maxlen\n",
        "    d=vect.vocabulary_\n",
        "    p=vect.build_preprocessor()\n",
        "    t=vect.build_tokenizer()\n",
        "    vec_list=[]\n",
        "    for doc in text:\n",
        "        tokens=t(p(doc))\n",
        "        s=len(tokens)\n",
        "        if s>maxlen:\n",
        "            maxlen=s\n",
        "        doc_vec=np.array([d[token] for token in tokens])\n",
        "        #doc_vec=sequence.pad_sequences(doc_vec,maxlen=maxlen)\n",
        "        vec_list.append(doc_vec)\n",
        "    vec_list=sequence.pad_sequences(vec_list,maxlen=maxlen)\n",
        "    corpus_vec=np.vstack(vec_list)\n",
        "    return torch.tensor(corpus_vec).float()\n",
        "\n",
        "# print(corpus_vec)\n",
        "\n",
        "bsize=32\n",
        "epochs=8\n",
        "lr=1e-5\n",
        "\n",
        "class lstm(nn.Module):\n",
        "    def __init__(self,input_size,hidden_size):\n",
        "        super(lstm,self).__init__()\n",
        "        self.input_size=input_size\n",
        "        self.hidden_size=hidden_size\n",
        "        self.lstm=nn.LSTM(input_size,hidden_size)\n",
        "        self.fc=nn.Linear(self.hidden_size,1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out, (_,_) = self.lstm(x)\n",
        "        return nn.Sigmoid()(self.fc(out))\n",
        "\n",
        "Encoder = LabelEncoder()\n",
        "corpus_vec = transform(data[\"text\"],vect)\n",
        "train_corpus_vec, test_corpus_vec, train_label, test_label = model_selection.train_test_split(corpus_vec,data[\"label\"],test_size=0.4)\n",
        "train_label = Encoder.fit_transform(train_label)\n",
        "test_label = Encoder.fit_transform(test_label)\n",
        "#train_corpus_vec = transform(train_corpus,vect)\n",
        "#test_corpus_vec = transform(test_corpus,vect)\n",
        "print(maxlen)\n",
        "print(len(train_corpus_vec[0]))\n",
        "lstm_classifier=lstm(1,100)\n",
        "loss_fn=nn.MSELoss()\n",
        "optimizer=torch.optim.Adam(lstm_classifier.parameters(),lr=lr)\n",
        "print(len(lstm_classifier(torch.reshape(train_corpus_vec[0],(maxlen,1,1)))))\n",
        "\n",
        "#print(train_corpus_vect)\n",
        "#features=train_corpus_vect.shape[1]\n",
        "\n",
        "#model=Sequential([InputLayer(shape=(features,1)),\n",
        "#                  LSTM(128),\n",
        "#                  Dense(1)])\n",
        "#model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "#model.fit(train_corpus_vect,train_label,epochs=epochs,batch_size=bsize)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uYNXk2MWyTCE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlFGFfT82mYxJQ6zRgKDam",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}