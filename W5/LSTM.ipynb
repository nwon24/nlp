{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nwon24/nlp/blob/main/W5/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dWAcH6mWYxkE",
        "outputId": "83174ba2-9de7-4a13-cda6-1ddc50ffb7c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(135)\n",
            "cost: 0.709937  [   20/12000]\n",
            "tensor(123)\n",
            "tensor(15)\n",
            "tensor(103)\n",
            "tensor(26)\n",
            "tensor(27)\n",
            "tensor(43)\n",
            "tensor(71)\n",
            "tensor(58)\n",
            "tensor(49)\n",
            "tensor(55)\n",
            "cost: 0.665210  [  220/12000]\n",
            "tensor(102)\n",
            "tensor(27)\n",
            "tensor(132)\n",
            "tensor(48)\n",
            "tensor(71)\n",
            "tensor(104)\n",
            "tensor(56)\n",
            "tensor(52)\n",
            "tensor(120)\n",
            "tensor(33)\n",
            "cost: 0.662861  [  420/12000]\n",
            "tensor(53)\n",
            "tensor(109)\n",
            "tensor(32)\n",
            "tensor(54)\n",
            "tensor(103)\n",
            "tensor(113)\n",
            "tensor(16)\n",
            "tensor(20)\n",
            "tensor(42)\n",
            "tensor(19)\n",
            "cost: 0.669909  [  620/12000]\n",
            "tensor(117)\n",
            "tensor(48)\n",
            "tensor(26)\n",
            "tensor(127)\n",
            "tensor(110)\n",
            "tensor(29)\n",
            "tensor(52)\n",
            "tensor(47)\n",
            "tensor(58)\n",
            "tensor(21)\n",
            "cost: 0.710167  [  820/12000]\n",
            "tensor(39)\n",
            "tensor(111)\n",
            "tensor(156)\n",
            "tensor(53)\n",
            "tensor(30)\n",
            "tensor(27)\n",
            "tensor(101)\n",
            "tensor(36)\n",
            "tensor(27)\n",
            "tensor(89)\n",
            "cost: 0.699284  [ 1020/12000]\n",
            "tensor(111)\n",
            "tensor(99)\n",
            "tensor(106)\n",
            "tensor(116)\n",
            "tensor(61)\n",
            "tensor(166)\n",
            "tensor(110)\n",
            "tensor(59)\n",
            "tensor(28)\n",
            "tensor(90)\n",
            "cost: 0.691416  [ 1220/12000]\n",
            "tensor(54)\n",
            "tensor(21)\n",
            "tensor(18)\n",
            "tensor(156)\n",
            "tensor(57)\n",
            "tensor(80)\n",
            "tensor(106)\n",
            "tensor(20)\n",
            "tensor(59)\n",
            "tensor(116)\n",
            "cost: 0.699642  [ 1420/12000]\n",
            "tensor(27)\n",
            "tensor(38)\n",
            "tensor(43)\n",
            "tensor(50)\n",
            "tensor(83)\n",
            "tensor(187)\n",
            "tensor(134)\n",
            "tensor(149)\n",
            "tensor(21)\n",
            "tensor(40)\n",
            "cost: 0.679636  [ 1620/12000]\n",
            "tensor(127)\n",
            "tensor(28)\n",
            "tensor(38)\n",
            "tensor(69)\n",
            "tensor(134)\n",
            "tensor(47)\n",
            "tensor(160)\n",
            "tensor(26)\n",
            "tensor(22)\n",
            "tensor(105)\n",
            "cost: 0.671285  [ 1820/12000]\n",
            "tensor(55)\n",
            "tensor(93)\n",
            "tensor(38)\n",
            "tensor(21)\n",
            "tensor(24)\n",
            "tensor(41)\n",
            "tensor(33)\n",
            "tensor(59)\n",
            "tensor(49)\n",
            "tensor(55)\n",
            "cost: 0.688606  [ 2020/12000]\n",
            "tensor(137)\n",
            "tensor(61)\n",
            "tensor(51)\n",
            "tensor(62)\n",
            "tensor(26)\n",
            "tensor(49)\n",
            "tensor(74)\n",
            "tensor(29)\n",
            "tensor(88)\n",
            "tensor(16)\n",
            "cost: 0.704514  [ 2220/12000]\n",
            "tensor(155)\n",
            "tensor(23)\n",
            "tensor(107)\n",
            "tensor(98)\n",
            "tensor(18)\n",
            "tensor(77)\n",
            "tensor(103)\n",
            "tensor(52)\n",
            "tensor(28)\n",
            "tensor(126)\n",
            "cost: 0.699432  [ 2420/12000]\n",
            "tensor(36)\n",
            "tensor(99)\n",
            "tensor(40)\n",
            "tensor(31)\n",
            "tensor(94)\n",
            "tensor(22)\n",
            "tensor(64)\n",
            "tensor(159)\n",
            "tensor(42)\n",
            "tensor(140)\n",
            "cost: 0.690909  [ 2620/12000]\n",
            "tensor(32)\n",
            "tensor(42)\n",
            "tensor(95)\n",
            "tensor(127)\n",
            "tensor(46)\n",
            "tensor(135)\n",
            "tensor(53)\n",
            "tensor(48)\n",
            "tensor(67)\n",
            "tensor(35)\n",
            "cost: 0.688930  [ 2820/12000]\n",
            "tensor(62)\n",
            "tensor(24)\n",
            "tensor(51)\n",
            "tensor(103)\n",
            "tensor(25)\n",
            "tensor(27)\n",
            "tensor(81)\n",
            "tensor(53)\n",
            "tensor(30)\n",
            "tensor(35)\n",
            "cost: 0.664293  [ 3020/12000]\n",
            "tensor(149)\n",
            "tensor(151)\n",
            "tensor(30)\n",
            "tensor(53)\n",
            "tensor(72)\n",
            "tensor(58)\n",
            "tensor(36)\n",
            "tensor(68)\n",
            "tensor(91)\n",
            "tensor(69)\n",
            "cost: 0.690138  [ 3220/12000]\n",
            "tensor(24)\n",
            "tensor(20)\n",
            "tensor(20)\n",
            "tensor(20)\n",
            "tensor(58)\n",
            "tensor(29)\n",
            "tensor(21)\n",
            "tensor(25)\n",
            "tensor(85)\n",
            "tensor(92)\n",
            "cost: 0.679811  [ 3420/12000]\n",
            "tensor(50)\n",
            "tensor(35)\n",
            "tensor(47)\n",
            "tensor(80)\n",
            "tensor(33)\n",
            "tensor(62)\n",
            "tensor(86)\n",
            "tensor(18)\n",
            "tensor(75)\n",
            "tensor(99)\n",
            "cost: 0.677336  [ 3620/12000]\n",
            "tensor(74)\n",
            "tensor(100)\n",
            "tensor(68)\n",
            "tensor(103)\n",
            "tensor(19)\n",
            "tensor(65)\n",
            "tensor(71)\n",
            "tensor(112)\n",
            "tensor(101)\n",
            "tensor(40)\n",
            "cost: 0.688405  [ 3820/12000]\n",
            "tensor(62)\n",
            "tensor(110)\n",
            "tensor(25)\n",
            "tensor(73)\n",
            "tensor(74)\n",
            "tensor(76)\n",
            "tensor(25)\n",
            "tensor(68)\n",
            "tensor(87)\n",
            "tensor(133)\n",
            "cost: 0.699698  [ 4020/12000]\n",
            "tensor(36)\n",
            "tensor(74)\n",
            "tensor(23)\n",
            "tensor(34)\n",
            "tensor(138)\n",
            "tensor(44)\n",
            "tensor(18)\n",
            "tensor(139)\n",
            "tensor(75)\n",
            "tensor(115)\n",
            "cost: 0.699814  [ 4220/12000]\n",
            "tensor(32)\n",
            "tensor(56)\n",
            "tensor(48)\n",
            "tensor(26)\n",
            "tensor(27)\n",
            "tensor(29)\n",
            "tensor(19)\n",
            "tensor(21)\n",
            "tensor(51)\n",
            "tensor(61)\n",
            "cost: 0.694923  [ 4420/12000]\n",
            "tensor(21)\n",
            "tensor(32)\n",
            "tensor(30)\n",
            "tensor(39)\n",
            "tensor(48)\n",
            "tensor(82)\n",
            "tensor(65)\n",
            "tensor(51)\n",
            "tensor(74)\n",
            "tensor(56)\n",
            "cost: 0.705684  [ 4620/12000]\n",
            "tensor(69)\n",
            "tensor(44)\n",
            "tensor(80)\n",
            "tensor(23)\n",
            "tensor(142)\n",
            "tensor(39)\n",
            "tensor(58)\n",
            "tensor(120)\n",
            "tensor(50)\n",
            "tensor(63)\n",
            "cost: 0.678044  [ 4820/12000]\n",
            "tensor(148)\n",
            "tensor(43)\n",
            "tensor(43)\n",
            "tensor(137)\n",
            "tensor(35)\n",
            "tensor(35)\n",
            "tensor(134)\n",
            "tensor(24)\n",
            "tensor(37)\n",
            "tensor(16)\n",
            "cost: 0.702602  [ 5020/12000]\n",
            "tensor(30)\n",
            "tensor(82)\n",
            "tensor(79)\n",
            "tensor(41)\n",
            "tensor(22)\n",
            "tensor(21)\n",
            "tensor(168)\n",
            "tensor(44)\n",
            "tensor(27)\n",
            "tensor(43)\n",
            "cost: 0.706636  [ 5220/12000]\n",
            "tensor(110)\n",
            "tensor(37)\n",
            "tensor(131)\n",
            "tensor(146)\n",
            "tensor(54)\n",
            "tensor(14)\n",
            "tensor(107)\n",
            "tensor(22)\n",
            "tensor(38)\n",
            "tensor(22)\n",
            "cost: 0.690030  [ 5420/12000]\n",
            "tensor(133)\n",
            "tensor(55)\n",
            "tensor(59)\n",
            "tensor(68)\n",
            "tensor(46)\n",
            "tensor(38)\n",
            "tensor(29)\n",
            "tensor(43)\n",
            "tensor(102)\n",
            "tensor(59)\n",
            "cost: 0.690492  [ 5620/12000]\n",
            "tensor(86)\n",
            "tensor(62)\n",
            "tensor(100)\n",
            "tensor(23)\n",
            "tensor(24)\n",
            "tensor(70)\n",
            "tensor(18)\n",
            "tensor(85)\n",
            "tensor(112)\n",
            "tensor(60)\n",
            "cost: 0.670682  [ 5820/12000]\n",
            "tensor(152)\n",
            "tensor(39)\n",
            "tensor(59)\n",
            "tensor(144)\n",
            "tensor(77)\n",
            "tensor(46)\n",
            "tensor(60)\n",
            "tensor(107)\n",
            "tensor(48)\n",
            "tensor(161)\n",
            "cost: 0.680409  [ 6020/12000]\n",
            "tensor(28)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-3805744627.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlstm_classifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlstm_classifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-3805744627.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m#pred=model(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mcost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense\n",
        "#from keras.layers import Flatten\n",
        "#from keras.layers import LSTM\n",
        "#from keras.layers import InputLayer\n",
        "from keras.preprocessing import sequence\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn import model_selection\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device=torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "\n",
        "data=pd.read_csv(\"xaa\",encoding=\"utf-8\")\n",
        "\n",
        "vect  = CountVectorizer()\n",
        "#corpus = vect.fit_transform(data[\"text\"])\n",
        "#train_corpus, test_corpus, train_label, test_label = model_selection.train_test_split(data[\"text\"],data[\"label\"],test_size=0.4)\n",
        "#Encoder = LabelEncoder()\n",
        "#train_label = Encoder.fit_transform(train_label)\n",
        "#test_label = Encoder.fit_transform(test_label)\n",
        "#train_corpus_vect=vect.transform(train_corpus)\n",
        "#test_corpus_vect=vect.transform(test_corpus)\n",
        "\n",
        "# Initalise vect.vocabulary_\n",
        "vect.fit_transform(data[\"text\"])\n",
        "\n",
        "maxlen=0\n",
        "\n",
        "def transform(text,vect):\n",
        "    global maxlen\n",
        "    d=vect.vocabulary_\n",
        "    p=vect.build_preprocessor()\n",
        "    t=vect.build_tokenizer()\n",
        "    vec_list=[]\n",
        "    for doc in text:\n",
        "        tokens=t(p(doc))\n",
        "        s=len(tokens)\n",
        "        if s>maxlen:\n",
        "            maxlen=s\n",
        "        doc_vec=np.array([d[token] for token in tokens])\n",
        "        #doc_vec=sequence.pad_sequences(doc_vec,maxlen=maxlen)\n",
        "        vec_list.append(doc_vec)\n",
        "    vec_list=sequence.pad_sequences(vec_list,maxlen=maxlen,padding=\"post\")\n",
        "    corpus_vec=np.vstack(vec_list)\n",
        "    return torch.tensor(corpus_vec).float()\n",
        "\n",
        "# print(corpus_vec)\n",
        "\n",
        "bsize=20\n",
        "epochs=3\n",
        "lr=1e-5\n",
        "\n",
        "class corpus(Dataset):\n",
        "    def __init__(self,corpus,label,seq):\n",
        "        self.corpus=corpus\n",
        "        self.label=label\n",
        "        self.seq=seq\n",
        "    def __len__(self):\n",
        "        return len(self.corpus)\n",
        "    def __getitem__(self,idx):\n",
        "        return self.corpus[idx],self.label[idx]\n",
        "\n",
        "class lstm(nn.Module):\n",
        "    def __init__(self,input_size,hidden_size,seq):\n",
        "        super(lstm,self).__init__()\n",
        "        self.input_size=input_size\n",
        "        self.hidden_size=hidden_size\n",
        "        self.seq=seq\n",
        "        self.lstm=nn.LSTM(input_size,hidden_size,batch_first=True,dropout=0.2)\n",
        "        self.fc=nn.Linear(self.hidden_size*seq,1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x, (_,_) = self.lstm(x)\n",
        "        x = torch.flatten(x,1)\n",
        "        x = self.fc(x)\n",
        "        return nn.Sigmoid()(x)\n",
        "\n",
        "Encoder = LabelEncoder()\n",
        "corpus_vec = transform(data[\"text\"],vect)\n",
        "train_corpus_vec, test_corpus_vec, train_label, test_label = model_selection.train_test_split(corpus_vec,data[\"label\"],test_size=0.4)\n",
        "train_label = torch.from_numpy(Encoder.fit_transform(train_label)).float()\n",
        "test_label = torch.from_numpy(Encoder.fit_transform(test_label)).float()\n",
        "#train_corpus_vec = transform(train_corpus,vect)\n",
        "#test_corpus_vec = transform(test_corpus,vect)\n",
        "lstm_classifier=lstm(1,100,maxlen)\n",
        "loss_fn=nn.BCELoss()\n",
        "#optimizer=torch.optim.Adam(lstm_classifier.parameters(),lr=lr)\n",
        "optimizer=torch.optim.SGD(lstm_classifier.parameters(),lr=lr)\n",
        "#print(len(lstm_classifier(torch.reshape(train_corpus_vec[0],(bsize,maxlen,1)))))\n",
        "\n",
        "train_dataloader=DataLoader(corpus(train_corpus_vec,train_label,maxlen),batch_size=bsize,shuffle=True)\n",
        "test_dataloader=DataLoader(corpus(test_corpus_vec,test_label,maxlen),batch_size=bsize,shuffle=True)\n",
        "\n",
        "def train(dataloader,model,loss_fn,optimizer):\n",
        "    size=len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch,(x,y) in enumerate(dataloader):\n",
        "        x,y=x.to(device),y.to(device)\n",
        "        #print(torch.sum(x[0]!=0))\n",
        "        pred=model(x.reshape(bsize,dataloader.dataset.seq,1).to(device))\n",
        "        #pred=model(x)\n",
        "        cost=loss_fn(pred.flatten(),y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % 10 == 0:\n",
        "            cost, current = cost.item(), batch * bsize + len(x)\n",
        "            print(f\"cost: {cost:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x,y=x.to(device),y.to(device)\n",
        "            pred=model(x.reshape(bsize,dataloader.dataset.seq,1).to(device))\n",
        "            #pred=model(x)\n",
        "            #print(torch.round(pred.flatten()),y)\n",
        "            test_loss += loss_fn(pred.flatten(), y).item()\n",
        "            ncorrect = (torch.round(pred.flatten()) == y).sum().item()\n",
        "            print(ncorrect)\n",
        "            correct += ncorrect\n",
        "\n",
        "    print(correct,size)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct/size):>0.1f}%, Avg loss: {test_loss/size:>8f} \\n\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(train_dataloader,lstm_classifier,loss_fn,optimizer)\n",
        "test_loop(test_dataloader,lstm_classifier,loss_fn)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMP7sHv1em4ibqVzQqCgRjl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}