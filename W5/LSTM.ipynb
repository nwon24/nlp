{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nwon24/nlp/blob/main/W5/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dWAcH6mWYxkE",
        "outputId": "dc9fd0b0-32d5-4c9a-93c7-36512cbbc7a3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_12\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_12 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m191\u001b[0m, \u001b[38;5;34m16\u001b[0m)          │       \u001b[38;5;34m769,456\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_12 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m191\u001b[0m, \u001b[38;5;34m20\u001b[0m)          │         \u001b[38;5;34m2,960\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_12 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3820\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1\u001b[0m)                │         \u001b[38;5;34m3,821\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">191</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">769,456</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">191</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,960</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3820</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,821</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m776,237\u001b[0m (2.96 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">776,237</span> (2.96 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m776,237\u001b[0m (2.96 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">776,237</span> (2.96 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost: 0.704712, accuracy: 0.468750  [   64/16000]\n",
            "cost: 0.674237, accuracy: 0.578125  [  704/16000]\n",
            "cost: 0.689275, accuracy: 0.625000  [ 1344/16000]\n",
            "cost: 0.680201, accuracy: 0.546875  [ 1984/16000]\n",
            "cost: 0.697675, accuracy: 0.468750  [ 2624/16000]\n",
            "cost: 0.704510, accuracy: 0.484375  [ 3264/16000]\n",
            "cost: 0.677637, accuracy: 0.593750  [ 3904/16000]\n",
            "cost: 0.663593, accuracy: 0.609375  [ 4544/16000]\n",
            "cost: 0.692447, accuracy: 0.546875  [ 5184/16000]\n",
            "cost: 0.666885, accuracy: 0.562500  [ 5824/16000]\n",
            "cost: 0.704796, accuracy: 0.500000  [ 6464/16000]\n",
            "cost: 0.671941, accuracy: 0.609375  [ 7104/16000]\n",
            "cost: 0.688760, accuracy: 0.500000  [ 7744/16000]\n",
            "cost: 0.650010, accuracy: 0.578125  [ 8384/16000]\n",
            "cost: 0.655104, accuracy: 0.593750  [ 9024/16000]\n",
            "cost: 0.662534, accuracy: 0.625000  [ 9664/16000]\n",
            "cost: 0.673349, accuracy: 0.546875  [10304/16000]\n",
            "cost: 0.636520, accuracy: 0.671875  [10944/16000]\n",
            "cost: 0.628207, accuracy: 0.687500  [11584/16000]\n",
            "cost: 0.619365, accuracy: 0.656250  [12224/16000]\n",
            "cost: 0.616981, accuracy: 0.609375  [12864/16000]\n",
            "cost: 0.623083, accuracy: 0.593750  [13504/16000]\n",
            "cost: 0.608323, accuracy: 0.609375  [14144/16000]\n",
            "cost: 0.622327, accuracy: 0.718750  [14784/16000]\n",
            "cost: 0.709216, accuracy: 0.515625  [15424/16000]\n",
            "cost: 0.540128, accuracy: 0.765625  [   64/16000]\n",
            "cost: 0.655041, accuracy: 0.578125  [  704/16000]\n",
            "cost: 0.580410, accuracy: 0.718750  [ 1344/16000]\n",
            "cost: 0.518817, accuracy: 0.718750  [ 1984/16000]\n",
            "cost: 0.617208, accuracy: 0.656250  [ 2624/16000]\n",
            "cost: 0.499990, accuracy: 0.718750  [ 3264/16000]\n",
            "cost: 0.558257, accuracy: 0.687500  [ 3904/16000]\n",
            "cost: 0.517831, accuracy: 0.765625  [ 4544/16000]\n",
            "cost: 0.550781, accuracy: 0.656250  [ 5184/16000]\n",
            "cost: 0.583423, accuracy: 0.687500  [ 5824/16000]\n",
            "cost: 0.666765, accuracy: 0.593750  [ 6464/16000]\n",
            "cost: 0.540989, accuracy: 0.781250  [ 7104/16000]\n",
            "cost: 0.518429, accuracy: 0.718750  [ 7744/16000]\n",
            "cost: 0.656852, accuracy: 0.640625  [ 8384/16000]\n",
            "cost: 0.480449, accuracy: 0.812500  [ 9024/16000]\n",
            "cost: 0.640052, accuracy: 0.656250  [ 9664/16000]\n",
            "cost: 0.603204, accuracy: 0.640625  [10304/16000]\n",
            "cost: 0.562060, accuracy: 0.734375  [10944/16000]\n",
            "cost: 0.463435, accuracy: 0.781250  [11584/16000]\n",
            "cost: 0.506052, accuracy: 0.765625  [12224/16000]\n",
            "cost: 0.479641, accuracy: 0.734375  [12864/16000]\n",
            "cost: 0.690624, accuracy: 0.593750  [13504/16000]\n",
            "cost: 0.482437, accuracy: 0.750000  [14144/16000]\n",
            "cost: 0.533132, accuracy: 0.812500  [14784/16000]\n",
            "cost: 0.557012, accuracy: 0.687500  [15424/16000]\n",
            "cost: 0.490947, accuracy: 0.781250  [   64/16000]\n",
            "cost: 0.509026, accuracy: 0.671875  [  704/16000]\n",
            "cost: 0.448301, accuracy: 0.796875  [ 1344/16000]\n",
            "cost: 0.419177, accuracy: 0.828125  [ 1984/16000]\n",
            "cost: 0.418303, accuracy: 0.796875  [ 2624/16000]\n",
            "cost: 0.515743, accuracy: 0.718750  [ 3264/16000]\n",
            "cost: 0.547540, accuracy: 0.796875  [ 3904/16000]\n",
            "cost: 0.509012, accuracy: 0.750000  [ 4544/16000]\n",
            "cost: 0.434352, accuracy: 0.812500  [ 5184/16000]\n",
            "cost: 0.455617, accuracy: 0.765625  [ 5824/16000]\n",
            "cost: 0.309887, accuracy: 0.921875  [ 6464/16000]\n",
            "cost: 0.385829, accuracy: 0.812500  [ 7104/16000]\n",
            "cost: 0.431305, accuracy: 0.812500  [ 7744/16000]\n",
            "cost: 0.413738, accuracy: 0.828125  [ 8384/16000]\n",
            "cost: 0.458985, accuracy: 0.781250  [ 9024/16000]\n",
            "cost: 0.305943, accuracy: 0.921875  [ 9664/16000]\n",
            "cost: 0.425353, accuracy: 0.796875  [10304/16000]\n",
            "cost: 0.447721, accuracy: 0.781250  [10944/16000]\n",
            "cost: 0.467054, accuracy: 0.781250  [11584/16000]\n",
            "cost: 0.482208, accuracy: 0.812500  [12224/16000]\n",
            "cost: 0.350836, accuracy: 0.906250  [12864/16000]\n",
            "cost: 0.476934, accuracy: 0.765625  [13504/16000]\n",
            "cost: 0.334559, accuracy: 0.875000  [14144/16000]\n",
            "cost: 0.543260, accuracy: 0.765625  [14784/16000]\n",
            "cost: 0.563678, accuracy: 0.765625  [15424/16000]\n",
            "cost: 0.359380, accuracy: 0.875000  [   64/16000]\n",
            "cost: 0.424713, accuracy: 0.859375  [  704/16000]\n",
            "cost: 0.304992, accuracy: 0.921875  [ 1344/16000]\n",
            "cost: 0.339814, accuracy: 0.843750  [ 1984/16000]\n",
            "cost: 0.381913, accuracy: 0.828125  [ 2624/16000]\n",
            "cost: 0.312616, accuracy: 0.843750  [ 3264/16000]\n",
            "cost: 0.353155, accuracy: 0.890625  [ 3904/16000]\n",
            "cost: 0.342855, accuracy: 0.906250  [ 4544/16000]\n",
            "cost: 0.369309, accuracy: 0.890625  [ 5184/16000]\n",
            "cost: 0.315557, accuracy: 0.796875  [ 5824/16000]\n",
            "cost: 0.291474, accuracy: 0.890625  [ 6464/16000]\n",
            "cost: 0.389055, accuracy: 0.812500  [ 7104/16000]\n",
            "cost: 0.311750, accuracy: 0.890625  [ 7744/16000]\n",
            "cost: 0.360551, accuracy: 0.875000  [ 8384/16000]\n",
            "cost: 0.448898, accuracy: 0.812500  [ 9024/16000]\n",
            "cost: 0.419459, accuracy: 0.796875  [ 9664/16000]\n",
            "cost: 0.508283, accuracy: 0.734375  [10304/16000]\n",
            "cost: 0.319465, accuracy: 0.859375  [10944/16000]\n",
            "cost: 0.294065, accuracy: 0.875000  [11584/16000]\n",
            "cost: 0.432352, accuracy: 0.859375  [12224/16000]\n",
            "cost: 0.280037, accuracy: 0.890625  [12864/16000]\n",
            "cost: 0.389484, accuracy: 0.781250  [13504/16000]\n",
            "cost: 0.312377, accuracy: 0.890625  [14144/16000]\n",
            "cost: 0.344506, accuracy: 0.859375  [14784/16000]\n",
            "cost: 0.384118, accuracy: 0.828125  [15424/16000]\n",
            "cost: 0.264701, accuracy: 0.890625  [   64/16000]\n",
            "cost: 0.366485, accuracy: 0.828125  [  704/16000]\n",
            "cost: 0.269702, accuracy: 0.906250  [ 1344/16000]\n",
            "cost: 0.333906, accuracy: 0.843750  [ 1984/16000]\n",
            "cost: 0.258864, accuracy: 0.890625  [ 2624/16000]\n",
            "cost: 0.303817, accuracy: 0.828125  [ 3264/16000]\n",
            "cost: 0.231371, accuracy: 0.906250  [ 3904/16000]\n",
            "cost: 0.327413, accuracy: 0.859375  [ 4544/16000]\n",
            "cost: 0.311396, accuracy: 0.812500  [ 5184/16000]\n",
            "cost: 0.422727, accuracy: 0.781250  [ 5824/16000]\n",
            "cost: 0.336221, accuracy: 0.859375  [ 6464/16000]\n",
            "cost: 0.333517, accuracy: 0.875000  [ 7104/16000]\n",
            "cost: 0.286136, accuracy: 0.906250  [ 7744/16000]\n",
            "cost: 0.367614, accuracy: 0.843750  [ 8384/16000]\n",
            "cost: 0.274299, accuracy: 0.859375  [ 9024/16000]\n",
            "cost: 0.228614, accuracy: 0.890625  [ 9664/16000]\n",
            "cost: 0.259083, accuracy: 0.906250  [10304/16000]\n",
            "cost: 0.263845, accuracy: 0.875000  [10944/16000]\n",
            "cost: 0.429477, accuracy: 0.812500  [11584/16000]\n",
            "cost: 0.207971, accuracy: 0.937500  [12224/16000]\n",
            "cost: 0.193355, accuracy: 0.937500  [12864/16000]\n",
            "cost: 0.270957, accuracy: 0.906250  [13504/16000]\n",
            "cost: 0.430139, accuracy: 0.828125  [14144/16000]\n",
            "cost: 0.316180, accuracy: 0.890625  [14784/16000]\n",
            "cost: 0.386939, accuracy: 0.828125  [15424/16000]\n",
            "cost: 0.221586, accuracy: 0.937500  [   64/16000]\n",
            "cost: 0.302897, accuracy: 0.875000  [  704/16000]\n",
            "cost: 0.299988, accuracy: 0.875000  [ 1344/16000]\n",
            "cost: 0.305411, accuracy: 0.890625  [ 1984/16000]\n",
            "cost: 0.197498, accuracy: 0.890625  [ 2624/16000]\n",
            "cost: 0.162101, accuracy: 0.953125  [ 3264/16000]\n",
            "cost: 0.295365, accuracy: 0.875000  [ 3904/16000]\n",
            "cost: 0.224152, accuracy: 0.890625  [ 4544/16000]\n",
            "cost: 0.235037, accuracy: 0.890625  [ 5184/16000]\n",
            "cost: 0.256606, accuracy: 0.906250  [ 5824/16000]\n",
            "cost: 0.360830, accuracy: 0.875000  [ 6464/16000]\n",
            "cost: 0.229941, accuracy: 0.921875  [ 7104/16000]\n",
            "cost: 0.246926, accuracy: 0.875000  [ 7744/16000]\n",
            "cost: 0.265533, accuracy: 0.890625  [ 8384/16000]\n",
            "cost: 0.360441, accuracy: 0.875000  [ 9024/16000]\n",
            "cost: 0.200322, accuracy: 0.921875  [ 9664/16000]\n",
            "cost: 0.214970, accuracy: 0.937500  [10304/16000]\n",
            "cost: 0.274003, accuracy: 0.906250  [10944/16000]\n",
            "cost: 0.199461, accuracy: 0.953125  [11584/16000]\n",
            "cost: 0.346813, accuracy: 0.859375  [12224/16000]\n",
            "cost: 0.203703, accuracy: 0.921875  [12864/16000]\n",
            "cost: 0.244333, accuracy: 0.890625  [13504/16000]\n",
            "cost: 0.315476, accuracy: 0.890625  [14144/16000]\n",
            "cost: 0.275039, accuracy: 0.875000  [14784/16000]\n",
            "cost: 0.426334, accuracy: 0.828125  [15424/16000]\n",
            "cost: 0.233404, accuracy: 0.921875  [   64/16000]\n",
            "cost: 0.223799, accuracy: 0.921875  [  704/16000]\n",
            "cost: 0.273232, accuracy: 0.875000  [ 1344/16000]\n",
            "cost: 0.226947, accuracy: 0.906250  [ 1984/16000]\n",
            "cost: 0.201378, accuracy: 0.937500  [ 2624/16000]\n",
            "cost: 0.227167, accuracy: 0.906250  [ 3264/16000]\n",
            "cost: 0.281453, accuracy: 0.875000  [ 3904/16000]\n",
            "cost: 0.233040, accuracy: 0.937500  [ 4544/16000]\n",
            "cost: 0.220377, accuracy: 0.937500  [ 5184/16000]\n",
            "cost: 0.322641, accuracy: 0.906250  [ 5824/16000]\n",
            "cost: 0.268042, accuracy: 0.921875  [ 6464/16000]\n",
            "cost: 0.145160, accuracy: 0.968750  [ 7104/16000]\n",
            "cost: 0.168127, accuracy: 0.937500  [ 7744/16000]\n",
            "cost: 0.155872, accuracy: 0.953125  [ 8384/16000]\n",
            "cost: 0.313991, accuracy: 0.843750  [ 9024/16000]\n",
            "cost: 0.201789, accuracy: 0.906250  [ 9664/16000]\n",
            "cost: 0.203287, accuracy: 0.906250  [10304/16000]\n",
            "cost: 0.272851, accuracy: 0.875000  [10944/16000]\n",
            "cost: 0.198024, accuracy: 0.921875  [11584/16000]\n",
            "cost: 0.254480, accuracy: 0.921875  [12224/16000]\n",
            "cost: 0.204291, accuracy: 0.906250  [12864/16000]\n",
            "cost: 0.235617, accuracy: 0.937500  [13504/16000]\n",
            "cost: 0.333327, accuracy: 0.890625  [14144/16000]\n",
            "cost: 0.236493, accuracy: 0.906250  [14784/16000]\n",
            "cost: 0.157038, accuracy: 0.968750  [15424/16000]\n",
            "52\n",
            "57\n",
            "47\n",
            "49\n",
            "55\n",
            "48\n",
            "53\n",
            "55\n",
            "54\n",
            "51\n",
            "47\n",
            "45\n",
            "54\n",
            "52\n",
            "54\n",
            "50\n",
            "52\n",
            "50\n",
            "48\n",
            "53\n",
            "57\n",
            "52\n",
            "56\n",
            "52\n",
            "58\n",
            "58\n",
            "55\n",
            "55\n",
            "51\n",
            "51\n",
            "47\n",
            "51\n",
            "52\n",
            "51\n",
            "50\n",
            "60\n",
            "54\n",
            "48\n",
            "52\n",
            "54\n",
            "48\n",
            "53\n",
            "55\n",
            "48\n",
            "51\n",
            "48\n",
            "52\n",
            "48\n",
            "51\n",
            "54\n",
            "53\n",
            "48\n",
            "50\n",
            "52\n",
            "53\n",
            "51\n",
            "55\n",
            "49\n",
            "50\n",
            "53\n",
            "52\n",
            "55\n",
            "25\n",
            "3244 4000\n",
            "Test Error: \n",
            " Accuracy: 81.1%, Avg loss: 0.736908 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import InputLayer\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn import model_selection\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device=torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "\n",
        "data=pd.read_csv(\"xaa\",encoding=\"utf-8\")\n",
        "\n",
        "vect  = CountVectorizer()\n",
        "#corpus = vect.fit_transform(data[\"text\"])\n",
        "#train_corpus, test_corpus, train_label, test_label = model_selection.train_test_split(data[\"text\"],data[\"label\"],test_size=0.4)\n",
        "#Encoder = LabelEncoder()\n",
        "#train_label = Encoder.fit_transform(train_label)\n",
        "#test_label = Encoder.fit_transform(test_label)\n",
        "#train_corpus_vect=vect.transform(train_corpus)\n",
        "#test_corpus_vect=vect.transform(test_corpus)\n",
        "\n",
        "# Initalise vect.vocabulary_\n",
        "vect.fit_transform(data[\"text\"])\n",
        "\n",
        "maxlen=0\n",
        "\n",
        "def transform(text,vect):\n",
        "    global maxlen\n",
        "    d=vect.vocabulary_\n",
        "    p=vect.build_preprocessor()\n",
        "    t=vect.build_tokenizer()\n",
        "    vec_list=[]\n",
        "    for doc in text:\n",
        "        tokens=t(p(doc))\n",
        "        s=len(tokens)\n",
        "        if s>maxlen:\n",
        "            maxlen=s\n",
        "        doc_vec=np.array([d[token] for token in tokens])\n",
        "        #doc_vec=sequence.pad_sequences(doc_vec,maxlen=maxlen)\n",
        "        vec_list.append(doc_vec)\n",
        "    vec_list=sequence.pad_sequences(vec_list,maxlen=maxlen,padding=\"post\")\n",
        "    corpus_vec=np.vstack(vec_list)\n",
        "    #return nn.functional.normalize(torch.tensor(corpus_vec).float())\n",
        "    return torch.tensor(corpus_vec)\n",
        "    #return torch.tensor(corpus_vec).float()\n",
        "\n",
        "# print(corpus_vec)\n",
        "\n",
        "bsize=64\n",
        "epochs=7\n",
        "lr=1e-3\n",
        "\n",
        "class corpus(Dataset):\n",
        "    def __init__(self,corpus,label,seq):\n",
        "        self.corpus=corpus\n",
        "        self.label=label\n",
        "        self.seq=seq\n",
        "    def __len__(self):\n",
        "        return len(self.corpus)\n",
        "    def __getitem__(self,idx):\n",
        "        return self.corpus[idx],self.label[idx]\n",
        "\n",
        "class lstm(nn.Module):\n",
        "    def __init__(self,input_size,hidden_size,seq):\n",
        "        super(lstm,self).__init__()\n",
        "        self.input_size=input_size\n",
        "        self.hidden_size=hidden_size\n",
        "        self.seq=seq\n",
        "        self.rnn=True\n",
        "        self.lstm=nn.LSTM(input_size,hidden_size,batch_first=True,num_layers=1)\n",
        "        self.fc=nn.Linear(self.hidden_size*seq,1)\n",
        "        #self.fc=nn.Linear(self.hidden_size,1)\n",
        "        self.embed=nn.Embedding(len(vect.vocabulary_),input_size)\n",
        "\n",
        "    def forward(self,x,h0=None,c0=None):\n",
        "        x=self.embed(x)\n",
        "        if h0==None and c0==None:\n",
        "            x, (hn,cn) = self.lstm(x)\n",
        "        else:\n",
        "            x, (hn,cn) = self.lstm(x,(h0,c0))\n",
        "        #print(x[:,-1,:].shape)\n",
        "        #x = torch.flatten(x[:,-1,:],1)\n",
        "        # Flatten like this so that all information from previous time steps is fed into fully connected layer\n",
        "        x=torch.flatten(x,1)\n",
        "        x = self.fc(x)\n",
        "        return nn.Sigmoid()(x), hn,cn\n",
        "        #return nn.Softmax()(x), hn,cn\n",
        "\n",
        "class dense(nn.Module):\n",
        "    def __init__(self,seq,vocab,embed_dim):\n",
        "        super(dense,self).__init__()\n",
        "        self.seq=seq\n",
        "        self.rnn=False\n",
        "        self.network=nn.Sequential(\n",
        "#            nn.Embedding(vocab,embed_dim),\n",
        "            nn.Linear(embed_dim*seq,360),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(360,180),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(180,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.embed=nn.Embedding(vocab,embed_dim)\n",
        "\n",
        "    def forward(self,x):\n",
        "        #print(x.shape)\n",
        "        x=self.embed(x)\n",
        "        x=torch.flatten(x,1)\n",
        "        #x=torch.transpose(x,1,2)\n",
        "        #x=torch.flatten(x,1)\n",
        "        return self.network(x)\n",
        "\n",
        "vocab=len(vect.vocabulary_)\n",
        "Encoder = LabelEncoder()\n",
        "corpus_vec = transform(data[\"text\"],vect)\n",
        "train_corpus_vec, test_corpus_vec, train_label, test_label = model_selection.train_test_split(corpus_vec,data[\"label\"],test_size=0.2)\n",
        "train_label = torch.from_numpy(Encoder.fit_transform(train_label)).float()\n",
        "test_label = torch.from_numpy(Encoder.fit_transform(test_label)).float()\n",
        "#train_corpus_vec = transform(train_corpus,vect)\n",
        "#test_corpus_vec = transform(test_corpus,vect)\n",
        "lstm_classifier=lstm(16,20,maxlen)\n",
        "loss_fn=nn.BCELoss()\n",
        "#loss_fn=nn.BCEWithLogitsLoss()\n",
        "#loss_fn=nn.MSELoss()\n",
        "optimizer=torch.optim.Adam(lstm_classifier.parameters(),lr=lr)\n",
        "#optimizer=torch.optim.SGD(lstm_classifier.parameters(),lr=lr)\n",
        "#print(len(lstm_classifier(torch.reshape(train_corpus_vec[0],(bsize,maxlen,1)))))\n",
        "\n",
        "dense_classifier=dense(maxlen,vocab,16)\n",
        "\n",
        "c=lstm_classifier\n",
        "\n",
        "train_dataloader=DataLoader(corpus(train_corpus_vec,train_label,maxlen),batch_size=bsize,shuffle=True)\n",
        "test_dataloader=DataLoader(corpus(test_corpus_vec,test_label,maxlen),batch_size=bsize,shuffle=True)\n",
        "\n",
        "def train(dataloader,model,loss_fn,optimizer):\n",
        "    hn,cn=None,None\n",
        "    size=len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch,(x,y) in enumerate(dataloader):\n",
        "        x,y=x.to(device),y.to(device)\n",
        "        #print(torch.sum(x[0]!=0))\n",
        "        if model.rnn==True:\n",
        "            #pred,hn,cn=model(x.reshape(bsize,dataloader.dataset.seq,1).to(device),hn,cn)\n",
        "            pred,hn,cn=model(x,hn,cn)\n",
        "        else:\n",
        "            pred=model(x)\n",
        "        #pred=model(x)\n",
        "        cost=loss_fn(pred.flatten(),y)\n",
        "        cost.backward()\n",
        "        #if model.rnn==True:\n",
        "        #    print(model.lstm.weight_ih_l0.grad[0][0])\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if model.rnn==True:\n",
        "            hn=hn.detach()\n",
        "            cn=cn.detach()\n",
        "        if batch % 10 == 0:\n",
        "            cost_val, current = cost.item(), batch * bsize + len(x)\n",
        "            print(f\"cost: {cost_val:>7f}, accuracy: {(torch.round(pred.flatten())==y).sum().item()/bsize:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x,y=x.to(device),y.to(device)\n",
        "            if model.rnn==True:\n",
        "                #pred,_,_=model(x.reshape(bsize,dataloader.dataset.seq,1).to(device))\n",
        "                pred,_,_=model(x)\n",
        "            else:\n",
        "                pred=model(x)\n",
        "            #pred=model(x)\n",
        "            #print(torch.round(pred.flatten()),y)\n",
        "            test_loss += loss_fn(pred.flatten(), y).item()\n",
        "            ncorrect = (torch.round(pred.flatten()) == y).sum().item()\n",
        "            print(ncorrect)\n",
        "            correct += ncorrect\n",
        "\n",
        "    print(correct,size)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct/size):>0.1f}%, Avg loss: {100*test_loss/size:>8f} \\n\")\n",
        "\n",
        "#keras_model=Sequential([InputLayer(input_shape=(maxlen,),batch_size=bsize),\n",
        "#                        Embedding(len(vect.vocabulary_),16),\n",
        "#                  Flatten(),\n",
        "#                  Dense(1,activation=\"sigmoid\")])\n",
        "keras_model=Sequential([InputLayer(shape=(maxlen,),batch_size=bsize),\n",
        "                        Embedding(len(vect.vocabulary_),16),\n",
        "                        LSTM(20,return_sequences=True),\n",
        "                  Flatten(),\n",
        "                  Dense(1,activation=\"sigmoid\")])\n",
        "keras_model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
        "keras_model.summary()\n",
        "\n",
        "use_torch=True\n",
        "\n",
        "#print(corpus_vec[0])\n",
        "if use_torch==True:\n",
        "    for epoch in range(epochs):\n",
        "        train(train_dataloader,c,loss_fn,optimizer)\n",
        "    test_loop(test_dataloader,c,loss_fn)\n",
        "else:\n",
        "    keras_model.fit(train_corpus_vec,train_label,batch_size=bsize,epochs=epochs,validation_data=(test_corpus_vec,test_label))\n",
        "    keras_model.evaluate(test_corpus_vec,test_label,batch_size=bsize)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cy5ekCL0mHOK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNj5yX1KbdD9d4DkORTsxL2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}